{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"CHAOS\", \"MMWHS\"]\n",
    "DOMAINS = [\"CT\", \"MR\"]\n",
    "DATA_PATH = \"data/\"\n",
    "CHECKPOINT_PATH = \"checkpoints/\"\n",
    "USE_3D = False\n",
    "TRAINING_EPOCHS = {\n",
    "    (\"CHAOS\", \"CT\"): 1,\n",
    "}\n",
    "BATCH_SIZE = 64\n",
    "SPATIAL_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# Set True to enable debug prints/timers/visualizations)\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe7c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_MAX_ITEMS = 32  # set the in-memory file cache size per dataset (images and segs)\n",
    "ENABLE_CACHE = True    # set to False to disable caching entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(CHECKPOINT_PATH)\n",
    "DATA_PATH = Path(DATA_PATH)\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if USE_3D:\n",
    "    encoder_type = \"swin_unetr\"\n",
    "else:\n",
    "    encoder_type = \"clipseg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai import transforms\n",
    "\n",
    "\n",
    "# Normalization stats (mean, std) per dataset/domain\n",
    "NORM_STATS = {\n",
    "    (\"MMWHS\", \"MR\"):  (186.5875, 258.5917),\n",
    "    (\"MMWHS\", \"CT\"):  (-745.0086, 1042.7251),\n",
    "    (\"CHAOS\",  \"MR\"): (90.8292, 168.8922),\n",
    "    (\"CHAOS\",  \"CT\"): (-478.1732, 476.7163),\n",
    "}\n",
    "\n",
    "# Optimized preprocessing: resize early\n",
    "\n",
    "def get_preprocessing(dataset_name: str, domain: str, is_training=True):\n",
    "    decode_func = get_decode_func(dataset_name, domain)\n",
    "    mean_std = NORM_STATS.get((dataset_name, domain))\n",
    "    mean, std = (mean_std if mean_std is not None else (None, None))\n",
    "\n",
    "    # Image-specific transforms\n",
    "    if USE_3D:\n",
    "        image_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "    else:\n",
    "        image_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "\n",
    "    # Resize early to reduce compute\n",
    "    image_transforms.append(\n",
    "        transforms.Resize(\n",
    "            spatial_size=SPATIAL_SIZE,\n",
    "            size_mode=\"longest\",\n",
    "            mode=\"area\",\n",
    "            anti_aliasing=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert to tensor and ensure float32 for stable CPU ops\n",
    "    image_transforms.extend([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.EnsureType(dtype=torch.float32),\n",
    "    ])\n",
    "\n",
    "    # Augmentations (training only) â€” run in float32 on CPU\n",
    "    if is_training:\n",
    "        image_transforms.extend(\n",
    "            [\n",
    "                transforms.RandGaussianNoise(prob=0.15, std=0.05),\n",
    "                transforms.RandAdjustContrast(prob=0.15, gamma=(0.95, 1.05)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Normalize (still in float32)\n",
    "    if mean is not None and std is not None:\n",
    "        image_transforms.append(\n",
    "            transforms.NormalizeIntensity(\n",
    "                subtrahend=float(mean),\n",
    "                divisor=float(std),\n",
    "                channel_wise=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Repeat to 3 channels only at the end (2D only)\n",
    "    if not USE_3D:\n",
    "        image_transforms.append(transforms.RepeatChannel(repeats=3))\n",
    "\n",
    "    image_transform = transforms.Compose(image_transforms)\n",
    "\n",
    "    # Segmentation transforms\n",
    "    if not USE_3D:\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "    else:\n",
    "        seg_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "\n",
    "    seg_transforms.extend(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "            transforms.Lambda(lambda x: decode_func(x)),  # decode after tensor conversion\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE, size_mode=\"longest\", mode=\"nearest\"\n",
    "            ),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    seg_transform = transforms.Compose(seg_transforms)\n",
    "    return image_transform, seg_transform\n",
    "\n",
    "\n",
    "def get_decode_func(dataset_name, domain):\n",
    "    from src.datasets.mmwhs import mmwhs_labels\n",
    "\n",
    "    decode = None\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        if domain in [\"MR\", \"MRI\"]:\n",
    "            def decode(labels):\n",
    "                # Convert intensity values to class indices (keep as float32)\n",
    "                return labels // 63\n",
    "        elif domain == \"CT\":\n",
    "            def decode(labels):\n",
    "                return torch.where(labels > 0, 1.0, 0.0)\n",
    "    elif dataset_name == \"MMWHS\":\n",
    "        def decode(labels):\n",
    "            decoded_labels = torch.zeros_like(labels, dtype=torch.float32)\n",
    "            for i, label_val in enumerate(mmwhs_labels.keys()):\n",
    "                decoded_labels[labels == label_val] = i\n",
    "            return decoded_labels\n",
    "\n",
    "    if decode is None:\n",
    "        def decode(labels):\n",
    "            return labels\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee8505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad_status(model):\n",
    "    trainable = []\n",
    "    frozen = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            trainable.append(name)\n",
    "        else:\n",
    "            frozen.append(name)\n",
    "    print(f\"\\n=== Parameters requiring grad: {len(trainable)} ===\")\n",
    "    # for n in trainable:\n",
    "        # print(f\"  + {n}\")\n",
    "    print(f\"\\n=== Parameters frozen (no grad): {len(frozen)}===\")\n",
    "    # for n in frozen:\n",
    "    #     print(f\"  - {n}\")\n",
    "\n",
    "    return trainable, frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f132bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_grad_status(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(not p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee31296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to snapshot parameters and report unchanged ones after finetuning\n",
    "import torch\n",
    "\n",
    "def snapshot_parameters(model):\n",
    "    \"\"\"Return a CPU snapshot of all parameter tensors by name.\"\"\"\n",
    "    return {name: p.detach().clone().cpu() for name, p in model.named_parameters()}\n",
    "\n",
    "\n",
    "def print_unchanged_parameters(model, snapshot, only_requires_grad: bool = True):\n",
    "    \"\"\"Print all parameter names whose values are identical to the snapshot.\n",
    "\n",
    "    Args:\n",
    "        model: The model to compare.\n",
    "        snapshot: Dict[name -> Tensor] with the pre-finetune parameter values.\n",
    "        only_requires_grad: If True, only consider parameters with requires_grad=True.\n",
    "    \"\"\"\n",
    "    unchanged = []\n",
    "    considered = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        if only_requires_grad and not p.requires_grad:\n",
    "            continue\n",
    "        prev = snapshot.get(name)\n",
    "        if prev is None:\n",
    "            continue\n",
    "        curr = p.detach().cpu()\n",
    "        considered += 1\n",
    "        if torch.equal(curr, prev):\n",
    "            unchanged.append(name)\n",
    "\n",
    "    print(\"\\n=== Parameters unchanged after finetuning ===\")\n",
    "    for n in unchanged:\n",
    "        print(f\"  - {n}\")\n",
    "    print(f\"Total unchanged: {len(unchanged)} of {considered} considered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning on CHAOS dataset in CT domain with 2d images \n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ğŸ”„ Loading CLIPSeg weights...\n",
      "Frozen 150 CLIP text-encoder params (kept visual+head trainable).\n",
      "\n",
      "=== Parameters requiring grad: 206 ===\n",
      "\n",
      "=== Parameters frozen (no grad): 150===\n",
      "ğŸš€ Starting training for 1 epochs\n",
      "   Device: cpu\n",
      "   Learning Rate: 0.001\n",
      "   Weight Decay: 1e-05\n",
      "   Trainable params: 206 of 356\n",
      "   Params: total=150,796,962, trainable=87,368,865\n",
      "   Batches: train=31, val=7\n",
      "   Tracking params:\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "\n",
      "ğŸ“– Epoch 1/1\n",
      "   LR(s): 1.000000e-03\n",
      "ğŸ”„ Loading CLIPSeg weights...\n",
      "Frozen 150 CLIP text-encoder params (kept visual+head trainable).\n",
      "\n",
      "=== Parameters requiring grad: 206 ===\n",
      "\n",
      "=== Parameters frozen (no grad): 150===\n",
      "ğŸš€ Starting training for 1 epochs\n",
      "   Device: cpu\n",
      "   Learning Rate: 0.001\n",
      "   Weight Decay: 1e-05\n",
      "   Trainable params: 206 of 356\n",
      "   Params: total=150,796,962, trainable=87,368,865\n",
      "   Batches: train=31, val=7\n",
      "   Tracking params:\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "\n",
      "ğŸ“– Epoch 1/1\n",
      "   LR(s): 1.000000e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–         | 1/31 [00:04<02:24,  4.82s/it, Loss=1.1545]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 0: 2.248s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–‹         | 2/31 [00:07<01:49,  3.78s/it, Loss=1.1732]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 1: 2.396s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–‰         | 3/31 [00:10<01:29,  3.19s/it, Loss=1.1485]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 2: 1.732s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|â–ˆâ–        | 4/31 [00:13<01:29,  3.31s/it, Loss=1.1178]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 3: 2.537s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–Œ        | 5/31 [00:16<01:23,  3.19s/it, Loss=1.0656]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 4: 1.883s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|â–ˆâ–‰        | 6/31 [00:20<01:27,  3.49s/it, Loss=1.0344]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 5: 1.944s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|â–ˆâ–ˆâ–       | 7/31 [00:24<01:23,  3.48s/it, Loss=1.0032]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 6: 2.356s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:28<01:24,  3.69s/it, Loss=0.9832]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 7: 2.014s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:31<01:16,  3.46s/it, Loss=0.9648]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 8: 2.161s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:36<01:24,  4.00s/it, Loss=0.9481]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 9: 2.566s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:51<02:23,  7.18s/it, Loss=0.9252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 10: 13.246s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [01:06<03:03,  9.67s/it, Loss=0.9201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 11: 4.061s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [01:09<02:20,  7.81s/it, Loss=0.9334]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 12: 2.164s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [01:13<01:50,  6.51s/it, Loss=0.9348]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 13: 1.741s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [01:16<01:26,  5.42s/it, Loss=0.9278]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 14: 1.990s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [01:18<01:08,  4.57s/it, Loss=0.8814]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 15: 1.879s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [01:21<00:56,  4.06s/it, Loss=0.8565]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 16: 1.947s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [01:25<00:50,  3.90s/it, Loss=0.8149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 17: 2.517s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [01:30<00:50,  4.24s/it, Loss=0.8194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 18: 2.372s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [01:33<00:41,  3.77s/it, Loss=0.8195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 19: 1.899s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [01:35<00:34,  3.43s/it, Loss=0.8352]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 20: 1.876s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [01:39<00:30,  3.39s/it, Loss=0.8281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 21: 2.278s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [01:42<00:26,  3.34s/it, Loss=0.8279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 22: 1.883s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [01:58<00:51,  7.32s/it, Loss=0.8208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 23: 12.307s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [02:08<00:48,  8.07s/it, Loss=0.8183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 24: 2.476s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [02:11<00:32,  6.47s/it, Loss=0.8106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 25: 1.864s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [02:14<00:21,  5.36s/it, Loss=0.8039]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 26: 2.107s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [02:16<00:13,  4.56s/it, Loss=0.7980]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 27: 1.802s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 29/31 [02:19<00:08,  4.08s/it, Loss=0.7848]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 28: 1.820s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [02:23<00:03,  3.84s/it, Loss=0.7771]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 29: 1.879s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [02:26<00:00,  4.73s/it, Loss=0.7697]\n",
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [02:26<00:00,  4.73s/it, Loss=0.7697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 30: 2.092s\n",
      "Epoch 1 - Train Loss: 0.9002\n",
      "   Param norm deltas (after epoch):\n",
      "     encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight: Î”norm=+1.373188e+00 (before=8.586586e+00, after=9.959774e+00)\n",
      "     encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias: Î”norm=+1.146293e-02 (before=1.183554e+00, after=1.195017e+00)\n",
      "     encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight: Î”norm=+1.150630e+00 (before=8.633878e+00, after=9.784508e+00)\n",
      "   Grad non-zero in batches: 31/31 (100.0%)\n",
      "   Labels seen this epoch: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  14%|â–ˆâ–        | 1/7 [00:02<00:16,  2.67s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Loss: 0.7656\n",
      "   New best Val Loss: 0.7656\n",
      "\n",
      "âœ… Training completed!\n",
      "\n",
      "=== Parameters unchanged after finetuning ===\n",
      "  - encoder.clipseg.clip_model.visual.proj\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_1.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_1.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_2.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_2.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_1.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_1.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_2.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_2.bias\n",
      "  - encoder.clipseg.clip_model.visual.ln_post.weight\n",
      "  - encoder.clipseg.clip_model.visual.ln_post.bias\n",
      "  - encoder.clipseg.reduce.weight\n",
      "  - encoder.clipseg.reduce.bias\n",
      "Total unchanged: 29 of 206 considered\n",
      "\n",
      "âœ… Training completed!\n",
      "\n",
      "=== Parameters unchanged after finetuning ===\n",
      "  - encoder.clipseg.clip_model.visual.proj\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_1.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_1.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_2.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_2.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_1.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_1.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_2.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_2.bias\n",
      "  - encoder.clipseg.clip_model.visual.ln_post.weight\n",
      "  - encoder.clipseg.clip_model.visual.ln_post.bias\n",
      "  - encoder.clipseg.reduce.weight\n",
      "  - encoder.clipseg.reduce.bias\n",
      "Total unchanged: 29 of 206 considered\n"
     ]
    }
   ],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in TRAINING_EPOCHS.items():\n",
    "    download_and_extract_dataset(dataset_name, DATA_PATH)\n",
    "\n",
    "    image_transform, seg_transform = get_preprocessing(\n",
    "        dataset_name, domain, is_training=True\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        transform=image_transform,  # Use transform instead of preprocess\n",
    "        seg_transform=seg_transform,  # Pass seg_transform too\n",
    "        base_path=DATA_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=0,\n",
    "        slice_2d=not USE_3D,\n",
    "        # new cache knobs\n",
    "        cache_max_items=CACHE_MAX_ITEMS,\n",
    "        enable_cache=ENABLE_CACHE,\n",
    "    )\n",
    "\n",
    "    #  Ensure the dataset is loaded correctly\n",
    "    if not isinstance(dataset, BaseDataset):\n",
    "        raise TypeError(\n",
    "            f\"Expected dataset to be an instance of BaseDataset, got {type(dataset)}\"\n",
    "        )\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=encoder_type,\n",
    "    )\n",
    "\n",
    "    # Unfreeze everything, then freeze CLIP text encoder (we cache text embeddings -> no grads)\n",
    "    model.unfreeze()\n",
    "    if encoder_type == \"clipseg\":\n",
    "        model.freeze_text_encoder()\n",
    "\n",
    "    # for p in model.encoder.clipseg.model.parameters():\n",
    "    #     p.requires_grad_(True)\n",
    "\n",
    "\n",
    "    # Print grad status before finetuning (after final freeze mask)\n",
    "    print_grad_status(model)\n",
    "\n",
    "    # Snapshot parameters before finetuning (after setting requires_grad correctly)\n",
    "    before_snapshot = snapshot_parameters(model)\n",
    "\n",
    "    history = model.finetune(\n",
    "        epochs=1,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        val_max_batches=1,\n",
    "        fast_val_metrics=True,\n",
    "        debug=DEBUG\n",
    "    )\n",
    "\n",
    "    # Print parameters that didn't change after finetuning (only those requiring grad)\n",
    "    print_unchanged_parameters(model, before_snapshot, only_requires_grad=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
