{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f6157a",
   "metadata": {},
   "source": [
    "The original task arithmetic paper uses CLIP and builds classification heads with zero-shot training based on some templates.\n",
    "In our case, we have some special characteristics to keep in mind:\n",
    "- We are operating on medical, 3D, data.\n",
    "- Our objective is semantic labeling, not simple classification.\n",
    "\n",
    "To deal with this we have a couple of options to try:\n",
    "- Copy the same flow using CLIP trained on imagenet and slice the 3d images into multiple 2d.\n",
    "    - Since we need to do segmentation, we need to find a model that can perform segmentation and be finetuned\n",
    "    - Even better if we access separately the segmentation head and the encoder\n",
    "- Use 3d resnet pretrained on medicalnet by monai, perform a few short training loops with frozen encoder to train the segmentation head. \n",
    "- Use medsam/medsam2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_vector import TaskVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"CHAOS\", \"MMWHS\"]\n",
    "domains = [\"CT\", \"MR\"]\n",
    "data_path = 'data/'\n",
    "save_path = \"checkpoints/\"\n",
    "outputs_path = \"out/\"\n",
    "use_3d = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(save_path)\n",
    "outputs_path = Path(outputs_path)\n",
    "data_path = Path(data_path)\n",
    "datasets_3d = {\n",
    "    (name, domain): get_dataset(\n",
    "        dataset_name=name,\n",
    "        domain=domain,\n",
    "        base_path=data_path,\n",
    "        slice_2d=False,\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    for domain in domains\n",
    "    for name in dataset_names\n",
    "}\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "outputs_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics(name, new_metrics):\n",
    "    with open(outputs_path / \"metrics.json\", \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "    metrics[name] = new_metrics\n",
    "    with open(outputs_path / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a17e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning on CHAOS dataset in CT domain with 3d images\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CHAOS.__init__() got an unexpected keyword argument 'is_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     15\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinetuning on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dataset in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m domain with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m3d\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39muse_3d\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m2d\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m dataset = \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mslice_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_3d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m model = dataset.get_model()\n\u001b[32m     29\u001b[39m model_metrics = model.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/datasets/registry.py:94\u001b[39m, in \u001b[36mget_dataset\u001b[39m\u001b[34m(dataset_name, base_path, preprocess, batch_size, num_workers, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m dataset_class = registry[dataset_name]\n\u001b[32m     93\u001b[39m location = Path(base_path) / dataset_name\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m dataset = \u001b[43mdataset_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[31mTypeError\u001b[39m: CHAOS.__init__() got an unexpected keyword argument 'is_train'"
     ]
    }
   ],
   "source": [
    "# finetuning loop\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    for domain in domains:\n",
    "        filename = f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned.pth\"\n",
    "        filename = save_path / filename\n",
    "        # Check if the finetuned checkpoint already exists\n",
    "        if filename.exists():\n",
    "            print(\n",
    "                f\"Finetuned model for {dataset_name} in {domain} domain with {'3d' if use_3d else '2d'} images already exists at {filename}. Skipping finetuning.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        print(\n",
    "            f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if use_3d else '2d'} images\"\n",
    "        )\n",
    "        dataset = get_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            domain=domain,\n",
    "            base_path=data_path,\n",
    "            batch_size=1,\n",
    "            num_workers=0,\n",
    "            slice_2d=not use_3d,\n",
    "        )\n",
    "\n",
    "        model = dataset.get_model()\n",
    "\n",
    "        model_metrics = model.evaluate()\n",
    "        update_metrics(\n",
    "            f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline\",\n",
    "            model_metrics,\n",
    "        )\n",
    "\n",
    "        tuned_model = model.finetune(\n",
    "            epochs=100,\n",
    "            save_path=filename,\n",
    "        )\n",
    "        model_metrics = model.evaluate()\n",
    "        update_metrics(\n",
    "            f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned\",\n",
    "            model_metrics,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090233",
   "metadata": {},
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1df630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in dataset_names:\n",
    "    for domain in domains:\n",
    "        print(f\"Building task vector for {dataset_name} dataset in {domain} domain with {'3d' if use_3d else '2d'} images\")\n",
    "        baseline_checkpoint = save_path / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline.pth\"\n",
    "        finetuned_checkpoint = save_path / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned.pth\"\n",
    "        task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322eb8af",
   "metadata": {},
   "source": [
    "## CHAOS \n",
    "CHAOS MRI has labels for multiple organs, but CHAOS CT has labels for only one organ (liver).\n",
    "For testing, we will use the liver label from CHAOS CT and the liver label from CHAOS MRI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af268f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_domain, target_domain in [(\"CT\", \"MR\"), (\"MR\", \"CT\")]:\n",
    "    task_vector: TaskVector = task_vectors[f\"CHAOS_{source_domain}\"]\n",
    "    chaos_target = get_dataset(\"CHAOS\", domain=target_domain, base_path=data_path, batch_size=1, num_workers=0, slice_2d=not use_3d, liver_only=True)\n",
    "    model = chaos_target.get_model()\n",
    "    model.load_task_vector(task_vector)\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(f\"CHAOS_{target_domain}_from_{source_domain}\", model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def63981",
   "metadata": {},
   "source": [
    "## MM-WHS\n",
    "Labels are the same in both domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_domain, target_domain in [(\"CT\", \"MR\"), (\"MR\", \"CT\")]:\n",
    "    task_vector: TaskVector = task_vectors[f\"MMWHS_{source_domain}\"]\n",
    "    mmwhs_target = get_dataset(\"MMWHS\", domain=target_domain, base_path=data_path, batch_size=1, num_workers=0, slice_2d=not use_3d)\n",
    "    model = mmwhs_target.get_model()\n",
    "    model.load_task_vector(task_vector)\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(f\"MMWHS_{target_domain}_from_{source_domain}\", model_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
