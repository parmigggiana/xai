{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0bb3b4",
   "metadata": {},
   "source": [
    "In Kaggle, add the following to the dependencies:\n",
    "```\n",
    "pip install torch\n",
    "pip install numpy\n",
    "pip install pydicom\n",
    "pip install PILlow\n",
    "pip install matplotlib\n",
    "```\n",
    "Enable file persistence and internet access.\n",
    "Remember that you can run the whole notebook and close the runtime without wasting resources by going to File > Save Version > Save & Run All (Double check that GPU is selected in the advanced settings).\n",
    "Later, by going to 'File' > 'Version history' you can view the full logs and download the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c8f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle\n",
    "import os\n",
    "\n",
    "IN_KAGGLE = False\n",
    "if os.environ.get(\"KAGGLE_URL_BASE\", \"\"):\n",
    "    IN_KAGGLE = True\n",
    "    !git clone https://github.com/parmigggiana/xai /kaggle/working/xai\n",
    "    %cd xai\n",
    "    !git fetch\n",
    "    !git reset --hard origin/main\n",
    "    %pip install 'monai[einops,itk,nibabel]>=1.5.0' git+https://github.com/timojl/clipseg.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c6af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "IN_COLAB = False\n",
    "if not IN_KAGGLE:\n",
    "    try:\n",
    "        import google.colab\n",
    "        from google.colab import drive\n",
    "\n",
    "        IN_COLAB = True\n",
    "        import os\n",
    "\n",
    "        drive.mount(\"/content/drive\")\n",
    "        os.makedirs(\"/content/drive/MyDrive/xai\", exist_ok=True)\n",
    "        !git clone https://github.com/parmigggiana/xai /content/xai\n",
    "        %cd /content/xai\n",
    "        !git fetch\n",
    "        !git reset --hard origin/main\n",
    "        %pip install -r requirements.txt\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_vector import TaskVector\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"CHAOS\", \"MMWHS\"]\n",
    "DOMAINS = [\"MR\", \"CT\"]\n",
    "DATA_PATH = \"data/\"\n",
    "CHECKPOINT_PATH = \"checkpoints/\"\n",
    "OUTPUTS_PATH = \"outputs/\"\n",
    "USE_3D = False\n",
    "TRAINING_EPOCHS = {\n",
    "    (\"CHAOS\", \"MR\"): 1,\n",
    "    (\"CHAOS\", \"CT\"): 1,\n",
    "    (\"MMWHS\", \"MR\"): 1,\n",
    "    (\"MMWHS\", \"CT\"): 1,\n",
    "    \n",
    "    \n",
    "}\n",
    "NUM_WORKERS = 4 if IN_KAGGLE or IN_COLAB else 0\n",
    "BATCH_SIZE = 8\n",
    "SPATIAL_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(CHECKPOINT_PATH)\n",
    "OUTPUTS_PATH = Path(OUTPUTS_PATH)\n",
    "DATA_PATH = Path(DATA_PATH)\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if USE_3D:\n",
    "    encoder_type = \"swin_unetr\"\n",
    "else:\n",
    "    encoder_type = \"clipseg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai import transforms\n",
    "from monai.data import MetaTensor\n",
    "\n",
    "\n",
    "def update_metrics(name, new_metrics):\n",
    "    metrics_file = OUTPUTS_PATH / \"metrics.json\"\n",
    "\n",
    "    if not metrics_file.exists():\n",
    "        metrics = {}\n",
    "    else:\n",
    "        with open(metrics_file, \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "    metrics[name] = new_metrics\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "\n",
    "def debug_metadata(data):\n",
    "    \"\"\"Debug transform to print metadata information\"\"\"\n",
    "    print(f\"üîç DEBUG - Data type: {type(data)}\")\n",
    "    if hasattr(data, \"meta\"):\n",
    "        print(\n",
    "            f\"üîç DEBUG - Metadata keys: {list(data.meta.keys()) if data.meta else 'No meta'}\"\n",
    "        )\n",
    "        print(f\"üîç DEBUG - Full metadata: {data.meta}\")\n",
    "    if hasattr(data, \"shape\"):\n",
    "        print(f\"üîç DEBUG - Shape: {data.shape}\")\n",
    "    if hasattr(data, \"dtype\"):\n",
    "        print(f\"üîç DEBUG - Dtype: {data.dtype}\")\n",
    "    print(\"üîç DEBUG - \" + \"=\" * 50)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "class MetadataAwareTransform:\n",
    "    \"\"\"Wrapper to make transforms work with 4-element tuples\"\"\"\n",
    "\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, data_tuple):\n",
    "        if isinstance(data_tuple, tuple) and len(data_tuple) == 4:\n",
    "            image, label, dict_data, metadata = data_tuple\n",
    "            \n",
    "            # Apply transform only to the image\n",
    "            try:\n",
    "                if isinstance(metadata, dict):\n",
    "                    wrapped = MetaTensor(image, meta=metadata)\n",
    "                else:\n",
    "                    wrapped = image\n",
    "                    \n",
    "                transformed_image = self.transform(wrapped)\n",
    "                \n",
    "                # Extract clean metadata if it's a MetaTensor\n",
    "                if isinstance(transformed_image, MetaTensor):\n",
    "                    clean_metadata = dict(transformed_image.meta) if transformed_image.meta else {}\n",
    "                    # Keep only simple, serializable values\n",
    "                    filtered_metadata = {}\n",
    "                    for k, v in clean_metadata.items():\n",
    "                        if isinstance(v, (str, int, float, bool)):\n",
    "                            filtered_metadata[k] = v\n",
    "                    return transformed_image, label, dict_data, filtered_metadata\n",
    "                \n",
    "                return transformed_image, label, dict_data, metadata\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in transform {self.transform}: {e}\")\n",
    "                return image, label, dict_data, metadata\n",
    "                \n",
    "        elif isinstance(data_tuple, tuple) and len(data_tuple) == 2:\n",
    "            # Fallback for 2-element tuples\n",
    "            data, metadata = data_tuple\n",
    "            try:\n",
    "                if isinstance(metadata, dict):\n",
    "                    wrapped = MetaTensor(data, meta=metadata)\n",
    "                else:\n",
    "                    wrapped = data\n",
    "                out = self.transform(wrapped)\n",
    "                if isinstance(out, MetaTensor):\n",
    "                    clean_metadata = dict(out.meta) if out.meta else {}\n",
    "                    return out, clean_metadata\n",
    "                return out, metadata\n",
    "            except Exception as e:\n",
    "                print(f\"Error in transform: {e}\")\n",
    "                return data, metadata\n",
    "        else:\n",
    "            # Fallback for non-tuple input\n",
    "            return self.transform(data_tuple)\n",
    "\n",
    "\n",
    "class MetadataCompose:\n",
    "    \"\"\"Custom Compose that handles 4-element tuples properly\"\"\"\n",
    "\n",
    "    def __init__(self, transforms_list):\n",
    "        self.transforms_list = transforms_list\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Apply transforms sequentially, handling 4-element tuples\"\"\"\n",
    "        if len(args) == 4 and not kwargs:\n",
    "            # Handle 4-element tuple: (image, label, dict_data, metadata)\n",
    "            image, label, dict_data, metadata = args\n",
    "            data_tuple = (image, label, dict_data, metadata)\n",
    "        elif len(args) == 2 and not kwargs:\n",
    "            # Handle 2-element tuple: (data, metadata)\n",
    "            data, metadata = args\n",
    "            data_tuple = (data, metadata)\n",
    "        elif len(args) == 1:\n",
    "            data_tuple = args[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected arguments: args={args}, kwargs={kwargs}\")\n",
    "\n",
    "        if isinstance(data_tuple, tuple) and len(data_tuple) == 4:\n",
    "            image, label, dict_data, metadata = data_tuple\n",
    "\n",
    "            # Apply each transform sequentially to the image only\n",
    "            for transform in self.transforms_list:\n",
    "                if isinstance(transform, MetadataAwareTransform):\n",
    "                    image, label, dict_data, metadata = transform((image, label, dict_data, metadata))\n",
    "                else:\n",
    "                    # For regular transforms, just transform the image\n",
    "                    image = transform(image)\n",
    "\n",
    "            return image, label, dict_data, metadata\n",
    "            \n",
    "        elif isinstance(data_tuple, tuple) and len(data_tuple) == 2:\n",
    "            # Handle 2-element tuples\n",
    "            data, metadata = data_tuple\n",
    "            for transform in self.transforms_list:\n",
    "                if isinstance(transform, MetadataAwareTransform):\n",
    "                    data, metadata = transform((data, metadata))\n",
    "                else:\n",
    "                    data = transform(data)\n",
    "            return data, metadata\n",
    "            \n",
    "        else:\n",
    "            # Fallback for non-tuple input\n",
    "            result = data_tuple\n",
    "            for transform in self.transforms_list:\n",
    "                if isinstance(transform, MetadataAwareTransform):\n",
    "                    result = transform.transform(result)\n",
    "                else:\n",
    "                    result = transform(result)\n",
    "            return result\n",
    "\n",
    "    def set_random_state(self, seed=None):\n",
    "        \"\"\"Set random state for randomizable transforms\"\"\"\n",
    "        for transform in self.transforms_list:\n",
    "            if hasattr(transform, \"set_random_state\"):\n",
    "                transform.set_random_state(seed=seed)\n",
    "            elif hasattr(transform, \"transform\") and hasattr(\n",
    "                transform.transform, \"set_random_state\"\n",
    "            ):\n",
    "                transform.transform.set_random_state(seed=seed)\n",
    "\n",
    "def get_preprocessing(dataset_name: str, domain: str, is_training=True):\n",
    "    \"\"\"\n",
    "    Get comprehensive preprocessing pipeline for volumetric medical data.\n",
    "\n",
    "    Returns separate transforms for images and segmentations to work with ImageDataset.\n",
    "    Handles different file formats based on dataset:\n",
    "    - CHAOS: DICOM images (directories), PNG labels (directories)\n",
    "    - MMWHS: NIfTI images and labels\n",
    "\n",
    "    Note: Spatial transforms (Spacing, Resize) are handled separately to ensure\n",
    "    synchronized dimensions between images and labels.\n",
    "    \"\"\"\n",
    "    # Image-specific transforms (applied to image files)\n",
    "\n",
    "    decode_func = get_decode_func(dataset_name, domain)\n",
    "\n",
    "    if USE_3D:\n",
    "        image_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "    else:\n",
    "        image_transforms = [\n",
    "            # transforms.Lambda(lambda x: print(f\"Image: {x.shape}\") or x),\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            # transforms.Orientation(axcodes=\"RA\"),\n",
    "        ]\n",
    "\n",
    "    # Domain-specific intensity normalization for images\n",
    "    if domain == \"CT\":\n",
    "        image_transforms.append(\n",
    "            transforms.ScaleIntensityRange(\n",
    "                a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True\n",
    "            ),\n",
    "        )\n",
    "    else:  # MR\n",
    "        image_transforms.append(\n",
    "            transforms.NormalizeIntensity(nonzero=True, channel_wise=True),\n",
    "        )\n",
    "\n",
    "    # Training-specific augmentations for images only\n",
    "    if is_training:\n",
    "        # Image-only augmentations (safe for ImageDataset)\n",
    "        augmentation_transforms = [\n",
    "            transforms.RandGaussianNoise(prob=0.2, std=0.05),\n",
    "            transforms.RandAdjustContrast(prob=0.2, gamma=(0.9, 1.1)),\n",
    "        ]\n",
    "        image_transforms.extend(augmentation_transforms)\n",
    "\n",
    "    if not USE_3D:\n",
    "        image_transforms.append(transforms.RepeatChannel(repeats=3))\n",
    "\n",
    "    # Final conversion to tensor for images\n",
    "    image_transforms.extend(\n",
    "        [\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE,\n",
    "                size_mode=\"longest\",\n",
    "                mode=\"area\",\n",
    "                anti_aliasing=True,\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Segmentation-specific transforms (applied to segmentation files)\n",
    "    if not USE_3D:\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(\n",
    "                channel_dim=\"no_channel\"\n",
    "            ),  # Ensure channel-first format\n",
    "            # transforms.Orientation(axcodes=\"RA\"),\n",
    "        ]\n",
    "    else:\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: print(f\": {x.shape}\") or x),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "\n",
    "    seg_transforms.extend(\n",
    "        [\n",
    "            transforms.Lambda(lambda x: decode_func(x)),\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE, size_mode=\"longest\", mode=\"nearest\"\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Wrap transforms to handle metadata properly\n",
    "    metadata_aware_image_transforms = [\n",
    "        MetadataAwareTransform(t) for t in image_transforms\n",
    "    ]\n",
    "    metadata_aware_seg_transforms = [MetadataAwareTransform(t) for t in seg_transforms]\n",
    "\n",
    "    # Create separate transform pipelines that handle metadata\n",
    "    image_transform = MetadataCompose(metadata_aware_image_transforms)\n",
    "    seg_transform = MetadataCompose(metadata_aware_seg_transforms)\n",
    "\n",
    "    return image_transform, seg_transform\n",
    "\n",
    "\n",
    "def get_decode_func(dataset_name, domain):\n",
    "    from src.datasets.mmwhs import mmwhs_labels\n",
    "\n",
    "    decode = None\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        if domain in [\"MR\", \"MRI\"]:\n",
    "\n",
    "            def decode(labels):\n",
    "                # Convert intensity values to class indices (keep as float32)\n",
    "                return labels // 63\n",
    "\n",
    "        elif domain == \"CT\":\n",
    "\n",
    "            def decode(labels):\n",
    "                return torch.where(labels > 0, 1.0, 0.0)\n",
    "\n",
    "    elif dataset_name == \"MMWHS\":\n",
    "\n",
    "        def decode(labels):\n",
    "            decoded_labels = torch.zeros_like(labels, dtype=torch.float32)\n",
    "            for i, label_val in enumerate(mmwhs_labels.keys()):\n",
    "                decoded_labels[labels == label_val] = i\n",
    "            return decoded_labels\n",
    "\n",
    "    if decode is None:\n",
    "        print(\n",
    "            f\"Warning: No decode function defined for {dataset_name} in {domain}. Returning labels unchanged.\"\n",
    "        )\n",
    "\n",
    "        def decode(labels):\n",
    "            return labels\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11869e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    import os\n",
    "    import subprocess\n",
    "    from IPython.display import FileLink, display\n",
    "    \n",
    "    def download_file(path, download_file_name):\n",
    "        os.chdir('/kaggle/working/')\n",
    "        zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n",
    "        command = f\"zip {zip_name} {path} -r\"\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(\"Unable to run zip command!\")\n",
    "            print(result.stderr)\n",
    "            return\n",
    "        display(FileLink(f'{download_file_name}.zip'))\n",
    "    \n",
    "    try:\n",
    "        download_file('/kaggle/working/xai/checkpoints/', 'pth')\n",
    "    except:\n",
    "        download_file('/xai/checkpoints/', 'pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned model for CHAOS in MR domain with 2d images already exists at checkpoints\\CHAOS_MR_2d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for CHAOS in CT domain with 2d images already exists at checkpoints\\CHAOS_CT_2d_finetuned.pth. Skipping finetuning.\n",
      "Finetuning on MMWHS dataset in MR domain with 2d images \n",
      "Dataset MR total samples: 2898\n",
      "Split sizes - Train: 2028, Val: 434, Test: 436\n",
      "\n",
      "Dataset: MMWHS, Domain: MR\n",
      "Number of training samples: 2028\n",
      "Number of validation samples: 434\n",
      "Number of test samples: 436\n",
      "Image shape: torch.Size([3, 128, 128])\n",
      "Segmentation shape: torch.Size([1, 128, 128])\n",
      "Number of classes: 8\n",
      "\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "üîÑ Loading CLIPSeg weights...\n",
      "üîß DEBUG: Initial model parameter check\n",
      "   Total parameters: 356\n",
      "   Trainable parameters: 54\n",
      "   Model device: cpu\n",
      "\n",
      "Processing MMWHS in MR domain with 2d images\n",
      "üîß DEBUG: Before unfreeze()\n",
      "   Frozen parameters before unfreeze: 302\n",
      "   Frozen parameters after unfreeze: 0\n",
      "   Total trainable parameters: 356\n",
      "\n",
      "üîß DEBUG: Starting full model finetuning\n",
      "üöÄ Starting training for 1 epochs\n",
      "   Device: cuda\n",
      "   Learning Rate: 0.001\n",
      "   Weight Decay: 1e-05\n",
      "   Params: total=150,796,962, trainable=150,796,962\n",
      "   Batches: train=254, val=55\n",
      "   Tracking params:\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "\n",
      "üìñ Epoch 1/1\n",
      "   LR(s): 1.000000e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 254/254 [44:19<00:00, 10.47s/it, Loss=0.6115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.6405\n",
      "   Param norm deltas (after epoch):\n",
      "     encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight: Œînorm=+0.000000e+00 (before=8.586612e+00, after=8.586612e+00)\n",
      "     encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias: Œînorm=+0.000000e+00 (before=1.183554e+00, after=1.183554e+00)\n",
      "     encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight: Œînorm=+0.000000e+00 (before=8.633878e+00, after=8.633878e+00)\n",
      "   Grad non-zero in batches: 254/254 (100.0%)\n",
      "   Labels seen this epoch: [0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [09:39<00:00, 10.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Loss: 0.6090, Val Dice: 0.4121\n",
      "   ‚úÖ New best Val Dice: 0.4121\n",
      "\n",
      "‚úÖ Training completed!\n",
      "üîß DEBUG: Parameter change analysis after finetuning\n",
      "   Parameter encoder.clipseg.clip_model.positional_embedding didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.text_projection didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.logit_scale didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.class_embedding didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.positional_embedding didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.proj didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.conv1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.ln_pre.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.ln_pre.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.0.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.1.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.2.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.3.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.4.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.5.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.6.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.7.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.8.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.9.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.ln_post.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.visual.ln_post.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.0.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.1.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.2.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.3.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.4.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.5.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.6.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.7.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.8.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.9.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.10.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.attn.in_proj_weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.attn.in_proj_bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.attn.out_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.attn.out_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.ln_1.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.ln_1.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.mlp.c_fc.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.mlp.c_fc.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.mlp.c_proj.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.mlp.c_proj.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.ln_2.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.transformer.resblocks.11.ln_2.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.token_embedding.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.ln_final.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.clip_model.ln_final.bias didn't change during training!\n",
      "   Parameter encoder.clipseg.reduce.weight didn't change during training!\n",
      "   Parameter encoder.clipseg.reduce.bias didn't change during training!\n",
      "   Parameters that changed: 52\n",
      "   Parameters that didn't change: 304\n",
      "   Maximum parameter change: 8.932339\n",
      "   ‚úÖ Parameters updated successfully\n",
      "\n",
      "üîß DEBUG: Training history analysis\n",
      "   Training losses: [0.6405130178909603]...[0.6405130178909603]\n",
      "   Loss range: 0.640513 - 0.640513\n",
      "   History keys: ['train_loss', 'val_loss', 'val_dice']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train:   0%|          | 0/254 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in TRAINING_EPOCHS.items():\n",
    "    download_and_extract_dataset(dataset_name, DATA_PATH)\n",
    "\n",
    "    image_transform, seg_transform = get_preprocessing(\n",
    "        dataset_name, domain, is_training=True\n",
    "    )\n",
    "\n",
    "    filename = f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned.pth\"\n",
    "    filename = CHECKPOINT_PATH / filename\n",
    "    # Check if the finetuned checkpoint already exists\n",
    "    if filename.exists():\n",
    "        print(\n",
    "            f\"Finetuned model for {dataset_name} in {domain} domain with {'3d' if USE_3D else '2d'} images already exists at {filename}. Skipping finetuning.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        transform=image_transform,  # Use transform instead of preprocess\n",
    "        seg_transform=seg_transform,  # Pass seg_transform too\n",
    "        base_path=DATA_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        slice_2d=not USE_3D,\n",
    "    )\n",
    "    \n",
    "\n",
    "    #  Ensure the dataset is loaded correctly\n",
    "    if not isinstance(dataset, BaseDataset):\n",
    "        raise TypeError(\n",
    "            f\"Expected dataset to be an instance of BaseDataset, got {type(dataset)}\"\n",
    "        )\n",
    "    # Print dataset information\n",
    "    print()\n",
    "    print(f\"Dataset: {dataset_name}, Domain: {domain}\")\n",
    "    print(f\"Number of training samples: {len(dataset.train_dataset)}\")\n",
    "    print(f\"Number of validation samples: {len(dataset.val_dataset)}\")\n",
    "    print(f\"Number of test samples: {len(dataset.test_dataset)}\")\n",
    "    print(f\"Image shape: {dataset.train_dataset[0][0].shape}\")\n",
    "    print(f\"Segmentation shape: {dataset.train_dataset[0][1].shape}\")\n",
    "    print(f\"Number of classes: {dataset.num_classes}\")\n",
    "    print()\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=encoder_type,\n",
    "    )\n",
    "\n",
    "    # üîß DEBUG: Check initial model parameters\n",
    "    print(\"üîß DEBUG: Initial model parameter check\")\n",
    "    initial_params = {}\n",
    "    param_count = 0\n",
    "    trainable_count = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        initial_params[name] = param.clone().detach()\n",
    "        param_count += 1\n",
    "        if param.requires_grad:\n",
    "            trainable_count += 1\n",
    "    print(f\"   Total parameters: {param_count}\")\n",
    "    print(f\"   Trainable parameters: {trainable_count}\")\n",
    "    print(f\"   Model device: {next(model.parameters()).device}\")\n",
    "    print()\n",
    "\n",
    "    # Save the baseline model's state_dict before finetuning\n",
    "    baseline_filename = (\n",
    "        CHECKPOINT_PATH\n",
    "        / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "    )\n",
    "    torch.save(model.encoder, baseline_filename)\n",
    "    print(\n",
    "        f\"Processing {dataset_name} in {domain} domain with {'3d' if USE_3D else '2d'} images\"\n",
    "    )\n",
    "\n",
    "    if USE_3D:\n",
    "        print(\n",
    "            f\"Warning: Using 3D model requires SWIN UNETR, which is not compatible with zero-shot training.\"\n",
    "        )\n",
    "        \n",
    "        # üîß DEBUG: Check freeze_body functionality\n",
    "        print(\"üîß DEBUG: Before freeze_body()\")\n",
    "        frozen_before = sum(1 for p in model.parameters() if not p.requires_grad)\n",
    "        model.freeze_body()\n",
    "        frozen_after = sum(1 for p in model.parameters() if not p.requires_grad)\n",
    "        print(f\"   Frozen parameters before: {frozen_before}\")\n",
    "        print(f\"   Frozen parameters after: {frozen_after}\")\n",
    "        print(f\"   Parameters frozen: {frozen_after - frozen_before}\")\n",
    "        \n",
    "        # Check which parameters are trainable\n",
    "        print(\"   Trainable parameters after freeze_body:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"     {name}: {param.shape}\")\n",
    "        print()\n",
    "        \n",
    "        model.finetune(\n",
    "            epochs=epochs, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "        metrics = model.evaluate()\n",
    "        update_metrics(\n",
    "            f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_head\",\n",
    "            metrics,\n",
    "        )\n",
    "\n",
    "    # üîß DEBUG: Check unfreeze functionality\n",
    "    print(\"üîß DEBUG: Before unfreeze()\")\n",
    "    frozen_before = sum(1 for p in model.parameters() if not p.requires_grad)\n",
    "    model.unfreeze()\n",
    "    frozen_after = sum(1 for p in model.parameters() if not p.requires_grad)\n",
    "    print(f\"   Frozen parameters before unfreeze: {frozen_before}\")\n",
    "    print(f\"   Frozen parameters after unfreeze: {frozen_after}\")\n",
    "    print(f\"   Total trainable parameters: {sum(1 for p in model.parameters() if p.requires_grad)}\")\n",
    "    print()\n",
    "\n",
    "    # üîß DEBUG: Monitor parameter changes during training\n",
    "    print(\"üîß DEBUG: Starting full model finetuning\")\n",
    "    history = model.finetune(\n",
    "        epochs=epochs,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "    \n",
    "    # üîß DEBUG: Check if parameters actually changed\n",
    "    print(\"üîß DEBUG: Parameter change analysis after finetuning\")\n",
    "    changed_params = 0\n",
    "    unchanged_params = 0\n",
    "    max_change = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in initial_params:\n",
    "            param_cuda = param.to(\"cuda\")\n",
    "            initial_param_cuda = initial_params[name].to(\"cuda\")\n",
    "            param_change = (param_cuda - initial_param_cuda).norm().item()\n",
    "            if param_change > 1e-8:  # Consider very small changes as unchanged\n",
    "                changed_params += 1\n",
    "                max_change = max(max_change, param_change)\n",
    "            else:\n",
    "                unchanged_params += 1\n",
    "                print(f\"   Parameter {name} didn't change during training!\")\n",
    "    \n",
    "    print(f\"   Parameters that changed: {changed_params}\")\n",
    "    print(f\"   Parameters that didn't change: {unchanged_params}\")\n",
    "    print(f\"   Maximum parameter change: {max_change:.6f}\")\n",
    "    \n",
    "    if changed_params == 0:\n",
    "        print(\"   ‚ö†Ô∏è WARNING: No parameters changed during training!\")\n",
    "    elif max_change < 1e-6:\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: Very small parameter changes (max: {max_change:.8f})\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Parameters updated successfully\")\n",
    "    print()\n",
    "\n",
    "    # üîß DEBUG: Check training history\n",
    "    if history:\n",
    "        print(\"üîß DEBUG: Training history analysis\")\n",
    "        if 'train_loss' in history:\n",
    "            train_losses = history['train_loss']\n",
    "            print(f\"   Training losses: {train_losses[:5]}...{train_losses[-5:] if len(train_losses) > 5 else train_losses}\")\n",
    "            print(f\"   Loss range: {min(train_losses):.6f} - {max(train_losses):.6f}\")\n",
    "            if len(train_losses) > 1:\n",
    "                loss_change = abs(train_losses[-1] - train_losses[0])\n",
    "                print(f\"   Total loss change: {loss_change:.6f}\")\n",
    "                if loss_change < 1e-6:\n",
    "                    print(\"   ‚ö†Ô∏è WARNING: Training loss barely changed!\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No 'train_loss' found in history\")\n",
    "        print(f\"   History keys: {list(history.keys()) if history else 'None'}\")\n",
    "    else:\n",
    "        print(\"üîß DEBUG: No training history returned\")\n",
    "    print()\n",
    "\n",
    "    # Save the finetuned model's state_dict\n",
    "    torch.save(model.encoder, filename)\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned\",\n",
    "        model_metrics,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090233",
   "metadata": {},
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWIN UNETR Task Vectors\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from monai.networks.nets.swin_unetr import BasicLayer\n",
    "from monai.networks.nets.swin_unetr import SwinTransformerBlock\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from monai.networks.nets.swin_unetr import WindowAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules.linear import Identity\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from torch.nn.modules.activation import GELU\n",
    "from monai.networks.nets.swin_unetr import PatchMerging\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from monai.networks.blocks.unetr_block import UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from torch.nn.modules.conv import ConvTranspose3d\n",
    "\n",
    "safe_globals = [\n",
    "    SwinUNETR,\n",
    "    SwinTransformer,\n",
    "    PatchEmbed,\n",
    "    Conv3d,\n",
    "    Dropout,\n",
    "    ModuleList,\n",
    "    BasicLayer,\n",
    "    SwinTransformerBlock,\n",
    "    LayerNorm,\n",
    "    WindowAttention,\n",
    "    Linear,\n",
    "    Softmax,\n",
    "    Identity,\n",
    "    MLPBlock,\n",
    "    GELU,\n",
    "    PatchMerging,\n",
    "    UnetrBasicBlock,\n",
    "    UnetResBlock,\n",
    "    Convolution,\n",
    "    LeakyReLU,\n",
    "    InstanceNorm3d,\n",
    "    UnetrUpBlock,\n",
    "    ConvTranspose3d,\n",
    "    UnetOutBlock,\n",
    "]\n",
    "##\n",
    "\n",
    "## CLIPSeg Task Vectors\n",
    "from src.CLIPSeg import CLIPSeg\n",
    "from clipseg.clipseg import CLIPDensePredT\n",
    "from clip.model import (\n",
    "    CLIP,\n",
    "    VisionTransformer,\n",
    "    LayerNorm,\n",
    "    Transformer,\n",
    "    ResidualAttentionBlock,\n",
    "    QuickGELU,\n",
    ")\n",
    "from torch.nn.modules.conv import Conv2d, ConvTranspose2d\n",
    "from torch.nn.modules.container import Sequential\n",
    "from torch.nn.modules.activation import MultiheadAttention, ReLU\n",
    "from torch.nn.modules.linear import NonDynamicallyQuantizableLinear\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.nn.modules.transformer import (\n",
    "    TransformerEncoderLayer,\n",
    "    TransformerEncoder,\n",
    "    TransformerDecoderLayer,\n",
    "    TransformerDecoder,\n",
    ")\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn.modules.container import ModuleDict\n",
    "\n",
    "safe_globals.extend(\n",
    "    [\n",
    "        CLIPSeg,\n",
    "        CLIPDensePredT,\n",
    "        CLIP,\n",
    "        VisionTransformer,\n",
    "        Conv2d,\n",
    "        LayerNorm,\n",
    "        Transformer,\n",
    "        Sequential,\n",
    "        ResidualAttentionBlock,\n",
    "        MultiheadAttention,\n",
    "        NonDynamicallyQuantizableLinear,\n",
    "        QuickGELU,\n",
    "        Embedding,\n",
    "        ReLU,\n",
    "        ConvTranspose2d,\n",
    "        TransformerEncoderLayer,\n",
    "        TransformerEncoder,\n",
    "        TransformerDecoderLayer,\n",
    "        TransformerDecoder,\n",
    "        relu,\n",
    "        ModuleDict,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for domain in DOMAINS:\n",
    "        print(\n",
    "            f\"Building task vector for {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images\"\n",
    "        )\n",
    "        baseline_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        finetuned_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned.pth\"\n",
    "        )\n",
    "        if not baseline_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Baseline checkpoint for {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "        if not finetuned_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Finetuned checkpoint {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        with torch.serialization.safe_globals(\n",
    "            safe_globals=safe_globals,\n",
    "        ):\n",
    "            task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "            # Remove keys associated with the output layers from the task vector\n",
    "            # For swin it's all layers starting with '.out'\n",
    "            # For clipseg it might not be necessary since the model architecture isn't dependent on the number of output features\n",
    "            if encoder_type == \"swin_unetr\":\n",
    "                for k in task_vector.keys():\n",
    "                    if k.startswith(\".out\"):\n",
    "                        del task_vector[k]\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS_CT\": task_vectors[\"MMWHS_MR\"]\n",
    "    + task_vectors[\"CHAOS_CT\"]\n",
    "    - task_vectors[\"CHAOS_MR\"],\n",
    "    \"MMWHS_MR\": task_vectors[\"MMWHS_CT\"]\n",
    "    + task_vectors[\"CHAOS_MR\"]\n",
    "    - task_vectors[\"CHAOS_CT\"],\n",
    "    \"CHAOS_CT\": task_vectors[\"CHAOS_MR\"]\n",
    "    + task_vectors[\"MMWHS_CT\"]\n",
    "    - task_vectors[\"MMWHS_MR\"],\n",
    "    \"CHAOS_MR\": task_vectors[\"CHAOS_CT\"]\n",
    "    + task_vectors[\"MMWHS_MR\"]\n",
    "    - task_vectors[\"MMWHS_CT\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Task Vector Cross-Domain Adaptation Experiments\n",
    "print(\"üîÑ Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for target_domain in DOMAINS:\n",
    "        print(f\"\\n{dataset_name}: {target_domain} adaptation\")\n",
    "\n",
    "        image_transform, seg_transform = get_preprocessing(\n",
    "            dataset_name, domain, is_training=False\n",
    "        )\n",
    "\n",
    "        dataset_kwargs = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"base_path\": DATA_PATH,\n",
    "            \"domain\": target_domain,\n",
    "            \"transform\": image_transform,  # Use transform instead of preprocess\n",
    "            \"seg_transform\": seg_transform,  # Pass seg_transform too\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"num_workers\": 0,\n",
    "            \"slide_2d\": not USE_3D,\n",
    "        }\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\":\n",
    "            extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "        # try:\n",
    "        target_dataset = get_dataset(**dataset_kwargs, **extra_kwargs)\n",
    "\n",
    "        composite_key = f\"{dataset_name}_{target_domain}\"\n",
    "        if composite_key in composite_task_vectors:\n",
    "            composite_task_vector = composite_task_vectors[composite_key]\n",
    "\n",
    "            target_model = target_dataset.get_model(encoder_type=encoder_type)\n",
    "            target_model.load_task_vector(composite_task_vector)\n",
    "\n",
    "            metrics = target_model.evaluate()\n",
    "            update_metrics(f\"{composite_key}_adaptation\", metrics)\n",
    "            print(\n",
    "                f\"   ‚úÖ {composite_key}: Dice={metrics.get('train', {}).get('dice', 0):.3f}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No composite task vector found for {composite_key}\")\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"   ‚ùå {dataset_name} {target_domain}: {str(e)[:100]}...\")\n",
    "        #     import traceback\n",
    "        #     traceback.print_exc()\n",
    "        #     # continue\n",
    "        #     break\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display all metrics\n",
    "metrics_file = OUTPUTS_PATH / \"metrics.json\"\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        all_metrics = json.load(f)\n",
    "\n",
    "    print(\"\\nüìä COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline performance\n",
    "    print(\"\\nüèÅ Baseline Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"baseline\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # After Head-training performance\n",
    "    print(\"\\nüèãÔ∏è‚Äç‚ôÇÔ∏è After Head-Training Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"head\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # Finetuned performance\n",
    "    print(\"\\nüèÜ Finetuned Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"finetuned\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # Cross-domain adaptation results\n",
    "    print(\"\\nüîÑ Cross-Domain Adaptation Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"adaptation\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "else:\n",
    "    print(\"No metrics file found. Run the experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "\n",
    "    # Copy checkpoints.zip to Google Drive\n",
    "    !zip -r /content/checkpoints.zip /content/xai/checkpoints\n",
    "    shutil.copy(\n",
    "        \"/content/checkpoints.zip\", \"/content/drive/MyDrive/xai/checkpoints.zip\"\n",
    "    )\n",
    "\n",
    "    # Copy metrics.json to Google Drive\n",
    "    shutil.copy(\n",
    "        \"/content/xai/outputs/metrics.json\", \"/content/drive/MyDrive/xai/metrics.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    !zip -r /kaggle/working/checkpoints.zip /kaggle/working/xai/checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
