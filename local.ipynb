{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0bb3b4",
   "metadata": {},
   "source": [
    "In Kaggle, add the following to the dependencies:\n",
    "```\n",
    "pip install torch\n",
    "pip install torchvision\n",
    "pip install numpy\n",
    "pip install pydicom\n",
    "pip install PILlow\n",
    "pip install matplotlib\n",
    "```\n",
    "Enable file persistence and internet access.\n",
    "Remember that you can run the whole notebook and close the runtime without wasting resources by going to File > Save Version > Save & Run All (Double check that GPU is selected in the advanced settings).\n",
    "Later, by going to 'File' > 'Version history' you can view the full logs and download the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c8f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle\n",
    "import os\n",
    "\n",
    "IN_KAGGLE = False\n",
    "if os.environ.get(\"KAGGLE_URL_BASE\", \"\"):\n",
    "    IN_KAGGLE = True\n",
    "    !git clone https://github.com/parmigggiana/xai /kaggle/working/xai\n",
    "    %cd xai\n",
    "    !git fetch\n",
    "    !git reset --hard origin/main\n",
    "    %pip install 'napari[pyqt6,optional]==0.6.2a1' 'monai[einops,nibabel]>=1.1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c6af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "IN_COLAB = False\n",
    "if not IN_KAGGLE:\n",
    "    try:\n",
    "        import google.colab\n",
    "\n",
    "        IN_COLAB = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if IN_COLAB:\n",
    "        !git clone https://github.com/parmigggiana/xai /content/xai\n",
    "        %cd /content/xai\n",
    "        !git fetch\n",
    "        !git reset --hard origin/main\n",
    "        %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_vector import TaskVector\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"CHAOS\", \"MMWHS\"]\n",
    "domains = [\"MR\", \"CT\"]\n",
    "data_path = \"data/\"\n",
    "checkpoint_path = \"checkpoints/\"\n",
    "outputs_path = \"outputs/\"\n",
    "use_3d = True\n",
    "training_epochs = {\n",
    "    (\"CHAOS\", \"MR\"): 30,\n",
    "    (\"CHAOS\", \"CT\"): 15,\n",
    "    (\"MMWHS\", \"MR\"): 35,\n",
    "    (\"MMWHS\", \"CT\"): 20,\n",
    "}\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(checkpoint_path)\n",
    "outputs_path = Path(outputs_path)\n",
    "data_path = Path(data_path)\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "outputs_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def update_metrics(name, new_metrics):\n",
    "    metrics_file = outputs_path / \"metrics.json\"\n",
    "\n",
    "    if not metrics_file.exists():\n",
    "        metrics = {}\n",
    "    else:\n",
    "        with open(metrics_file, \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "    metrics[name] = new_metrics\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "\n",
    "def get_preprocessing(domain):\n",
    "    if use_3d:\n",
    "\n",
    "        def preprocess(x):\n",
    "            # Only scale down to (96, 128, 128) if larger\n",
    "            target_shape = (96, 128, 128)\n",
    "            img = x[\"image\"].float()\n",
    "            lbl = x[\"label\"].float() if x[\"label\"] is not None else None\n",
    "\n",
    "            if img.shape[-3:] != target_shape:\n",
    "                img = torch.nn.functional.interpolate(\n",
    "                    img.unsqueeze(0),\n",
    "                    size=target_shape,\n",
    "                    mode=\"trilinear\",\n",
    "                    align_corners=False,\n",
    "                ).squeeze(0)\n",
    "                if lbl is not None:\n",
    "                    lbl = (\n",
    "                        torch.nn.functional.interpolate(\n",
    "                            lbl.unsqueeze(0), size=target_shape, mode=\"nearest\"\n",
    "                        )\n",
    "                        .squeeze(0)\n",
    "                        .long()\n",
    "                    )\n",
    "            else:\n",
    "                if lbl is not None:\n",
    "                    lbl = lbl.long()\n",
    "\n",
    "            return {\"image\": img, \"label\": lbl}\n",
    "\n",
    "    else:\n",
    "        preprocess = torch.nn.Identity\n",
    "\n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned model for CHAOS in MR domain with 3d images already exists at checkpoints/CHAOS_MR_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for CHAOS in CT domain with 3d images already exists at checkpoints/CHAOS_CT_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for MMWHS in MR domain with 3d images already exists at checkpoints/MMWHS_MR_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for MMWHS in CT domain with 3d images already exists at checkpoints/MMWHS_CT_3d_finetuned.pth. Skipping finetuning.\n"
     ]
    }
   ],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in training_epochs.items():\n",
    "    download_and_extract_dataset(dataset_name, data_path)\n",
    "    preprocess = get_preprocessing(domain)\n",
    "\n",
    "    filename = f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned.pth\"\n",
    "    filename = checkpoint_path / filename\n",
    "    # Check if the finetuned checkpoint already exists\n",
    "    if filename.exists():\n",
    "        print(\n",
    "            f\"Finetuned model for {dataset_name} in {domain} domain with {'3d' if use_3d else '2d'} images already exists at {filename}. Skipping finetuning.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if use_3d else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        preprocess=preprocess,\n",
    "        base_path=data_path,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        slice_2d=not use_3d,\n",
    "    )\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=\"swin_unetr\",\n",
    "    )\n",
    "\n",
    "    # Save the baseline model's state_dict before finetuning\n",
    "    baseline_filename = (\n",
    "        checkpoint_path\n",
    "        / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline.pth\"\n",
    "    )\n",
    "    torch.save(model.encoder, baseline_filename)\n",
    "    print(\n",
    "        f\"Processing {dataset_name} in {domain} domain with {'3d' if use_3d else '2d'} images\"\n",
    "    )\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline\",\n",
    "        model_metrics,\n",
    "    )\n",
    "\n",
    "    history = model.finetune(\n",
    "        epochs=epochs,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "    # Save the finetuned model's state_dict\n",
    "\n",
    "    torch.save(model.encoder, filename)\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned\",\n",
    "        model_metrics,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090233",
   "metadata": {},
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f1df630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector for CHAOS dataset in MR domain with 3d images\n",
      "Building task vector for CHAOS dataset in CT domain with 3d images\n",
      "Building task vector for MMWHS dataset in MR domain with 3d images\n",
      "Building task vector for MMWHS dataset in CT domain with 3d images\n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from monai.networks.nets.swin_unetr import BasicLayer\n",
    "from monai.networks.nets.swin_unetr import SwinTransformerBlock\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from monai.networks.nets.swin_unetr import WindowAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules.linear import Identity\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from torch.nn.modules.activation import GELU\n",
    "from monai.networks.nets.swin_unetr import PatchMerging\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from monai.networks.blocks.unetr_block import UnetrUpBlock\n",
    "from torch.nn.modules.conv import ConvTranspose3d\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "\n",
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in dataset_names:\n",
    "    for domain in domains:\n",
    "        print(\n",
    "            f\"Building task vector for {dataset_name} dataset in {domain} domain with {'3d' if use_3d else '2d'} images\"\n",
    "        )\n",
    "        baseline_checkpoint = (\n",
    "            checkpoint_path\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        finetuned_checkpoint = (\n",
    "            checkpoint_path\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned.pth\"\n",
    "        )\n",
    "        with torch.serialization.safe_globals(\n",
    "            [\n",
    "                SwinUNETR,\n",
    "                SwinTransformer,\n",
    "                PatchEmbed,\n",
    "                Conv3d,\n",
    "                Dropout,\n",
    "                ModuleList,\n",
    "                BasicLayer,\n",
    "                SwinTransformerBlock,\n",
    "                LayerNorm,\n",
    "                WindowAttention,\n",
    "                Linear,\n",
    "                Softmax,\n",
    "                Identity,\n",
    "                MLPBlock,\n",
    "                GELU,\n",
    "                PatchMerging,\n",
    "                UnetrBasicBlock,\n",
    "                UnetResBlock,\n",
    "                Convolution,\n",
    "                LeakyReLU,\n",
    "                InstanceNorm3d,\n",
    "                UnetrUpBlock,\n",
    "                ConvTranspose3d,\n",
    "                UnetOutBlock,\n",
    "            ]\n",
    "        ):\n",
    "            task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Task Vector Cross-Domain Adaptation Experiments\n",
      "================================================================================\n",
      "\n",
      "CHAOS: MR ‚Üí CT adaptation\n",
      "2025-07-20 22:12:35,112 - INFO - Expected md5 is None, skip md5 check for file data/ssl_pretrained_weights.pth.\n",
      "2025-07-20 22:12:35,112 - INFO - File exists: data/ssl_pretrained_weights.pth, skipped downloading.\n",
      "Total updated layers 159 / 159\n",
      "Pretrained Weights Succesfully Loaded !\n",
      "üîç Evaluating train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Task Vector Cross-Domain Evaluation (merged version, with nested loops for all configs)\n",
    "print(\"\\nüîÑ Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    for source_domain in domains:\n",
    "        for target_domain in domains:\n",
    "            if source_domain == target_domain:\n",
    "                continue\n",
    "            key = (dataset_name, source_domain, target_domain)\n",
    "            metrics_key = f\"{dataset_name}_{target_domain}_from_{source_domain}\"\n",
    "            # Only add extra_kwarg 'liver_only' for CHAOS dataset\n",
    "            extra_kwargs = {}\n",
    "            if dataset_name == \"CHAOS\":\n",
    "                extra_kwargs[\"liver_only\"] = True\n",
    "            try:\n",
    "                task_vector_key = f\"{dataset_name}_{source_domain}\"\n",
    "                if task_vector_key in task_vectors:\n",
    "                    print(\n",
    "                        f\"\\n{dataset_name}: {source_domain} ‚Üí {target_domain} adaptation\"\n",
    "                    )\n",
    "                    # Load target domain dataset\n",
    "                    dataset_kwargs = dict(\n",
    "                        dataset_name=dataset_name,\n",
    "                        domain=target_domain,\n",
    "                        base_path=data_path,\n",
    "                        batch_size=1,\n",
    "                        num_workers=1,\n",
    "                        slice_2d=False,\n",
    "                    )\n",
    "                    dataset_kwargs.update(extra_kwargs)\n",
    "                    target_dataset = get_dataset(**dataset_kwargs)\n",
    "                    target_model = target_dataset.get_model(encoder_type=\"swin_unetr\")\n",
    "\n",
    "                    # Apply task vector from source domain\n",
    "                    source_task_vector = task_vectors[task_vector_key]\n",
    "                    target_model.load_task_vector(source_task_vector)\n",
    "\n",
    "                    # Evaluate cross-domain performance\n",
    "                    cross_domain_metrics = target_model.evaluate()\n",
    "                    update_metrics(metrics_key, cross_domain_metrics)\n",
    "\n",
    "                    print(\n",
    "                        f\"   ‚úÖ {source_domain}‚Üí{target_domain}: Dice={cross_domain_metrics.get('dice', 0):.3f}\"\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"   üìä Hausdorff Distance: {cross_domain_metrics.get('hausdorff', 0):.3f}\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"   ‚ùå {dataset_name} {source_domain}‚Üí{target_domain} error: {e}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display all metrics\n",
    "metrics_file = outputs_path / \"metrics.json\"\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        all_metrics = json.load(f)\n",
    "\n",
    "    print(\"\\nüìä COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline performance\n",
    "    print(\"\\nüèÅ Baseline Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"baseline\" in key:\n",
    "            dice = metrics.get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # Cross-domain adaptation results\n",
    "    print(\"\\nüîÑ Cross-Domain Adaptation Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"from\" in key:\n",
    "            dice = metrics.get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "else:\n",
    "    print(\"No metrics file found. Run the experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files, runtime\n",
    "\n",
    "    !zip -r /content/checkpoints.zip /content/xai/checkpoints\n",
    "    files.download(\"/content/checkpoints.zip\")\n",
    "    files.download(\"/content/xai/outputs/metrics.json\")\n",
    "    runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    from IPython.display import FileLink\n",
    "\n",
    "    !zip -r /kaggle/working/checkpoints.zip /kaggle/working/xai/checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
