{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0bb3b4",
   "metadata": {},
   "source": [
    "In Kaggle, \n",
    "Enable file persistence and internet access.\n",
    "Remember that you can run the whole notebook and close the runtime without wasting resources by going to File > Save Version > Save & Run All (Double check that GPU is selected in the advanced settings).\n",
    "Later, by going to 'File' > 'Version history' you can view the full logs and download the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c8f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle\n",
    "import os\n",
    "\n",
    "IN_KAGGLE = False\n",
    "if os.environ.get(\"KAGGLE_URL_BASE\", \"\"):\n",
    "    IN_KAGGLE = True\n",
    "    !git clone https://github.com/parmigggiana/xai /kaggle/working/xai\n",
    "    %cd xai\n",
    "    !git fetch\n",
    "    !git reset --hard origin/main\n",
    "    %pip install 'monai[einops,itk,nibabel]>=1.5.0' git+https://github.com/timojl/clipseg.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c6af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "IN_COLAB = False\n",
    "if not IN_KAGGLE:\n",
    "    try:\n",
    "        import google.colab\n",
    "        from google.colab import drive\n",
    "\n",
    "        IN_COLAB = True\n",
    "        import os\n",
    "\n",
    "        drive.mount(\"/content/drive\")\n",
    "        os.makedirs(\"/content/drive/MyDrive/xai\", exist_ok=True)\n",
    "        !git clone https://github.com/parmigggiana/xai /content/xai\n",
    "        %cd /content/xai\n",
    "        !git fetch\n",
    "        !git reset --hard origin/main\n",
    "        %pip install -r requirements.txt\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_vector import TaskVector\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"CHAOS\", \"MMWHS\"]\n",
    "DOMAINS = [\"CT\", \"MR\"]\n",
    "DATA_PATH = \"data/\"\n",
    "CHECKPOINT_PATH = \"checkpoints/\"\n",
    "OUTPUTS_PATH = \"outputs/\"\n",
    "USE_3D = False\n",
    "TRAINING_EPOCHS = {\n",
    "    (\"CHAOS\", \"CT\"): 10,\n",
    "    (\"CHAOS\", \"MR\"): 15,\n",
    "    (\"MMWHS\", \"CT\"): 30,\n",
    "    (\"MMWHS\", \"MR\"): 30,\n",
    "\n",
    "}\n",
    "BATCH_SIZE = 16\n",
    "SPATIAL_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 5e-5\n",
    "# Number of DataLoader workers (set >0 to enable parallel data loading)\n",
    "NUM_WORKERS = 0\n",
    "# Set True to enable debug prints/timers/visualizations)\n",
    "DEBUG = False\n",
    "\n",
    "# Profiling controls: False | 'cprofile' | 'torch'\n",
    "PROFILE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe7c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_MAX_ITEMS = 32  # set the in-memory file cache size per dataset (images and segs)\n",
    "ENABLE_CACHE = True    # set to False to disable caching entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(CHECKPOINT_PATH)\n",
    "OUTPUTS_PATH = Path(OUTPUTS_PATH)\n",
    "DATA_PATH = Path(DATA_PATH)\n",
    "PROFILE_DIR = OUTPUTS_PATH / \"profiling\"\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "PROFILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if USE_3D:\n",
    "    encoder_type = \"swin_unetr\"\n",
    "else:\n",
    "    encoder_type = \"clipseg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai import transforms\n",
    "\n",
    "\n",
    "def update_metrics(name, new_metrics):\n",
    "    metrics_file = OUTPUTS_PATH / \"metrics.json\"\n",
    "\n",
    "    if not metrics_file.exists():\n",
    "        metrics = {}\n",
    "    else:\n",
    "        with open(metrics_file, \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "    metrics[name] = new_metrics\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "\n",
    "# Normalization stats (mean, std) per dataset/domain\n",
    "NORM_STATS = {\n",
    "    (\"MMWHS\", \"MR\"):  (186.5875, 258.5917),\n",
    "    (\"MMWHS\", \"CT\"):  (-745.0086, 1042.7251),\n",
    "    (\"CHAOS\",  \"MR\"): (90.8292, 168.8922),\n",
    "    (\"CHAOS\",  \"CT\"): (-478.1732, 476.7163),\n",
    "}\n",
    "\n",
    "# Optimized preprocessing: resize early\n",
    "\n",
    "def get_preprocessing(dataset_name: str, domain: str, is_training=True):\n",
    "    decode_func = get_decode_func(dataset_name, domain)\n",
    "    mean_std = NORM_STATS.get((dataset_name, domain))\n",
    "    mean, std = (mean_std if mean_std is not None else (None, None))\n",
    "\n",
    "    # Image-specific transforms\n",
    "    if USE_3D:\n",
    "        image_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "    else:\n",
    "        image_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "\n",
    "    # Resize early to reduce compute\n",
    "    image_transforms.append(\n",
    "        transforms.Resize(\n",
    "            spatial_size=SPATIAL_SIZE,\n",
    "            size_mode=\"longest\",\n",
    "            mode=\"area\",\n",
    "            anti_aliasing=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert to tensor and ensure float32 for stable CPU ops\n",
    "    image_transforms.extend([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.EnsureType(dtype=torch.float32),\n",
    "    ])\n",
    "\n",
    "    # Augmentations (training only) — run in float32 on CPU\n",
    "    # if is_training:\n",
    "        # image_transforms.extend(\n",
    "        #     [\n",
    "        #         transforms.RandGaussianNoise(prob=0.15, std=0.05),\n",
    "        #         transforms.RandAdjustContrast(prob=0.15, gamma=(0.95, 1.05)),\n",
    "        #     ]\n",
    "        # )\n",
    "\n",
    "    # Normalize (still in float32)\n",
    "    if mean is not None and std is not None:\n",
    "        image_transforms.append(\n",
    "            transforms.NormalizeIntensity(\n",
    "                subtrahend=float(mean),\n",
    "                divisor=float(std),\n",
    "                channel_wise=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Repeat to 3 channels only at the end (2D only)\n",
    "    if not USE_3D:\n",
    "        image_transforms.append(transforms.RepeatChannel(repeats=3))\n",
    "\n",
    "    image_transform = transforms.Compose(image_transforms)\n",
    "\n",
    "    # Segmentation transforms\n",
    "    if not USE_3D:\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "    else:\n",
    "        seg_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "\n",
    "    seg_transforms.extend(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "            transforms.Lambda(lambda x: decode_func(x)),  # decode after tensor conversion\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE, size_mode=\"longest\", mode=\"nearest\"\n",
    "            ),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    seg_transform = transforms.Compose(seg_transforms)\n",
    "    return image_transform, seg_transform\n",
    "\n",
    "\n",
    "def get_decode_func(dataset_name, domain):\n",
    "    from src.datasets.mmwhs import mmwhs_labels\n",
    "\n",
    "    decode = None\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        if domain in [\"MR\", \"MRI\"]:\n",
    "            def decode(labels):\n",
    "                # Convert intensity values to class indices (keep as float32)\n",
    "                return labels // 63\n",
    "        elif domain == \"CT\":\n",
    "            def decode(labels):\n",
    "                return torch.where(labels > 0, 1.0, 0.0)\n",
    "    elif dataset_name == \"MMWHS\":\n",
    "        def decode(labels):\n",
    "            decoded_labels = torch.zeros_like(labels, dtype=torch.float32)\n",
    "            for i, label_val in enumerate(mmwhs_labels.keys()):\n",
    "                decoded_labels[labels == label_val] = i\n",
    "            return decoded_labels\n",
    "\n",
    "    if decode is None:\n",
    "        def decode(labels):\n",
    "            return labels\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned model for CHAOS in CT domain with 2d images already exists at checkpoints/CHAOS_CT_2d_finetuned.pth. Skipping finetuning.\n",
      "Finetuning on CHAOS dataset in MR domain with 2d images \n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "🔄 Loading CLIPSeg weights...\n",
      "🚀 Starting training for 1 epochs\n",
      "   Device: cpu\n",
      "\n",
      "📖 Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 27/27 [01:35<00:00,  3.55s/it, Loss=1.8640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 1.8974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 6/6 [00:36<00:00,  6.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Loss: 1.8386, Val Dice: 0.1106\n",
      "   New best Val Dice: 0.1106\n",
      "\n",
      "✅ Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train: 100%|██████████| 27/27 [02:00<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train - Dice: 0.1093, Hausdorff: N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating val: 100%|██████████| 6/6 [00:14<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ val - Dice: 0.1106, Hausdorff: N/A\n",
      "Finetuning on MMWHS dataset in CT domain with 2d images \n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "🔄 Loading CLIPSeg weights...\n",
      "🚀 Starting training for 1 epochs\n",
      "   Device: cpu\n",
      "\n",
      "📖 Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▊         | 20/232 [03:29<37:00, 10.47s/it, Loss=2.6084] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     60\u001b[39m     update_metrics(\n\u001b[32m     61\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m3d\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mUSE_3D\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m2d\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_head\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     62\u001b[39m         metrics,\n\u001b[32m     63\u001b[39m     )\n\u001b[32m     65\u001b[39m model.unfreeze()\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_max_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_val_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPROFILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprofile_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPROFILE_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m torch.save(model.encoder, filename)\n\u001b[32m     78\u001b[39m model_metrics = model.evaluate(profile=PROFILE, profile_dir=\u001b[38;5;28mstr\u001b[39m(PROFILE_DIR))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/semantic_segmentation.py:492\u001b[39m, in \u001b[36mMedicalSegmenter.finetune\u001b[39m\u001b[34m(self, epochs, learning_rate, weight_decay, save_best, max_grad_norm, visualize_batches, early_stop_patience, profile, profile_dir, debug, compile_model, val_max_batches, fast_val_metrics)\u001b[39m\n\u001b[32m    483\u001b[39m ctx = (\n\u001b[32m    484\u001b[39m     _timer(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _timer\n\u001b[32m    486\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext()\n\u001b[32m    487\u001b[39m )\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[32m    489\u001b[39m     outputs, loss_value, success, dbg = (\n\u001b[32m    490\u001b[39m         \u001b[38;5;28mself\u001b[39m._process_training_batch(\n\u001b[32m    491\u001b[39m             batch,\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m             device,\n\u001b[32m    493\u001b[39m             loss_function,\n\u001b[32m    494\u001b[39m             optimizer,\n\u001b[32m    495\u001b[39m             scaler,\n\u001b[32m    496\u001b[39m             max_grad_norm,\n\u001b[32m    497\u001b[39m             batch_idx,\n\u001b[32m    498\u001b[39m             compute_debug=debug,\n\u001b[32m    499\u001b[39m         )\n\u001b[32m    500\u001b[39m     )\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m    503\u001b[39m     train_losses.append(loss_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/semantic_segmentation.py:851\u001b[39m, in \u001b[36m_process_training_batch\u001b[39m\u001b[34m(self, batch, device, loss_function, optimizer, scaler, max_grad_norm, batch_idx, compute_debug)\u001b[39m\n\u001b[32m    848\u001b[39m     outputs = self.forward(images)\n\u001b[32m    849\u001b[39m     loss = loss_function(outputs, labels)\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m # Debug ogni 20 batch\n\u001b[32m    852\u001b[39m # if batch_idx % 20 == 0:\n\u001b[32m    853\u001b[39m #     print(\n\u001b[32m    854\u001b[39m #         \"labels dtype/min/max:\",\n\u001b[32m    855\u001b[39m #         labels.dtype,\n\u001b[32m    856\u001b[39m #         labels.min().item(),\n\u001b[32m    857\u001b[39m #         labels.max().item(),\n\u001b[32m    858\u001b[39m #     )\n\u001b[32m    859\u001b[39m #     uniq = np.unique(labels.detach().cpu().numpy())\n\u001b[32m    860\u001b[39m #     print(f\"[DEBUG] Batch {batch_idx} - Loss: {loss.item():.6f}\")\n\u001b[32m    861\u001b[39m #     print(f\"[DEBUG] Unique labels: {uniq}\")\n\u001b[32m    862\u001b[39m #     print(\n\u001b[32m    863\u001b[39m #         f\"[DEBUG] Outputs -> mean: {outputs.mean().item():.6f}, std: {outputs.std().item():.6f}\"\n\u001b[32m    864\u001b[39m #     )\n\u001b[32m    865\u001b[39m #     print(\"Unique labels in batch:\", torch.unique(labels))\n\u001b[32m    866\u001b[39m #     # Convert to class indices for compact debug (choose argmax for multi-class)\n\u001b[32m    867\u001b[39m #     try:\n\u001b[32m    868\u001b[39m #         preds_idx = torch.argmax(outputs, dim=1, keepdim=False)\n\u001b[32m    869\u001b[39m #         print(\n\u001b[32m    870\u001b[39m #             \"Unique prediction classes in batch:\", torch.unique(preds_idx)\n\u001b[32m    871\u001b[39m #         )\n\u001b[32m    872\u001b[39m #     except Exception:\n\u001b[32m    873\u001b[39m #         # Fallback: show summary stats if argmax not applicable\n\u001b[32m    874\u001b[39m #         print(\"Unique predictions (summary):\", torch.unique(outputs))\n\u001b[32m    875\u001b[39m \n\u001b[32m    876\u001b[39m # Backward/step: use GradScaler only for FP16; BF16/FP32 use standard path\n\u001b[32m    877\u001b[39m if (\n\u001b[32m    878\u001b[39m     device.type == \"cuda\"\n\u001b[32m    879\u001b[39m     and getattr(self, \"_amp_dtype\", None) == torch.float16\n\u001b[32m    880\u001b[39m     and scaler is not None\n\u001b[32m    881\u001b[39m ):\n\u001b[32m    882\u001b[39m     scaler.scale(loss).backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/semantic_segmentation.py:246\u001b[39m, in \u001b[36mMedicalSegmenter.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encoder_type == \u001b[33m\"\u001b[39m\u001b[33mswin_unetr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    244\u001b[39m     x, original_shape = \u001b[38;5;28mself\u001b[39m._pad_input_for_swin_unetr(x)\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encoder_type == \u001b[33m\"\u001b[39m\u001b[33mswin_unetr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    249\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._crop_output_to_original_size(result, original_shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/CLIPSeg.py:250\u001b[39m, in \u001b[36mCLIPSeg.forward\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classes:\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.medical_templates \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset_info:\n\u001b[32m    249\u001b[39m         \u001b[38;5;66;03m# Use cached medical prompt embedding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_single_class_with_medical_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    252\u001b[39m         \u001b[38;5;66;03m# Fallback to original CLIPSeg behavior\u001b[39;00m\n\u001b[32m    253\u001b[39m         pred = \u001b[38;5;28mself\u001b[39m.clipseg(image, \u001b[38;5;28mcls\u001b[39m)[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (B, 1, H, W)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/CLIPSeg.py:221\u001b[39m, in \u001b[36mCLIPSeg.predict_single_class_with_medical_prompts\u001b[39m\u001b[34m(self, image, class_name)\u001b[39m\n\u001b[32m    219\u001b[39m avg_embedding = \u001b[38;5;28mself\u001b[39m._get_avg_prompt_embedding(class_name, device=image.device)\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# Run the segmentation forward pass and convert logits to probabilities\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclipseg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditional\u001b[49m\u001b[43m=\u001b[49m\u001b[43mavg_embedding\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m# Resize if needed\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred.shape[\u001b[32m2\u001b[39m:] != image.shape[\u001b[32m2\u001b[39m:]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/clipseg/clipseg.py:362\u001b[39m, in \u001b[36mCLIPDensePredT.forward\u001b[39m\u001b[34m(self, inp_image, conditional, return_features, mask)\u001b[39m\n\u001b[32m    358\u001b[39m bs, dev = inp_image.shape[\u001b[32m0\u001b[39m], x_inp.device\n\u001b[32m    360\u001b[39m cond = \u001b[38;5;28mself\u001b[39m.get_cond_vec(conditional, bs)\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m visual_q, activations, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m activation1 = activations[\u001b[32m0\u001b[39m]\n\u001b[32m    365\u001b[39m activations = activations[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/clipseg/clipseg.py:172\u001b[39m, in \u001b[36mCLIPDenseBase.visual_forward\u001b[39m\u001b[34m(self, x_inp, extract_layers, skip, mask)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     attn_mask = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m x, aff_per_head = \u001b[43mforward_multihead_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_block\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_aff\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m extract_layers:\n\u001b[32m    175\u001b[39m     affinities += [aff_per_head]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/clipseg/clipseg.py:33\u001b[39m, in \u001b[36mforward_multihead_attention\u001b[39m\u001b[34m(x, b, with_aff, attn_mask)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" \u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03mSimplified version of multihead attention (taken from torch source code but without tons of if clauses). \u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03mThe mlp and layer norm come from CLIP.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03mx: input.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03mb: multihead attention module. \u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     32\u001b[39m x_ = b.ln_1(x)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m q, k, v = \u001b[43mnnf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m.chunk(\u001b[32m3\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     34\u001b[39m tgt_len, bsz, embed_dim = q.size()\n\u001b[32m     36\u001b[39m head_dim = embed_dim // b.attn.num_heads\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in TRAINING_EPOCHS.items():\n",
    "    download_and_extract_dataset(dataset_name, DATA_PATH)\n",
    "\n",
    "    image_transform, seg_transform = get_preprocessing(\n",
    "        dataset_name, domain, is_training=True\n",
    "    )\n",
    "\n",
    "    filename = f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned.pth\"\n",
    "    filename = CHECKPOINT_PATH / filename\n",
    "    # Check if the finetuned checkpoint already exists\n",
    "    if filename.exists():\n",
    "        print(\n",
    "            f\"Finetuned model for {dataset_name} in {domain} domain with {'3d' if USE_3D else '2d'} images already exists at {filename}. Skipping finetuning.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        transform=image_transform,  # Use transform instead of preprocess\n",
    "        seg_transform=seg_transform,  # Pass seg_transform too\n",
    "        base_path=DATA_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        slice_2d=not USE_3D,\n",
    "        # new cache knobs\n",
    "        cache_max_items=CACHE_MAX_ITEMS,\n",
    "        enable_cache=ENABLE_CACHE,\n",
    "    )\n",
    "\n",
    "    #  Ensure the dataset is loaded correctly\n",
    "    if not isinstance(dataset, BaseDataset):\n",
    "        raise TypeError(\n",
    "            f\"Expected dataset to be an instance of BaseDataset, got {type(dataset)}\"\n",
    "        )\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=encoder_type,\n",
    "    )\n",
    "\n",
    "    # Save the baseline model's state_dict before finetuning\n",
    "    baseline_filename = (\n",
    "        CHECKPOINT_PATH\n",
    "        / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "    )\n",
    "    torch.save(model.encoder, baseline_filename)\n",
    "\n",
    "    if USE_3D:\n",
    "        model.freeze_body()\n",
    "        model.finetune(\n",
    "            epochs=epochs, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY,\n",
    "            profile=PROFILE, profile_dir=str(PROFILE_DIR)\n",
    "        )\n",
    "        metrics = model.evaluate(profile=PROFILE, profile_dir=str(PROFILE_DIR))\n",
    "        update_metrics(\n",
    "            f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_head\",\n",
    "            metrics,\n",
    "        )\n",
    "\n",
    "    model.unfreeze()\n",
    "\n",
    "    history = model.finetune(\n",
    "        epochs=epochs,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        val_max_batches=16,\n",
    "        fast_val_metrics=True,\n",
    "        profile=PROFILE,\n",
    "        profile_dir=str(PROFILE_DIR),\n",
    "    )\n",
    "\n",
    "    torch.save(model.encoder, filename)\n",
    "    model_metrics = model.evaluate(profile=PROFILE, profile_dir=str(PROFILE_DIR))\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned\",\n",
    "        model_metrics,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090233",
   "metadata": {},
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWIN UNETR Task Vectors\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from monai.networks.nets.swin_unetr import BasicLayer\n",
    "from monai.networks.nets.swin_unetr import SwinTransformerBlock\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from monai.networks.nets.swin_unetr import WindowAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules.linear import Identity\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from torch.nn.modules.activation import GELU\n",
    "from monai.networks.nets.swin_unetr import PatchMerging\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from monai.networks.blocks.unetr_block import UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from torch.nn.modules.conv import ConvTranspose3d\n",
    "\n",
    "safe_globals = [\n",
    "    SwinUNETR,\n",
    "    SwinTransformer,\n",
    "    PatchEmbed,\n",
    "    Conv3d,\n",
    "    Dropout,\n",
    "    ModuleList,\n",
    "    BasicLayer,\n",
    "    SwinTransformerBlock,\n",
    "    LayerNorm,\n",
    "    WindowAttention,\n",
    "    Linear,\n",
    "    Softmax,\n",
    "    Identity,\n",
    "    MLPBlock,\n",
    "    GELU,\n",
    "    PatchMerging,\n",
    "    UnetrBasicBlock,\n",
    "    UnetResBlock,\n",
    "    Convolution,\n",
    "    LeakyReLU,\n",
    "    InstanceNorm3d,\n",
    "    UnetrUpBlock,\n",
    "    ConvTranspose3d,\n",
    "    UnetOutBlock,\n",
    "]\n",
    "##\n",
    "\n",
    "## CLIPSeg Task Vectors\n",
    "from src.CLIPSeg import CLIPSeg\n",
    "from clipseg.clipseg import CLIPDensePredT\n",
    "from clip.model import (\n",
    "    CLIP,\n",
    "    VisionTransformer,\n",
    "    LayerNorm,\n",
    "    Transformer,\n",
    "    ResidualAttentionBlock,\n",
    "    QuickGELU,\n",
    ")\n",
    "from torch.nn.modules.conv import Conv2d, ConvTranspose2d\n",
    "from torch.nn.modules.container import Sequential\n",
    "from torch.nn.modules.activation import MultiheadAttention, ReLU\n",
    "from torch.nn.modules.linear import NonDynamicallyQuantizableLinear\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.nn.modules.transformer import (\n",
    "    TransformerEncoderLayer,\n",
    "    TransformerEncoder,\n",
    "    TransformerDecoderLayer,\n",
    "    TransformerDecoder,\n",
    ")\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn.modules.container import ModuleDict\n",
    "\n",
    "safe_globals.extend(\n",
    "    [\n",
    "        CLIPSeg,\n",
    "        CLIPDensePredT,\n",
    "        CLIP,\n",
    "        VisionTransformer,\n",
    "        Conv2d,\n",
    "        LayerNorm,\n",
    "        Transformer,\n",
    "        Sequential,\n",
    "        ResidualAttentionBlock,\n",
    "        MultiheadAttention,\n",
    "        NonDynamicallyQuantizableLinear,\n",
    "        QuickGELU,\n",
    "        Embedding,\n",
    "        ReLU,\n",
    "        ConvTranspose2d,\n",
    "        TransformerEncoderLayer,\n",
    "        TransformerEncoder,\n",
    "        TransformerDecoderLayer,\n",
    "        TransformerDecoder,\n",
    "        relu,\n",
    "        ModuleDict,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for domain in DOMAINS:\n",
    "        print(\n",
    "            f\"Building task vector for {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images\"\n",
    "        )\n",
    "        baseline_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        finetuned_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned.pth\"\n",
    "        )\n",
    "        if not baseline_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Baseline checkpoint for {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "        if not finetuned_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Finetuned checkpoint {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        with torch.serialization.safe_globals(\n",
    "            safe_globals=safe_globals,\n",
    "        ):\n",
    "            task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "            # Remove keys associated with the output layers from the task vector\n",
    "            # For swin it's all layers starting with '.out'\n",
    "            # For clipseg it might not be necessary since the model architecture isn't dependent on the number of output features\n",
    "            if encoder_type == \"swin_unetr\":\n",
    "                for k in task_vector.keys():\n",
    "                    if k.startswith(\".out\"):\n",
    "                        del task_vector[k]\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe4664",
   "metadata": {},
   "source": [
    "## Part 1: Improve robustness post-hoc with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS\": task_vectors[\"MMWHS_MR\"]\n",
    "    + task_vectors[\"MMWHS_CT\"],\n",
    "    \"CHAOS\": task_vectors[\"CHAOS_MR\"]\n",
    "    + task_vectors[\"CHAOS_CT\"],\n",
    "    \"MR\": task_vectors[\"CHAOS_MR\"]\n",
    "    + task_vectors[\"MMWHS_MR\"],\n",
    "    \"CT\": task_vectors[\"CHAOS_CT\"]\n",
    "    + task_vectors[\"MMWHS_CT\"],\n",
    "}\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task vector simple composition experiments\n",
    "print(\"🔄 Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for target_domain in DOMAINS:\n",
    "        print(f\"\\n{dataset_name}: {target_domain} adaptation\")\n",
    "\n",
    "        image_transform, seg_transform = get_preprocessing(\n",
    "            dataset_name, target_domain, is_training=False\n",
    "        )\n",
    "\n",
    "        dataset_kwargs = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"base_path\": DATA_PATH,\n",
    "            \"domain\": target_domain,\n",
    "            \"transform\": image_transform,  # Use transform instead of preprocess\n",
    "            \"seg_transform\": seg_transform,  # Pass seg_transform too\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"num_workers\": NUM_WORKERS,\n",
    "            \"slice_2d\": not USE_3D,\n",
    "            # pass cache knobs as well\n",
    "            \"cache_max_items\": CACHE_MAX_ITEMS,\n",
    "            \"enable_cache\": ENABLE_CACHE,\n",
    "        }\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\":\n",
    "            extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "        target_dataset = get_dataset(**dataset_kwargs, **extra_kwargs)\n",
    "\n",
    "        composite_task_vector = composite_task_vectors[dataset_name]\n",
    "\n",
    "        target_model = target_dataset.get_model(encoder_type=encoder_type)\n",
    "        target_model.load_task_vector(composite_task_vector, scaling_coef=alpha)\n",
    "\n",
    "        metrics = target_model.evaluate()\n",
    "        update_metrics(f\"{dataset_name}_composite_at_{target_domain}\", metrics)\n",
    "        train_d = metrics.get(\"train\", {}).get(\"dice\", 0)\n",
    "        val_d = metrics.get(\"val\", {}).get(\"dice\")\n",
    "        if val_d is not None:\n",
    "            print(\n",
    "                f\"   ✅ {dataset_name} at {target_domain}: Train Dice={train_d:.3f} | Val Dice={val_d:.3f}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   ✅ {dataset_name} at {target_domain}: Train Dice={train_d:.3f}\")\n",
    "\n",
    "        composite_task_vector = composite_task_vectors[target_domain]\n",
    "\n",
    "        target_model = target_dataset.get_model(encoder_type=encoder_type)\n",
    "        target_model.load_task_vector(composite_task_vector, scaling_coef=alpha)\n",
    "\n",
    "        metrics = target_model.evaluate()\n",
    "        update_metrics(f\"{target_domain}_composite_at_{dataset_name}\", metrics)\n",
    "        train_d = metrics.get(\"train\", {}).get(\"dice\", 0)\n",
    "        val_d = metrics.get(\"val\", {}).get(\"dice\")\n",
    "        if val_d is not None:\n",
    "            print(\n",
    "                f\"   ✅ {target_domain} at {dataset_name}: Train Dice={train_d:.3f} | Val Dice={val_d:.3f}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   ✅ {target_domain} at {dataset_name}: Train Dice={train_d:.3f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0784a95d",
   "metadata": {},
   "source": [
    "## Part 2: Improve robustness post-hoc without data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS_CT\": task_vectors[\"MMWHS_MR\"]\n",
    "    + task_vectors[\"CHAOS_CT\"]\n",
    "    - task_vectors[\"CHAOS_MR\"],\n",
    "    \"MMWHS_MR\": task_vectors[\"MMWHS_CT\"]\n",
    "    + task_vectors[\"CHAOS_MR\"]\n",
    "    - task_vectors[\"CHAOS_CT\"],\n",
    "    \"CHAOS_CT\": task_vectors[\"CHAOS_MR\"]\n",
    "    + task_vectors[\"MMWHS_CT\"]\n",
    "    - task_vectors[\"MMWHS_MR\"],\n",
    "    \"CHAOS_MR\": task_vectors[\"CHAOS_CT\"]\n",
    "    + task_vectors[\"MMWHS_MR\"]\n",
    "    - task_vectors[\"MMWHS_CT\"],\n",
    "}\n",
    "alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 Task Vector Cross-Domain Adaptation Experiments\n",
    "print(\"🔄 Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for target_domain in DOMAINS:\n",
    "        print(f\"\\n{dataset_name}: {target_domain} adaptation\")\n",
    "\n",
    "        image_transform, seg_transform = get_preprocessing(\n",
    "            dataset_name, target_domain, is_training=False\n",
    "        )\n",
    "\n",
    "        dataset_kwargs = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"base_path\": DATA_PATH,\n",
    "            \"domain\": target_domain,\n",
    "            \"transform\": image_transform,  # Use transform instead of preprocess\n",
    "            \"seg_transform\": seg_transform,  # Pass seg_transform too\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"num_workers\": NUM_WORKERS,\n",
    "            \"slice_2d\": not USE_3D,\n",
    "            # pass cache knobs as well\n",
    "            \"cache_max_items\": CACHE_MAX_ITEMS,\n",
    "            \"enable_cache\": ENABLE_CACHE,\n",
    "        }\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\":\n",
    "            extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "        # try:\n",
    "        target_dataset = get_dataset(**dataset_kwargs, **extra_kwargs)\n",
    "\n",
    "        composite_key = f\"{dataset_name}_{target_domain}\"\n",
    "        if composite_key in composite_task_vectors:\n",
    "            composite_task_vector = composite_task_vectors[composite_key]\n",
    "\n",
    "            target_model = target_dataset.get_model(encoder_type=encoder_type)\n",
    "            target_model.load_task_vector(composite_task_vector, scaling_coef=alpha)\n",
    "\n",
    "            metrics = target_model.evaluate()\n",
    "            update_metrics(f\"{composite_key}_adaptation\", metrics)\n",
    "            train_d = metrics.get(\"train\", {}).get(\"dice\", 0)\n",
    "            val_d = metrics.get(\"val\", {}).get(\"dice\")\n",
    "            if val_d is not None:\n",
    "                print(\n",
    "                    f\"   ✅ {composite_key}: Train Dice={train_d:.3f} | Val Dice={val_d:.3f}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"   ✅ {composite_key}: Train Dice={train_d:.3f}\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ No composite task vector found for {composite_key}\")\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"   ❌ {dataset_name} {target_domain}: {str(e)[:100]}...\")\n",
    "        #     import traceback\n",
    "        #     traceback.print_exc()\n",
    "        #     # continue\n",
    "        #     break\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display all metrics\n",
    "metrics_file = OUTPUTS_PATH / \"metrics.json\"\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        all_metrics = json.load(f)\n",
    "\n",
    "    print(\"\\n📊 COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    def fmt_pair(m):\n",
    "        if not isinstance(m, dict):\n",
    "            return \"Dice=N/A\"\n",
    "        t = m.get(\"train\", {})\n",
    "        v = m.get(\"val\", {})\n",
    "        t_d = t.get(\"dice\")\n",
    "        v_d = v.get(\"dice\")\n",
    "        if t_d is not None and v_d is not None:\n",
    "            return f\"Train Dice={t_d:.3f}, Val Dice={v_d:.3f}\"\n",
    "        if t_d is not None:\n",
    "            return f\"Train Dice={t_d:.3f}\"\n",
    "        if v_d is not None:\n",
    "            return f\"Val Dice={v_d:.3f}\"\n",
    "        return \"Dice=N/A\"\n",
    "\n",
    "    # Baseline performance\n",
    "    print(\"\\n\udfc1 Baseline Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"baseline\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # After Head-training performance\n",
    "    print(\"\\n🏋️‍♂️ After Head-Training Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"head\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # Finetuned performance\n",
    "    print(\"\\n🏆 Finetuned Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"finetuned\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # Composite task vector results\n",
    "    print(\"\\n🧩 Composite Task Vector Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"composite_at\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # Dataless adaptation results\n",
    "    print(\"\\n🔄 Dataless Adaptation Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"adaptation\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "else:\n",
    "    print(\"No metrics file found. Run the experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "\n",
    "    # Copy checkpoints.zip to Google Drive\n",
    "    !zip -r /content/checkpoints.zip /content/xai/checkpoints\n",
    "    shutil.copy(\n",
    "        \"/content/checkpoints.zip\", \"/content/drive/MyDrive/xai/checkpoints.zip\"\n",
    "    )\n",
    "\n",
    "    # Copy metrics.json to Google Drive\n",
    "    shutil.copy(\n",
    "        \"/content/xai/outputs/metrics.json\", \"/content/drive/MyDrive/xai/metrics.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    !zip -r /kaggle/working/checkpoints.zip /kaggle/working/xai/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f3841",
   "metadata": {},
   "source": [
    "# Statistiche dei 4 dataset (CHAOS/MMWHS × CT/MR)\n",
    "Questo blocco calcola e visualizza statistiche per ciascuna combinazione dataset/dominio:\n",
    "- Dimensioni degli split (train/val/test)\n",
    "- Forma media di immagini e maschere\n",
    "- Statistiche di intensità (min/max/media/dev.std) su un sottoinsieme del train\n",
    "- Distribuzione delle classi (bar chart) sul sottoinsieme del train\n",
    "\n",
    "Nota: per rapidità, le statistiche vengono calcolate su un sottoinsieme dei primi N campioni del train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "SUBSET_N = 1e16-1  # numero massimo di campioni del train da usare per le statistiche\n",
    "PRINT_EVERY = 8\n",
    "\n",
    "# helper: estrai numpy dai MetaTensor o torch.Tensor\n",
    "\n",
    "def to_numpy(x):\n",
    "    if hasattr(x, \"detach\"):\n",
    "        x = x.detach()\n",
    "    if hasattr(x, \"cpu\"):\n",
    "        x = x.cpu()\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "def class_histogram(labels_np):\n",
    "    # considera solo valori >=0\n",
    "    flat = labels_np.astype(np.int64).ravel()\n",
    "    flat = flat[flat >= 0]\n",
    "    counts = Counter(flat.tolist())\n",
    "    return counts\n",
    "\n",
    "\n",
    "def summarize_split(loader, max_items=SUBSET_N):\n",
    "    n = 0\n",
    "    shapes_img, shapes_seg = [], []\n",
    "    stats = {\n",
    "        \"img_min\": [],\n",
    "        \"img_max\": [],\n",
    "        \"img_mean\": [],\n",
    "        \"img_std\": [],\n",
    "        \"class_counts\": Counter(),\n",
    "    }\n",
    "    if loader is None:\n",
    "        return {\n",
    "            \"n_seen\": 0,\n",
    "            \"img_shape_examples\": [],\n",
    "            \"seg_shape_examples\": [],\n",
    "            \"img_min\": None,\n",
    "            \"img_max\": None,\n",
    "            \"img_mean\": None,\n",
    "            \"img_std\": None,\n",
    "            \"class_hist\": {},\n",
    "        }\n",
    "    for batch in loader:\n",
    "        img = batch.get(\"image\")\n",
    "        seg = batch.get(\"label\")\n",
    "        if img is None:\n",
    "            continue\n",
    "        # img/seg possono essere MetaTensor con shape (B, C, H, W) o (B, C, H, W, D)\n",
    "        img_np = to_numpy(img)\n",
    "        stats[\"img_min\"].append(float(img_np.min()))\n",
    "        stats[\"img_max\"].append(float(img_np.max()))\n",
    "        stats[\"img_mean\"].append(float(img_np.mean()))\n",
    "        stats[\"img_std\"].append(float(img_np.std()))\n",
    "        shapes_img.append(tuple(img_np.shape))\n",
    "        if seg is not None:\n",
    "            seg_np = to_numpy(seg)\n",
    "            shapes_seg.append(tuple(seg_np.shape))\n",
    "            stats[\"class_counts\"].update(class_histogram(seg_np))\n",
    "        n += img_np.shape[0]\n",
    "        if n >= max_items:\n",
    "            break\n",
    "    # aggrega\n",
    "    agg = {\n",
    "        \"n_seen\": n,\n",
    "        \"img_shape_examples\": shapes_img[: min(3, len(shapes_img))],\n",
    "        \"seg_shape_examples\": shapes_seg[: min(3, len(shapes_seg))],\n",
    "        \"img_min\": float(np.mean(stats[\"img_min\"])) if stats[\"img_min\"] else None,\n",
    "        \"img_max\": float(np.mean(stats[\"img_max\"])) if stats[\"img_max\"] else None,\n",
    "        \"img_mean\": float(np.mean(stats[\"img_mean\"])) if stats[\"img_mean\"] else None,\n",
    "        \"img_std\": float(np.mean(stats[\"img_std\"])) if stats[\"img_std\"] else None,\n",
    "        \"class_hist\": dict(stats[\"class_counts\"]),\n",
    "    }\n",
    "    return agg\n",
    "\n",
    "\n",
    "def plot_histogram(hist_dict, title, classnames=None):\n",
    "    if not hist_dict:\n",
    "        print(f\"   Nessuna maschera/nessuna classe trovata per {title}\")\n",
    "        return\n",
    "    keys = sorted(hist_dict.keys())\n",
    "    vals = [hist_dict[k] for k in keys]\n",
    "    labels = [classnames[k] if classnames and k < len(classnames) else str(k) for k in keys]\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.bar(range(len(keys)), vals)\n",
    "    plt.xticks(range(len(keys)), labels, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for domain in DOMAINS:\n",
    "        print(f\"\\n== {dataset_name} / {domain} ==\")\n",
    "        image_transform, seg_transform = get_preprocessing(dataset_name, domain, is_training=False)\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\" and domain == \"MR\":\n",
    "            # opzionale: limita a fegato\n",
    "            extra_kwargs[\"liver_only\"] = False\n",
    "\n",
    "        ds = get_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            base_path=DATA_PATH,\n",
    "            domain=domain,\n",
    "            transform=image_transform,\n",
    "            seg_transform=seg_transform,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            slice_2d=not USE_3D,\n",
    "            cache_max_items=CACHE_MAX_ITEMS,\n",
    "            enable_cache=ENABLE_CACHE,\n",
    "            **extra_kwargs,\n",
    "        )\n",
    "\n",
    "        # dimensioni split\n",
    "        n_train = len(ds.train_dataset) if ds.train_dataset is not None else 0\n",
    "        n_val = len(ds.val_dataset) if ds.val_dataset is not None else 0\n",
    "        n_test = len(ds.test_dataset) if ds.test_dataset is not None else 0\n",
    "        print(f\"Split -> train: {n_train}, val: {n_val}, test: {n_test}\")\n",
    "        print(f\"Num classi: {getattr(ds, 'num_classes', 'N/A')} | Classnames: {getattr(ds, 'classnames', None)}\")\n",
    "\n",
    "        # statistiche su subset del train\n",
    "        train_stats = summarize_split(ds.train_loader, SUBSET_N)\n",
    "        imin = train_stats['img_min']\n",
    "        imax = train_stats['img_max']\n",
    "        imean = train_stats['img_mean']\n",
    "        istd = train_stats['img_std']\n",
    "        fmt = lambda v: (f\"{v:.4f}\" if isinstance(v, (int, float)) else \"N/A\")\n",
    "        print(f\"   Visti nel subset: {train_stats['n_seen']}\")\n",
    "        print(f\"   Esempi img shape: {train_stats['img_shape_examples']}\")\n",
    "        print(f\"   Esempi seg shape: {train_stats['seg_shape_examples']}\")\n",
    "        print(\n",
    "            f\"   Intensità ~ min:{fmt(imin)} max:{fmt(imax)} \"\n",
    "            f\"mean:{fmt(imean)} std:{fmt(istd)}\"\n",
    "        )\n",
    "\n",
    "        all_stats[f\"{dataset_name}_{domain}\"] = {\n",
    "            \"splits\": {\"train\": n_train, \"val\": n_val, \"test\": n_test},\n",
    "            \"subset\": train_stats,\n",
    "            \"classnames\": getattr(ds, \"classnames\", None),\n",
    "        }\n",
    "\n",
    "        # bar chart distribuzione classi\n",
    "        plot_histogram(\n",
    "            train_stats[\"class_hist\"],\n",
    "            title=f\"Distribuzione classi (subset) - {dataset_name} {domain}\",\n",
    "            classnames=ds.classnames if hasattr(ds, \"classnames\") else None,\n",
    "        )\n",
    "\n",
    "# salva riepilogo su file\n",
    "try:\n",
    "    out_file = OUTPUTS_PATH / \"dataset_stats.json\"\n",
    "    with open(out_file, \"w\") as f:\n",
    "        json.dump(all_stats, f, indent=2)\n",
    "    print(f\"\\nSalvato riepilogo in: {out_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore salvataggio stats: {e}\")\n",
    "\n",
    "print(\"\\nRiepilogo sintetico:\")\n",
    "for k, v in all_stats.items():\n",
    "    s = v[\"splits\"]\n",
    "    print(f\" - {k}: train={s['train']}, val={s['val']}, test={s['test']} | visti(subset)={v['subset']['n_seen']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
