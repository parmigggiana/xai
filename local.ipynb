{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0bb3b4",
   "metadata": {},
   "source": [
    "In Kaggle, \n",
    "Enable file persistence and internet access.\n",
    "Remember that you can run the whole notebook and close the runtime without wasting resources by going to File > Save Version > Save & Run All (Double check that GPU is selected in the advanced settings).\n",
    "Later, by going to 'File' > 'Version history' you can view the full logs and download the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c8f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle\n",
    "import os\n",
    "\n",
    "IN_KAGGLE = False\n",
    "if os.environ.get(\"KAGGLE_URL_BASE\", \"\"):\n",
    "    IN_KAGGLE = True\n",
    "    !git clone https://github.com/parmigggiana/xai /kaggle/working/xai\n",
    "    %cd xai\n",
    "    !git fetch\n",
    "    !git reset --hard origin/main\n",
    "    %pip install 'monai[einops,itk,nibabel]>=1.5.0' git+https://github.com/timojl/clipseg.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c6af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "IN_COLAB = False\n",
    "if not IN_KAGGLE:\n",
    "    try:\n",
    "        import google.colab\n",
    "        from google.colab import drive\n",
    "\n",
    "        IN_COLAB = True\n",
    "        import os\n",
    "\n",
    "        drive.mount(\"/content/drive\")\n",
    "        os.makedirs(\"/content/drive/MyDrive/xai\", exist_ok=True)\n",
    "        !git clone https://github.com/parmigggiana/xai /content/xai\n",
    "        %cd /content/xai\n",
    "        !git fetch\n",
    "        !git reset --hard origin/main\n",
    "        %pip install -r requirements.txt\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_vector import TaskVector\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"CHAOS\", \"MMWHS\"]\n",
    "DOMAINS = [\"CT\", \"MR\"]\n",
    "DATA_PATH = \"data/\"\n",
    "CHECKPOINT_PATH = \"checkpoints/\"\n",
    "OUTPUTS_PATH = \"outputs/\"\n",
    "USE_3D = False\n",
    "TRAINING_EPOCHS = {\n",
    "    (\"CHAOS\", \"CT\"): 100,\n",
    "    (\"CHAOS\", \"MR\"): 100,\n",
    "    (\"MMWHS\", \"CT\"): 100,\n",
    "    (\"MMWHS\", \"MR\"): 100,\n",
    "}\n",
    "BATCH_SIZE = 8\n",
    "SPATIAL_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 5e-5\n",
    "# Number of DataLoader workers (set >0 to enable parallel data loading)\n",
    "NUM_WORKERS = 0 if not IN_KAGGLE and not IN_COLAB else 2\n",
    "# Set True to enable debug prints/timers/visualizations)\n",
    "DEBUG = False\n",
    "\n",
    "# Profiling controls: False | 'cprofile' | 'torch'\n",
    "PROFILE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(CHECKPOINT_PATH)\n",
    "OUTPUTS_PATH = Path(OUTPUTS_PATH)\n",
    "DATA_PATH = Path(DATA_PATH)\n",
    "PROFILE_DIR = OUTPUTS_PATH / \"profiling\"\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "PROFILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if USE_3D:\n",
    "    encoder_type = \"swin_unetr\"\n",
    "else:\n",
    "    encoder_type = \"clipseg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai import transforms\n",
    "\n",
    "\n",
    "def update_metrics(name, new_metrics):\n",
    "    metrics_file = OUTPUTS_PATH / \"metrics.json\"\n",
    "\n",
    "    if not metrics_file.exists():\n",
    "        metrics = {}\n",
    "    else:\n",
    "        with open(metrics_file, \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "    metrics[name] = new_metrics\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "\n",
    "# Normalization stats (mean, std) per dataset/domain\n",
    "NORM_STATS = {\n",
    "    (\"MMWHS\", \"MR\"): (186.5875, 258.5917),\n",
    "    (\"MMWHS\", \"CT\"): (-745.0086, 1042.7251),\n",
    "    (\"CHAOS\", \"MR\"): (90.8292, 168.8922),\n",
    "    (\"CHAOS\", \"CT\"): (-478.1732, 476.7163),\n",
    "}\n",
    "\n",
    "# Optimized preprocessing: resize early\n",
    "\n",
    "\n",
    "def get_preprocessing(dataset_name: str, domain: str, is_training=True):\n",
    "    decode_func = get_decode_func(dataset_name, domain)\n",
    "    mean_std = NORM_STATS.get((dataset_name, domain))\n",
    "    mean, std = mean_std if mean_std is not None else (None, None)\n",
    "\n",
    "    # Image-specific transforms\n",
    "    if USE_3D:\n",
    "        image_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "    else:\n",
    "        image_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "\n",
    "    # Resize early to reduce compute\n",
    "    image_transforms.append(\n",
    "        transforms.Resize(\n",
    "            spatial_size=SPATIAL_SIZE,\n",
    "            size_mode=\"longest\",\n",
    "            mode=\"area\",\n",
    "            anti_aliasing=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert to tensor and ensure float32 for stable CPU ops\n",
    "    image_transforms.extend(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Normalize (still in float32)\n",
    "    if mean is not None and std is not None:\n",
    "        image_transforms.append(\n",
    "            transforms.NormalizeIntensity(\n",
    "                subtrahend=float(mean),\n",
    "                divisor=float(std),\n",
    "                channel_wise=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Augmentations (training only) â€” run in float32 on CPU\n",
    "    if is_training:\n",
    "        image_transforms.extend(\n",
    "            [\n",
    "                transforms.RandGaussianNoise(prob=0.15, std=0.05),\n",
    "                transforms.RandAdjustContrast(prob=0.15, gamma=(0.95, 1.05)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Repeat to 3 channels only at the end (2D only)\n",
    "    if not USE_3D:\n",
    "        image_transforms.append(transforms.RepeatChannel(repeats=3))\n",
    "\n",
    "    image_transform = transforms.Compose(image_transforms)\n",
    "\n",
    "    # Segmentation transforms\n",
    "    if not USE_3D:\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "    else:\n",
    "        seg_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "\n",
    "    seg_transforms.extend(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.long),\n",
    "            transforms.Lambda(\n",
    "                lambda x: decode_func(x)\n",
    "            ),  # decode after tensor conversion\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE, size_mode=\"longest\", mode=\"nearest\"\n",
    "            ),\n",
    "            # transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    seg_transform = transforms.Compose(seg_transforms)\n",
    "    return image_transform, seg_transform\n",
    "\n",
    "\n",
    "def get_decode_func(dataset_name, domain):\n",
    "    from src.datasets.mmwhs import mmwhs_labels\n",
    "\n",
    "    decode = None\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        if domain in [\"MR\", \"MRI\"]:\n",
    "\n",
    "            def decode(labels):\n",
    "                # Convert intensity values to class indices (keep as float32)\n",
    "                return labels // 63\n",
    "\n",
    "        elif domain == \"CT\":\n",
    "\n",
    "            def decode(labels):\n",
    "                return torch.where(labels > 0, 1.0, 0.0)\n",
    "\n",
    "    elif dataset_name == \"MMWHS\":\n",
    "\n",
    "        def decode(labels):\n",
    "            decoded_labels = torch.zeros_like(labels, dtype=torch.float32)\n",
    "            for i, label_val in enumerate(mmwhs_labels.keys()):\n",
    "                decoded_labels[labels == label_val] = i\n",
    "            return decoded_labels\n",
    "\n",
    "    if decode is None:\n",
    "\n",
    "        def decode(labels):\n",
    "            return labels\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning on CHAOS dataset in CT domain with 2d images \n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "ðŸš€ Starting training for 100 epochs\n",
      "   Device: cuda\n",
      "\n",
      "ðŸ“– Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/251 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     79\u001b[39m         p.requires_grad = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Train Only Visual Encoder\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# for p in model.encoder.clipseg.model.parameters():\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m#     p.requires_grad_(not p.requires_grad)\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# model.freeze_text_encoder()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEBUG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPROFILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprofile_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPROFILE_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m torch.save(model.encoder, filename)\n\u001b[32m     96\u001b[39m model_metrics = model.evaluate(profile=PROFILE, profile_dir=\u001b[38;5;28mstr\u001b[39m(PROFILE_DIR))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\semantic_segmentation.py:627\u001b[39m, in \u001b[36mMedicalSegmenter.finetune\u001b[39m\u001b[34m(self, epochs, learning_rate, weight_decay, save_best, max_grad_norm, visualize_batches, early_stop_patience, profile, profile_dir, debug, compile_model, val_max_batches, lr_start, lr_end, lr_decay_epochs, train_decoder_only)\u001b[39m\n\u001b[32m    624\u001b[39m     _timer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    626\u001b[39m train_pbar = tqdm(\u001b[38;5;28mself\u001b[39m.dataset.train_loader, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_pbar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_timer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_timer\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontextlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnullcontext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\PersistentDataset.py:106\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, collections.abc.Sequence):\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# dataset[[1, 3, 4]]\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Subset(dataset=\u001b[38;5;28mself\u001b[39m, indices=index)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\PersistentDataset.py:368\u001b[39m, in \u001b[36mPersistentDataset._transform\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     pre_random_item = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cachecheck\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post_transform(pre_random_item)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\PersistentDataset.py:337\u001b[39m, in \u001b[36mPersistentDataset._cachecheck\u001b[39m\u001b[34m(self, item_transformed)\u001b[39m\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m _item_transformed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pre_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_transformed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# keep the original hashed\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hashfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _item_transformed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\PersistentDataset.py:271\u001b[39m, in \u001b[36mPersistentDataset._pre_transform\u001b[39m\u001b[34m(self, item_transformed)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03mProcess the data from original state up to the first random element.\u001b[39;00m\n\u001b[32m    259\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    266\u001b[39m \n\u001b[32m    267\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    268\u001b[39m first_random = \u001b[38;5;28mself\u001b[39m.transform.get_index_of_first(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28misinstance\u001b[39m(t, RandomizableTrait) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, Transform)\n\u001b[32m    270\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m item_transformed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mitem_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_random\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreading\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    273\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reset_ops_id:\n\u001b[32m    276\u001b[39m     reset_ops_id(item_transformed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\monai\\transforms\\compose.py:346\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, input_, start, end, threading, lazy)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_, start=\u001b[32m0\u001b[39m, end=\u001b[38;5;28;01mNone\u001b[39;00m, threading=\u001b[38;5;28;01mFalse\u001b[39;00m, lazy: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    345\u001b[39m     _lazy = \u001b[38;5;28mself\u001b[39m._lazy \u001b[38;5;28;01mif\u001b[39;00m lazy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lazy\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     result = \u001b[43mexecute_compose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_lazy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthreading\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\monai\\transforms\\compose.py:116\u001b[39m, in \u001b[36mexecute_compose\u001b[39m\u001b[34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m threading:\n\u001b[32m    115\u001b[39m         _transform = deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     data = \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m data = apply_pending_transforms(data, \u001b[38;5;28;01mNone\u001b[39;00m, overrides, logger_name=log_stats)\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\monai\\transforms\\transform.py:150\u001b[39m, in \u001b[36mapply_transform\u001b[39m\u001b[34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m map_items_ > \u001b[32m0\u001b[39m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    147\u001b[39m             apply_transform(transform, item, map_items_ - \u001b[32m1\u001b[39m, unpack_items, log_stats, lazy, overrides)\n\u001b[32m    148\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[32m    149\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m MONAIEnvVars.debug():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\monai\\transforms\\transform.py:98\u001b[39m, in \u001b[36m_apply_transform\u001b[39m\u001b[34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m unpack_parameters:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(*data, lazy=lazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(*data)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m transform(data, lazy=lazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\PersistentDataset.py:469\u001b[39m, in \u001b[36mLoadPathOrSliceD.__call__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    467\u001b[39m         \u001b[38;5;28mself\u001b[39m._cache_put(path, img, meta \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(meta, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     img, meta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m slice_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img.shape) >= \u001b[32m3\u001b[39m:\n\u001b[32m    471\u001b[39m     aff = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\monai\\transforms\\io\\array.py:267\u001b[39m, in \u001b[36mLoadImage.__call__\u001b[39m\u001b[34m(self, filename, reader)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# try the user designated readers\u001b[39;00m\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m         img = \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    269\u001b[39m         err.append(traceback.format_exc())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\monai\\data\\image_reader.py:269\u001b[39m, in \u001b[36mITKReader.read\u001b[39m\u001b[34m(self, data, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m         img_.append(_obj)\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m         img_.append(\u001b[43mitk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img_ \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filenames) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m img_[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\itk\\support\\extras.py:1352\u001b[39m, in \u001b[36mimread\u001b[39m\u001b[34m(filename, pixel_type, fallback_only, imageio, series_uid)\u001b[39m\n\u001b[32m   1350\u001b[39m     kwargs = {\u001b[33m\"\u001b[39m\u001b[33mFileNames\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m filename]}\n\u001b[32m   1351\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1352\u001b[39m     template_reader_type = \u001b[43mitk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mImageFileReader\u001b[49m\n\u001b[32m   1353\u001b[39m     io_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1354\u001b[39m     increase_dimension = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\itk\\support\\lazy.py:138\u001b[39m, in \u001b[36mLazyITKModule.__getattribute__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    136\u001b[39m module = \u001b[38;5;28mself\u001b[39m.__belong_lazy_attributes[attr]\n\u001b[32m    137\u001b[39m namespace = {}\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[43mbase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitk_load_swig_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28mself\u001b[39m.loaded_lazy_modules.add(module)\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m namespace.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\itk\\support\\base.py:130\u001b[39m, in \u001b[36mitk_load_swig_module\u001b[39m\u001b[34m(name, namespace)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# SWIG-generated modules have 'Python' appended. Only load the SWIG module\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# if we haven't already.\u001b[39;00m\n\u001b[32m    129\u001b[39m loader = LibraryLoader()\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m l_module = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mswig_module_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# OK, now the modules on which this one depends are loaded and\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# template_feature-instantiated, and the SWIG module for this one is also loaded.\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# We're going to put the things we load and create in two places: the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# stomp on an existing 'swig' namespace, nor do we want to share 'swig'\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# namespaces between this_module and namespace.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\itk\\support\\base.py:289\u001b[39m, in \u001b[36mLibraryLoader.load\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# since version 3.4: Use importlib.util.find_spec() instead.\u001b[39;00m\n\u001b[32m    288\u001b[39m     l_spec = importlib.util.find_spec(name)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[43ml_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml_module\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m l_module\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:940\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:241\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\itk\\support\\..\\ITKIOImageBasePython.py:66\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mITKCommonPython\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mitkImageSeriesWriterPython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mitkImageSeriesReaderPython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mitkImageFileWriterPython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mitkImageFileReaderPython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\itk\\itkImageSeriesReaderPython.py:7522\u001b[39m\n\u001b[32m   7519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mitkImageSeriesReaderISS3_New\u001b[39m():\n\u001b[32m   7520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m itkImageSeriesReaderISS3.New()\n\u001b[32m-> \u001b[39m\u001b[32m7522\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitkImageSeriesReaderISS3\u001b[39;00m(\u001b[43mitk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitkImageSourcePython\u001b[49m.itkImageSourceISS3):\n\u001b[32m   7523\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Proxy of C++ itkImageSeriesReaderISS3 class.\"\"\"\u001b[39;00m\n\u001b[32m   7525\u001b[39m     thisown = \u001b[38;5;28mproperty\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.this.own(), \u001b[38;5;28;01mlambda\u001b[39;00m x, v: x.this.own(v), doc=\u001b[33m\"\u001b[39m\u001b[33mThe membership flag\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\itk\\support\\lazy.py:130\u001b[39m, in \u001b[36mLazyITKModule.__getattribute__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    127\u001b[39m         \u001b[38;5;28mcls\u001b[39m._lock = ITKLazyLoadLock()\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._lock\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[32m    131\u001b[39m     value = types.ModuleType.\u001b[34m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m not_loaded:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in TRAINING_EPOCHS.items():\n",
    "    download_and_extract_dataset(dataset_name, DATA_PATH)\n",
    "\n",
    "    image_transform, seg_transform = get_preprocessing(\n",
    "        dataset_name, domain, is_training=True\n",
    "    )\n",
    "\n",
    "    filename = f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned.pth\"\n",
    "    filename = CHECKPOINT_PATH / filename\n",
    "    # Check if the finetuned checkpoint already exists\n",
    "    if filename.exists():\n",
    "        print(\n",
    "            f\"Finetuned model for {dataset_name} in {domain} domain with {'3d' if USE_3D else '2d'} images already exists at {filename}. Skipping finetuning.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        transform=image_transform,\n",
    "        seg_transform=seg_transform,\n",
    "        base_path=DATA_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        slice_2d=not USE_3D,\n",
    "    )\n",
    "\n",
    "    #  Ensure the dataset is loaded correctly\n",
    "    if not isinstance(dataset, BaseDataset):\n",
    "        raise TypeError(\n",
    "            f\"Expected dataset to be an instance of BaseDataset, got {type(dataset)}\"\n",
    "        )\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=encoder_type,\n",
    "    )\n",
    "\n",
    "    # Save the baseline model's state_dict before finetuning\n",
    "    baseline_filename = (\n",
    "        CHECKPOINT_PATH\n",
    "        / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "    )\n",
    "    torch.save(model.encoder, baseline_filename)\n",
    "\n",
    "    if USE_3D:\n",
    "        model.freeze_body()\n",
    "        model.finetune(\n",
    "            epochs=epochs,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            profile=PROFILE,\n",
    "            profile_dir=str(PROFILE_DIR),\n",
    "        )\n",
    "        metrics = model.evaluate(profile=PROFILE, profile_dir=str(PROFILE_DIR))\n",
    "        update_metrics(\n",
    "            f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_head\",\n",
    "            metrics,\n",
    "        )\n",
    "\n",
    "    # Train Only Segmentation Head\n",
    "    # pass\n",
    "\n",
    "    # Train Visual Encoder + Segmentation head\n",
    "    # model.unfreeze()\n",
    "    # model.freeze_text_encoder()\n",
    "\n",
    "    # Train Last 2 ResBlocks of Visual Encoder + Segmentation head\n",
    "    # for p in model.encoder.clipseg.reduce.parameters():  # Not in forward pass anyway\n",
    "    #     p.requires_grad = False\n",
    "    # for i in range(8, 10):\n",
    "    #     for p in model.encoder.clipseg.clip_model.visual.transformer.resblocks[\n",
    "    #         i\n",
    "    #     ].parameters():\n",
    "    #         p.requires_grad = True\n",
    "\n",
    "    # Train Only Visual Encoder\n",
    "    # for p in model.encoder.clipseg.model.parameters():\n",
    "    #     p.requires_grad_(not p.requires_grad)\n",
    "    # model.freeze_text_encoder()\n",
    "\n",
    "    history = model.finetune(\n",
    "        epochs=epochs,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        debug=DEBUG,\n",
    "        profile=PROFILE,\n",
    "        profile_dir=str(PROFILE_DIR),\n",
    "    )\n",
    "\n",
    "    torch.save(model.encoder, filename)\n",
    "    model_metrics = model.evaluate(profile=PROFILE, profile_dir=str(PROFILE_DIR))\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned\",\n",
    "        model_metrics,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090233",
   "metadata": {},
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWIN UNETR Task Vectors\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from monai.networks.nets.swin_unetr import BasicLayer\n",
    "from monai.networks.nets.swin_unetr import SwinTransformerBlock\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from monai.networks.nets.swin_unetr import WindowAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules.linear import Identity\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from torch.nn.modules.activation import GELU\n",
    "from monai.networks.nets.swin_unetr import PatchMerging\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from monai.networks.blocks.unetr_block import UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from torch.nn.modules.conv import ConvTranspose3d\n",
    "\n",
    "safe_globals = [\n",
    "    SwinUNETR,\n",
    "    SwinTransformer,\n",
    "    PatchEmbed,\n",
    "    Conv3d,\n",
    "    Dropout,\n",
    "    ModuleList,\n",
    "    BasicLayer,\n",
    "    SwinTransformerBlock,\n",
    "    LayerNorm,\n",
    "    WindowAttention,\n",
    "    Linear,\n",
    "    Softmax,\n",
    "    Identity,\n",
    "    MLPBlock,\n",
    "    GELU,\n",
    "    PatchMerging,\n",
    "    UnetrBasicBlock,\n",
    "    UnetResBlock,\n",
    "    Convolution,\n",
    "    LeakyReLU,\n",
    "    InstanceNorm3d,\n",
    "    UnetrUpBlock,\n",
    "    ConvTranspose3d,\n",
    "    UnetOutBlock,\n",
    "]\n",
    "##\n",
    "\n",
    "## CLIPSeg Task Vectors\n",
    "from src.CLIPSeg import CLIPSeg\n",
    "from clipseg.clipseg import CLIPDensePredT\n",
    "from clip.model import (\n",
    "    CLIP,\n",
    "    VisionTransformer,\n",
    "    LayerNorm,\n",
    "    Transformer,\n",
    "    ResidualAttentionBlock,\n",
    "    QuickGELU,\n",
    ")\n",
    "from torch.nn.modules.conv import Conv2d, ConvTranspose2d\n",
    "from torch.nn.modules.container import Sequential\n",
    "from torch.nn.modules.activation import MultiheadAttention, ReLU\n",
    "from torch.nn.modules.linear import NonDynamicallyQuantizableLinear\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.nn.modules.transformer import (\n",
    "    TransformerEncoderLayer,\n",
    "    TransformerEncoder,\n",
    "    TransformerDecoderLayer,\n",
    "    TransformerDecoder,\n",
    ")\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn.modules.container import ModuleDict\n",
    "\n",
    "safe_globals.extend(\n",
    "    [\n",
    "        CLIPSeg,\n",
    "        CLIPDensePredT,\n",
    "        CLIP,\n",
    "        VisionTransformer,\n",
    "        Conv2d,\n",
    "        LayerNorm,\n",
    "        Transformer,\n",
    "        Sequential,\n",
    "        ResidualAttentionBlock,\n",
    "        MultiheadAttention,\n",
    "        NonDynamicallyQuantizableLinear,\n",
    "        QuickGELU,\n",
    "        Embedding,\n",
    "        ReLU,\n",
    "        ConvTranspose2d,\n",
    "        TransformerEncoderLayer,\n",
    "        TransformerEncoder,\n",
    "        TransformerDecoderLayer,\n",
    "        TransformerDecoder,\n",
    "        relu,\n",
    "        ModuleDict,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for domain in DOMAINS:\n",
    "        print(\n",
    "            f\"Building task vector for {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images\"\n",
    "        )\n",
    "        baseline_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        finetuned_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned.pth\"\n",
    "        )\n",
    "        if not baseline_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Baseline checkpoint for {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "        if not finetuned_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Finetuned checkpoint {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        with torch.serialization.safe_globals(\n",
    "            safe_globals=safe_globals,\n",
    "        ):\n",
    "            task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "            # Remove keys associated with the output layers from the task vector\n",
    "            # For swin it's all layers starting with '.out'\n",
    "            # For clipseg it might not be necessary since the model architecture isn't dependent on the number of output features\n",
    "            if encoder_type == \"swin_unetr\":\n",
    "                for k in task_vector.keys():\n",
    "                    if k.startswith(\".out\"):\n",
    "                        del task_vector[k]\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe4664",
   "metadata": {},
   "source": [
    "## Part 1: Improve robustness post-hoc with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS\": (task_vectors[\"MMWHS_MR\"]*0.55)\n",
    "    + (task_vectors[\"MMWHS_CT\"]*0.45),\n",
    "    \"CHAOS\": (task_vectors[\"CHAOS_MR\"]*0.7)\n",
    "    + (task_vectors[\"CHAOS_CT\"]*0.3),\n",
    "    \"MR\": (task_vectors[\"CHAOS_MR\"]*0.64)\n",
    "     + (task_vectors[\"MMWHS_MR\"]*0.36),\n",
    "    \"CT\": (task_vectors[\"CHAOS_CT\"]*0.475)\n",
    "     + (task_vectors[\"MMWHS_CT\"]*0.525),\n",
    "}\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task vector simple composition experiments\n",
    "print(\"ðŸ”„ Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for target_domain in DOMAINS:\n",
    "        print(f\"\\n{dataset_name}: {target_domain} adaptation\")\n",
    "\n",
    "        image_transform, seg_transform = get_preprocessing(\n",
    "            dataset_name, target_domain, is_training=False\n",
    "        )\n",
    "\n",
    "        dataset_kwargs = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"base_path\": DATA_PATH,\n",
    "            \"domain\": target_domain,\n",
    "            \"transform\": image_transform,  # Use transform instead of preprocess\n",
    "            \"seg_transform\": seg_transform,  # Pass seg_transform too\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"num_workers\": NUM_WORKERS,\n",
    "            \"slice_2d\": not USE_3D,\n",
    "        }\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\":\n",
    "            extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "        target_dataset = get_dataset(**dataset_kwargs, **extra_kwargs)\n",
    "\n",
    "        composite_task_vector = composite_task_vectors[dataset_name]\n",
    "\n",
    "        target_model = target_dataset.get_model(encoder_type=encoder_type)\n",
    "        target_model.load_task_vector(composite_task_vector, scaling_coef=alpha)\n",
    "\n",
    "        metrics = target_model.evaluate()\n",
    "        update_metrics(f\"{dataset_name}_composite_at_{target_domain}\", metrics)\n",
    "        train_d = metrics.get(\"train\", {}).get(\"dice\", 0)\n",
    "        val_d = metrics.get(\"val\", {}).get(\"dice\")\n",
    "        if val_d is not None:\n",
    "            print(\n",
    "                f\"   âœ… {dataset_name} at {target_domain}: Train Dice={train_d:.3f} | Val Dice={val_d:.3f}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   âœ… {dataset_name} at {target_domain}: Train Dice={train_d:.3f}\")\n",
    "\n",
    "        composite_task_vector = composite_task_vectors[target_domain]\n",
    "\n",
    "        target_model = target_dataset.get_model(encoder_type=encoder_type)\n",
    "        target_model.load_task_vector(composite_task_vector, scaling_coef=alpha)\n",
    "\n",
    "        metrics = target_model.evaluate()\n",
    "        update_metrics(f\"{target_domain}_composite_at_{dataset_name}\", metrics)\n",
    "        train_d = metrics.get(\"train\", {}).get(\"dice\", 0)\n",
    "        val_d = metrics.get(\"val\", {}).get(\"dice\")\n",
    "        if val_d is not None:\n",
    "            print(\n",
    "                f\"   âœ… {target_domain} at {dataset_name}: Train Dice={train_d:.3f} | Val Dice={val_d:.3f}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   âœ… {target_domain} at {dataset_name}: Train Dice={train_d:.3f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0784a95d",
   "metadata": {},
   "source": [
    "## Part 2: Improve robustness post-hoc without data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS_CT\":  1.04 * task_vectors[\"MMWHS_MR\"] \\\n",
    "               + 0.76 * task_vectors[\"CHAOS_CT\"] \\\n",
    "               - 1.79 * task_vectors[\"CHAOS_MR\"],\n",
    "\n",
    "    \"MMWHS_MR\":  0.85 * task_vectors[\"MMWHS_CT\"] \\\n",
    "               + 1.79 * task_vectors[\"CHAOS_MR\"] \\\n",
    "               - 0.76 * task_vectors[\"CHAOS_CT\"],\n",
    "\n",
    "    \"CHAOS_CT\":  1.79 * task_vectors[\"CHAOS_MR\"] \\\n",
    "               + 0.85 * task_vectors[\"MMWHS_CT\"] \\\n",
    "               - 1.04 * task_vectors[\"MMWHS_MR\"],\n",
    "\n",
    "    \"CHAOS_MR\":  0.76 * task_vectors[\"CHAOS_CT\"] \\\n",
    "               + 1.04 * task_vectors[\"MMWHS_MR\"] \\\n",
    "               - 0.85 * task_vectors[\"MMWHS_CT\"],\n",
    "}\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”„ Task Vector Cross-Domain Adaptation Experiments\n",
    "print(\"ðŸ”„ Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for target_domain in DOMAINS:\n",
    "        print(f\"\\n{dataset_name}: {target_domain} adaptation\")\n",
    "\n",
    "        image_transform, seg_transform = get_preprocessing(\n",
    "            dataset_name, target_domain, is_training=False\n",
    "        )\n",
    "\n",
    "        dataset_kwargs = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"base_path\": DATA_PATH,\n",
    "            \"domain\": target_domain,\n",
    "            \"transform\": image_transform,  # Use transform instead of preprocess\n",
    "            \"seg_transform\": seg_transform,  # Pass seg_transform too\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"num_workers\": NUM_WORKERS,\n",
    "            \"slice_2d\": not USE_3D,\n",
    "        }\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\":\n",
    "            extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "        # try:\n",
    "        target_dataset = get_dataset(**dataset_kwargs, **extra_kwargs)\n",
    "\n",
    "        composite_key = f\"{dataset_name}_{target_domain}\"\n",
    "        if composite_key in composite_task_vectors:\n",
    "            composite_task_vector = composite_task_vectors[composite_key]\n",
    "\n",
    "            target_model = target_dataset.get_model(encoder_type=encoder_type)\n",
    "            target_model.load_task_vector(composite_task_vector, scaling_coef=alpha)\n",
    "\n",
    "            metrics = target_model.evaluate()\n",
    "            update_metrics(f\"{composite_key}_adaptation\", metrics)\n",
    "            train_d = metrics.get(\"train\", {}).get(\"dice\", 0)\n",
    "            val_d = metrics.get(\"val\", {}).get(\"dice\")\n",
    "            if val_d is not None:\n",
    "                print(\n",
    "                    f\"   âœ… {composite_key}: Train Dice={train_d:.3f} | Val Dice={val_d:.3f}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"   âœ… {composite_key}: Train Dice={train_d:.3f}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ No composite task vector found for {composite_key}\")\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"   âŒ {dataset_name} {target_domain}: {str(e)[:100]}...\")\n",
    "        #     import traceback\n",
    "        #     traceback.print_exc()\n",
    "        #     # continue\n",
    "        #     break\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display all metrics\n",
    "metrics_file = OUTPUTS_PATH / \"metrics.json\"\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        all_metrics = json.load(f)\n",
    "\n",
    "    print(\"\\nðŸ“Š COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    def fmt_pair(m):\n",
    "        if not isinstance(m, dict):\n",
    "            return \"Dice=N/A\"\n",
    "        t = m.get(\"train\", {})\n",
    "        v = m.get(\"val\", {})\n",
    "        t_d = t.get(\"dice\")\n",
    "        v_d = v.get(\"dice\")\n",
    "        if t_d is not None and v_d is not None:\n",
    "            return f\"Train Dice={t_d:.3f}, Val Dice={v_d:.3f}\"\n",
    "        if t_d is not None:\n",
    "            return f\"Train Dice={t_d:.3f}\"\n",
    "        if v_d is not None:\n",
    "            return f\"Val Dice={v_d:.3f}\"\n",
    "        return \"Dice=N/A\"\n",
    "\n",
    "    # Baseline performance\n",
    "    print(\"\\nBaseline Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"baseline\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # After Head-training performance\n",
    "    print(\"\\nðŸ‹ï¸â€â™‚ï¸ After Head-Training Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"head\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # Finetuned performance\n",
    "    print(\"\\nðŸ† Finetuned Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"finetuned\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # Composite task vector results\n",
    "    print(\"\\nðŸ§© Composite Task Vector Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"composite_at\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # Dataless adaptation results\n",
    "    print(\"\\nðŸ”„ Dataless Adaptation Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"adaptation\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "else:\n",
    "    print(\"No metrics file found. Run the experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "\n",
    "    # Copy checkpoints.zip to Google Drive\n",
    "    !zip -r /content/checkpoints.zip /content/xai/checkpoints\n",
    "    shutil.copy(\n",
    "        \"/content/checkpoints.zip\", \"/content/drive/MyDrive/xai/checkpoints.zip\"\n",
    "    )\n",
    "\n",
    "    # Copy metrics.json to Google Drive\n",
    "    shutil.copy(\n",
    "        \"/content/xai/outputs/metrics.json\", \"/content/drive/MyDrive/xai/metrics.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    !zip -r /kaggle/working/checkpoints.zip /kaggle/working/xai/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f3841",
   "metadata": {},
   "source": [
    "# Statistiche dei 4 dataset (CHAOS/MMWHS Ã— CT/MR)\n",
    "Questo blocco calcola e visualizza statistiche per ciascuna combinazione dataset/dominio:\n",
    "- Dimensioni degli split (train/val/test)\n",
    "- Forma media di immagini e maschere\n",
    "- Statistiche di intensitÃ  (min/max/media/dev.std) su un sottoinsieme del train\n",
    "- Distribuzione delle classi (bar chart) sul sottoinsieme del train\n",
    "\n",
    "Nota: per rapiditÃ , le statistiche vengono calcolate su un sottoinsieme dei primi N campioni del train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "SUBSET_N = 1e16 - 1  # numero massimo di campioni del train da usare per le statistiche\n",
    "PRINT_EVERY = 8\n",
    "\n",
    "# helper: estrai numpy dai MetaTensor o torch.Tensor\n",
    "\n",
    "\n",
    "def to_numpy(x):\n",
    "    if hasattr(x, \"detach\"):\n",
    "        x = x.detach()\n",
    "    if hasattr(x, \"cpu\"):\n",
    "        x = x.cpu()\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "def class_histogram(labels_np):\n",
    "    # considera solo valori >=0\n",
    "    flat = labels_np.astype(np.int64).ravel()\n",
    "    flat = flat[flat >= 0]\n",
    "    counts = Counter(flat.tolist())\n",
    "    return counts\n",
    "\n",
    "\n",
    "def summarize_split(loader, max_items=SUBSET_N):\n",
    "    n = 0\n",
    "    shapes_img, shapes_seg = [], []\n",
    "    stats = {\n",
    "        \"img_min\": [],\n",
    "        \"img_max\": [],\n",
    "        \"img_mean\": [],\n",
    "        \"img_std\": [],\n",
    "        \"class_counts\": Counter(),\n",
    "    }\n",
    "    if loader is None:\n",
    "        return {\n",
    "            \"n_seen\": 0,\n",
    "            \"img_shape_examples\": [],\n",
    "            \"seg_shape_examples\": [],\n",
    "            \"img_min\": None,\n",
    "            \"img_max\": None,\n",
    "            \"img_mean\": None,\n",
    "            \"img_std\": None,\n",
    "            \"class_hist\": {},\n",
    "        }\n",
    "    for batch in loader:\n",
    "        img = batch.get(\"image\")\n",
    "        seg = batch.get(\"label\")\n",
    "        if img is None:\n",
    "            continue\n",
    "        # img/seg possono essere MetaTensor con shape (B, C, H, W) o (B, C, H, W, D)\n",
    "        img_np = to_numpy(img)\n",
    "        stats[\"img_min\"].append(float(img_np.min()))\n",
    "        stats[\"img_max\"].append(float(img_np.max()))\n",
    "        stats[\"img_mean\"].append(float(img_np.mean()))\n",
    "        stats[\"img_std\"].append(float(img_np.std()))\n",
    "        shapes_img.append(tuple(img_np.shape))\n",
    "        if seg is not None:\n",
    "            seg_np = to_numpy(seg)\n",
    "            shapes_seg.append(tuple(seg_np.shape))\n",
    "            stats[\"class_counts\"].update(class_histogram(seg_np))\n",
    "        n += img_np.shape[0]\n",
    "        if n >= max_items:\n",
    "            break\n",
    "    # aggrega\n",
    "    agg = {\n",
    "        \"n_seen\": n,\n",
    "        \"img_shape_examples\": shapes_img[: min(3, len(shapes_img))],\n",
    "        \"seg_shape_examples\": shapes_seg[: min(3, len(shapes_seg))],\n",
    "        \"img_min\": float(np.mean(stats[\"img_min\"])) if stats[\"img_min\"] else None,\n",
    "        \"img_max\": float(np.mean(stats[\"img_max\"])) if stats[\"img_max\"] else None,\n",
    "        \"img_mean\": float(np.mean(stats[\"img_mean\"])) if stats[\"img_mean\"] else None,\n",
    "        \"img_std\": float(np.mean(stats[\"img_std\"])) if stats[\"img_std\"] else None,\n",
    "        \"class_hist\": dict(stats[\"class_counts\"]),\n",
    "    }\n",
    "    return agg\n",
    "\n",
    "\n",
    "def plot_histogram(hist_dict, title, classnames=None):\n",
    "    if not hist_dict:\n",
    "        print(f\"   Nessuna maschera/nessuna classe trovata per {title}\")\n",
    "        return\n",
    "    keys = sorted(hist_dict.keys())\n",
    "    vals = [hist_dict[k] for k in keys]\n",
    "    labels = [\n",
    "        classnames[k] if classnames and k < len(classnames) else str(k) for k in keys\n",
    "    ]\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.bar(range(len(keys)), vals)\n",
    "    plt.xticks(range(len(keys)), labels, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for domain in DOMAINS:\n",
    "        print(f\"\\n== {dataset_name} / {domain} ==\")\n",
    "        image_transform, seg_transform = get_preprocessing(\n",
    "            dataset_name, domain, is_training=False\n",
    "        )\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\" and domain == \"MR\":\n",
    "            # opzionale: limita a fegato\n",
    "            extra_kwargs[\"liver_only\"] = False\n",
    "\n",
    "        ds = get_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            base_path=DATA_PATH,\n",
    "            domain=domain,\n",
    "            transform=image_transform,\n",
    "            seg_transform=seg_transform,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            slice_2d=not USE_3D,\n",
    "            **extra_kwargs,\n",
    "        )\n",
    "\n",
    "        # dimensioni split\n",
    "        n_train = len(ds.train_dataset) if ds.train_dataset is not None else 0\n",
    "        n_val = len(ds.val_dataset) if ds.val_dataset is not None else 0\n",
    "        n_test = len(ds.test_dataset) if ds.test_dataset is not None else 0\n",
    "        print(f\"Split -> train: {n_train}, val: {n_val}, test: {n_test}\")\n",
    "        print(\n",
    "            f\"Num classi: {getattr(ds, 'num_classes', 'N/A')} | Classnames: {getattr(ds, 'classnames', None)}\"\n",
    "        )\n",
    "\n",
    "        # statistiche su subset del train\n",
    "        train_stats = summarize_split(ds.train_loader, SUBSET_N)\n",
    "        imin = train_stats[\"img_min\"]\n",
    "        imax = train_stats[\"img_max\"]\n",
    "        imean = train_stats[\"img_mean\"]\n",
    "        istd = train_stats[\"img_std\"]\n",
    "        fmt = lambda v: (f\"{v:.4f}\" if isinstance(v, (int, float)) else \"N/A\")\n",
    "        print(f\"   Visti nel subset: {train_stats['n_seen']}\")\n",
    "        print(f\"   Esempi img shape: {train_stats['img_shape_examples']}\")\n",
    "        print(f\"   Esempi seg shape: {train_stats['seg_shape_examples']}\")\n",
    "        print(\n",
    "            f\"   IntensitÃ  ~ min:{fmt(imin)} max:{fmt(imax)} \"\n",
    "            f\"mean:{fmt(imean)} std:{fmt(istd)}\"\n",
    "        )\n",
    "\n",
    "        all_stats[f\"{dataset_name}_{domain}\"] = {\n",
    "            \"splits\": {\"train\": n_train, \"val\": n_val, \"test\": n_test},\n",
    "            \"subset\": train_stats,\n",
    "            \"classnames\": getattr(ds, \"classnames\", None),\n",
    "        }\n",
    "\n",
    "        # bar chart distribuzione classi\n",
    "        plot_histogram(\n",
    "            train_stats[\"class_hist\"],\n",
    "            title=f\"Distribuzione classi (subset) - {dataset_name} {domain}\",\n",
    "            classnames=ds.classnames if hasattr(ds, \"classnames\") else None,\n",
    "        )\n",
    "\n",
    "# salva riepilogo su file\n",
    "try:\n",
    "    out_file = OUTPUTS_PATH / \"dataset_stats.json\"\n",
    "    with open(out_file, \"w\") as f:\n",
    "        json.dump(all_stats, f, indent=2)\n",
    "    print(f\"\\nSalvato riepilogo in: {out_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore salvataggio stats: {e}\")\n",
    "\n",
    "print(\"\\nRiepilogo sintetico:\")\n",
    "for k, v in all_stats.items():\n",
    "    s = v[\"splits\"]\n",
    "    print(\n",
    "        f\" - {k}: train={s['train']}, val={s['val']}, test={s['test']} | visti(subset)={v['subset']['n_seen']}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
