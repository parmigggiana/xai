{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0bb3b4",
   "metadata": {},
   "source": [
    "In Kaggle, add the following to the dependencies:\n",
    "```\n",
    "pip install torch\n",
    "pip install numpy\n",
    "pip install pydicom\n",
    "pip install PILlow\n",
    "pip install matplotlib\n",
    "```\n",
    "Enable file persistence and internet access.\n",
    "Remember that you can run the whole notebook and close the runtime without wasting resources by going to File > Save Version > Save & Run All (Double check that GPU is selected in the advanced settings).\n",
    "Later, by going to 'File' > 'Version history' you can view the full logs and download the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c8f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle\n",
    "import os\n",
    "\n",
    "IN_KAGGLE = False\n",
    "if os.environ.get(\"KAGGLE_URL_BASE\", \"\"):\n",
    "    IN_KAGGLE = True\n",
    "    !git clone https://github.com/parmigggiana/xai /kaggle/working/xai\n",
    "    %cd xai\n",
    "    !git fetch\n",
    "    !git reset --hard origin/main\n",
    "    %pip install 'monai[einops,itk,nibabel]>=1.5.0' git+https://github.com/timojl/clipseg.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c6af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Removed custom MetadataAwareTransform/MetadataCompose: using MONAI transforms.Compose with MetaTensor support in ImageDataset)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_vector import TaskVector\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"CHAOS\", \"MMWHS\"]\n",
    "DOMAINS = [\"MR\", \"CT\"]\n",
    "DATA_PATH = \"data/\"\n",
    "CHECKPOINT_PATH = \"checkpoints/\"\n",
    "OUTPUTS_PATH = \"outputs/\"\n",
    "USE_3D = False\n",
    "TRAINING_EPOCHS = {\n",
    "    (\"MMWHS\", \"MR\"): 10,\n",
    "    (\"MMWHS\", \"CT\"): 10,\n",
    "    (\"CHAOS\", \"MR\"): 10,\n",
    "    (\"CHAOS\", \"CT\"): 10,\n",
    "\n",
    "}\n",
    "BATCH_SIZE = 8\n",
    "SPATIAL_SIZE = 96\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(CHECKPOINT_PATH)\n",
    "OUTPUTS_PATH = Path(OUTPUTS_PATH)\n",
    "DATA_PATH = Path(DATA_PATH)\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if USE_3D:\n",
    "    encoder_type = \"swin_unetr\"\n",
    "else:\n",
    "    encoder_type = \"clipseg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai import transforms\n",
    "from monai.data import MetaTensor\n",
    "\n",
    "\n",
    "def update_metrics(name, new_metrics):\n",
    "    metrics_file = OUTPUTS_PATH / \"metrics.json\"\n",
    "\n",
    "    if not metrics_file.exists():\n",
    "        metrics = {}\n",
    "    else:\n",
    "        with open(metrics_file, \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "    metrics[name] = new_metrics\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "\n",
    "def debug_metadata(data):\n",
    "    \"\"\"Debug transform to print metadata information\"\"\"\n",
    "    print(f\"🔍 DEBUG - Data type: {type(data)}\")\n",
    "    if hasattr(data, \"meta\"):\n",
    "        print(\n",
    "            f\"🔍 DEBUG - Metadata keys: {list(data.meta.keys()) if data.meta else 'No meta'}\"\n",
    "        )\n",
    "        print(f\"🔍 DEBUG - Full metadata: {data.meta}\")\n",
    "    if hasattr(data, \"shape\"):\n",
    "        print(f\"🔍 DEBUG - Shape: {data.shape}\")\n",
    "    if hasattr(data, \"dtype\"):\n",
    "        print(f\"🔍 DEBUG - Dtype: {data.dtype}\")\n",
    "    print(\"🔍 DEBUG - \" + \"=\" * 50)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_preprocessing(dataset_name: str, domain: str, is_training=True):\n",
    "    \"\"\"\n",
    "    Get preprocessing pipelines for images and segmentations using MONAI-native transforms.\n",
    "\n",
    "    Returns separate Compose pipelines for images and segmentations, leveraging MetaTensor\n",
    "    propagation done in ImageDataset when transform_with_metadata=True.\n",
    "    \"\"\"\n",
    "    # Image-specific transforms (applied to image files)\n",
    "    decode_func = get_decode_func(dataset_name, domain)\n",
    "\n",
    "    if USE_3D:\n",
    "        image_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "    else:\n",
    "        image_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "\n",
    "    # Domain-specific intensity normalization for images\n",
    "    if domain == \"CT\":\n",
    "        image_transforms.append(\n",
    "            transforms.ScaleIntensityRange(\n",
    "                a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True\n",
    "            ),\n",
    "        )\n",
    "    else:  # MR\n",
    "        image_transforms.append(\n",
    "            transforms.NormalizeIntensity(nonzero=True, channel_wise=True),\n",
    "        )\n",
    "\n",
    "    # Training augmentations (image-only)\n",
    "    if is_training:\n",
    "        image_transforms.extend(\n",
    "            [\n",
    "                transforms.RandGaussianNoise(prob=0.2, std=0.05),\n",
    "                transforms.RandAdjustContrast(prob=0.2, gamma=(0.9, 1.1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if not USE_3D:\n",
    "        image_transforms.append(transforms.RepeatChannel(repeats=3))\n",
    "\n",
    "    image_transforms.extend(\n",
    "        [\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE,\n",
    "                size_mode=\"longest\",\n",
    "                mode=\"area\",\n",
    "                anti_aliasing=True,\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Segmentation-specific transforms (applied to segmentation files)\n",
    "    if not USE_3D:\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "    else:\n",
    "        seg_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "\n",
    "    seg_transforms.extend(\n",
    "        [\n",
    "            transforms.Lambda(lambda x: decode_func(x)),\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE, size_mode=\"longest\", mode=\"nearest\"\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Use MONAI's native Compose\n",
    "    image_transform = transforms.Compose(image_transforms)\n",
    "    seg_transform = transforms.Compose(seg_transforms)\n",
    "\n",
    "    return image_transform, seg_transform\n",
    "\n",
    "\n",
    "def get_decode_func(dataset_name, domain):\n",
    "    from src.datasets.mmwhs import mmwhs_labels\n",
    "\n",
    "    decode = None\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        if domain in [\"MR\", \"MRI\"]:\n",
    "            def decode(labels):\n",
    "                # Convert intensity values to class indices (keep as float32)\n",
    "                return labels // 63\n",
    "        elif domain == \"CT\":\n",
    "            def decode(labels):\n",
    "                return torch.where(labels > 0, 1.0, 0.0)\n",
    "    elif dataset_name == \"MMWHS\":\n",
    "        def decode(labels):\n",
    "            decoded_labels = torch.zeros_like(labels, dtype=torch.float32)\n",
    "            for i, label_val in enumerate(mmwhs_labels.keys()):\n",
    "                decoded_labels[labels == label_val] = i\n",
    "            return decoded_labels\n",
    "\n",
    "    if decode is None:\n",
    "        print(\n",
    "            f\"Warning: No decode function defined for {dataset_name} in {domain}. Returning labels unchanged.\"\n",
    "        )\n",
    "        def decode(labels):\n",
    "            return labels\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning on MMWHS dataset in MR domain with 2d images \n",
      "Dataset MR total samples: 2898\n",
      "Split sizes - Train: 2028, Val: 434, Test: 436\n",
      "\n",
      "Dataset: MMWHS, Domain: MR\n",
      "Number of training samples: 2028\n",
      "Number of validation samples: 434\n",
      "Number of test samples: 436\n",
      "Image shape: torch.Size([3, 96, 96])\n",
      "Segmentation shape: torch.Size([1, 96, 96])\n",
      "Number of classes: 8\n",
      "\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "🔄 Loading CLIPSeg weights...\n",
      "🔧 DEBUG: Initial model parameter check\n",
      "   Total parameters: 356\n",
      "   Trainable parameters: 54\n",
      "   Model device: cpu\n",
      "\n",
      "Processing MMWHS in MR domain with 2d images\n",
      "🔧 DEBUG: Before unfreeze()\n",
      "   Frozen parameters before unfreeze: 302\n",
      "   Frozen parameters after unfreeze: 0\n",
      "   Total trainable parameters: 356\n",
      "\n",
      "🔧 DEBUG: Starting full model finetuning\n",
      "🚀 Starting training for 10 epochs\n",
      "   Device: cpu\n",
      "   Learning Rate: 0.0005\n",
      "   Weight Decay: 0\n",
      "   Params: total=150,796,962, trainable=150,796,962\n",
      "   Batches: train=254, val=55\n",
      "   Tracking params:\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "\n",
      "📖 Epoch 1/10\n",
      "   LR(s): 5.000000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/254 [00:11<49:26, 11.72s/it, Loss=0.9963]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 0: 4.693s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 2/254 [00:16<32:38,  7.77s/it, Loss=0.8686]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 1: 3.995s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 3/254 [00:21<26:59,  6.45s/it, Loss=0.8446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 2: 4.268s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 4/254 [00:26<24:32,  5.89s/it, Loss=0.7657]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 3: 4.173s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 5/254 [00:31<22:20,  5.38s/it, Loss=0.7663]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 4: 4.353s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 6/254 [00:35<20:28,  4.95s/it, Loss=0.7468]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 5: 4.019s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 6/254 [00:35<24:43,  5.98s/it, Loss=0.7468]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# 🔧 DEBUG: Monitor parameter changes during training\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🔧 DEBUG: Starting full model finetuning\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# 🔧 DEBUG: Check if parameters actually changed\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🔧 DEBUG: Parameter change analysis after finetuning\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/semantic_segmentation.py:438\u001b[39m, in \u001b[36mMedicalSegmenter.finetune\u001b[39m\u001b[34m(self, epochs, learning_rate, weight_decay, save_best, max_grad_norm, visualize_batches, early_stop_patience, profile, profile_dir)\u001b[39m\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    437\u001b[39m train_pbar = tqdm(\u001b[38;5;28mself\u001b[39m.dataset.train_loader, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_pbar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuccess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbg\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_training_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m                \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/y5fslgbwfjvkczl1g3ynkd0pp95j3rwg-python3.12-tqdm-4.67.1/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/ImageDataset.py:179\u001b[39m, in \u001b[36mImageDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    177\u001b[39m         meta_data = \u001b[38;5;28mdict\u001b[39m(meta_data)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     img, meta_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_cache:\n\u001b[32m    181\u001b[39m         _cache_put(\u001b[38;5;28mself\u001b[39m._img_cache, img_key, (img, meta_data))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/transforms/io/array.py:289\u001b[39m, in \u001b[36mLoadImage.__call__\u001b[39m\u001b[34m(self, filename, reader)\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    283\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot find a suitable reader for file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    284\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    Please install the reader libraries, see also the installation instructions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   The current registered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.readers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    287\u001b[39m     )\n\u001b[32m    288\u001b[39m img_array: NdarrayOrTensor\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m img_array, meta_data = \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m img_array = convert_to_dst_type(img_array, dst=img_array, dtype=\u001b[38;5;28mself\u001b[39m.dtype)[\u001b[32m0\u001b[39m]\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(meta_data, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/data/image_reader.py:1121\u001b[39m, in \u001b[36mNibabelReader.get_data\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m   1119\u001b[39m header[MetaKeys.SPATIAL_SHAPE] = \u001b[38;5;28mself\u001b[39m._get_spatial_shape(i)\n\u001b[32m   1120\u001b[39m header[MetaKeys.SPACE] = SpaceKeys.RAS\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_array_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.squeeze_non_spatial_dims:\n\u001b[32m   1123\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data.shape), \u001b[38;5;28mlen\u001b[39m(header[MetaKeys.SPATIAL_SHAPE]), -\u001b[32m1\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/data/image_reader.py:1215\u001b[39m, in \u001b[36mNibabelReader._get_array_data\u001b[39m\u001b[34m(self, img, filename)\u001b[39m\n\u001b[32m   1213\u001b[39m     data_dtype = img.dataobj.dtype\n\u001b[32m   1214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image[data_offset:].view(data_dtype).reshape(data_shape, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in TRAINING_EPOCHS.items():\n",
    "    download_and_extract_dataset(dataset_name, DATA_PATH)\n",
    "\n",
    "    image_transform, seg_transform = get_preprocessing(\n",
    "        dataset_name, domain, is_training=True\n",
    "    )\n",
    "\n",
    "    filename = f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned.pth\"\n",
    "    filename = CHECKPOINT_PATH / filename\n",
    "    # Check if the finetuned checkpoint already exists\n",
    "    if filename.exists():\n",
    "        print(\n",
    "            f\"Finetuned model for {dataset_name} in {domain} domain with {'3d' if USE_3D else '2d'} images already exists at {filename}. Skipping finetuning.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        transform=image_transform,  # Use transform instead of preprocess\n",
    "        seg_transform=seg_transform,  # Pass seg_transform too\n",
    "        base_path=DATA_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=0,\n",
    "        slice_2d=not USE_3D,\n",
    "    )\n",
    "\n",
    "\n",
    "    #  Ensure the dataset is loaded correctly\n",
    "    if not isinstance(dataset, BaseDataset):\n",
    "        raise TypeError(\n",
    "            f\"Expected dataset to be an instance of BaseDataset, got {type(dataset)}\"\n",
    "        )\n",
    "    # Print dataset information\n",
    "    print()\n",
    "    print(f\"Dataset: {dataset_name}, Domain: {domain}\")\n",
    "    print(f\"Number of training samples: {len(dataset.train_dataset)}\")\n",
    "    print(f\"Number of validation samples: {len(dataset.val_dataset)}\")\n",
    "    print(f\"Number of test samples: {len(dataset.test_dataset)}\")\n",
    "    print(f\"Image shape: {dataset.train_dataset[0][\"image\"].shape}\")\n",
    "    print(f\"Segmentation shape: {dataset.train_dataset[0][\"label\"].shape}\")\n",
    "    print(f\"Number of classes: {dataset.num_classes}\")\n",
    "    print()\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=encoder_type,\n",
    "    )\n",
    "\n",
    "    # 🔧 DEBUG: Check initial model parameters\n",
    "    print(\"🔧 DEBUG: Initial model parameter check\")\n",
    "    initial_params = {}\n",
    "    param_count = 0\n",
    "    trainable_count = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        initial_params[name] = param.clone().detach()\n",
    "        param_count += 1\n",
    "        if param.requires_grad:\n",
    "            trainable_count += 1\n",
    "    print(f\"   Total parameters: {param_count}\")\n",
    "    print(f\"   Trainable parameters: {trainable_count}\")\n",
    "    print(f\"   Model device: {next(model.parameters()).device}\")\n",
    "    print()\n",
    "\n",
    "    # Save the baseline model's state_dict before finetuning\n",
    "    baseline_filename = (\n",
    "        CHECKPOINT_PATH\n",
    "        / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "    )\n",
    "    torch.save(model.encoder, baseline_filename)\n",
    "    print(\n",
    "        f\"Processing {dataset_name} in {domain} domain with {'3d' if USE_3D else '2d'} images\"\n",
    "    )\n",
    "\n",
    "    if USE_3D:\n",
    "        print(\n",
    "            f\"Warning: Using 3D model requires SWIN UNETR, which is not compatible with zero-shot training.\"\n",
    "        )\n",
    "\n",
    "        # 🔧 DEBUG: Check freeze_body functionality\n",
    "        print(\"🔧 DEBUG: Before freeze_body()\")\n",
    "        frozen_before = sum(1 for p in model.parameters() if not p.requires_grad)\n",
    "        model.freeze_body()\n",
    "        frozen_after = sum(1 for p in model.parameters() if not p.requires_grad)\n",
    "        print(f\"   Frozen parameters before: {frozen_before}\")\n",
    "        print(f\"   Frozen parameters after: {frozen_after}\")\n",
    "        print(f\"   Parameters frozen: {frozen_after - frozen_before}\")\n",
    "\n",
    "        # Check which parameters are trainable\n",
    "        print(\"   Trainable parameters after freeze_body:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"     {name}: {param.shape}\")\n",
    "        print()\n",
    "\n",
    "        model.finetune(\n",
    "            epochs=epochs, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "        metrics = model.evaluate()\n",
    "        update_metrics(\n",
    "            f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_head\",\n",
    "            metrics,\n",
    "        )\n",
    "\n",
    "    # 🔧 DEBUG: Check unfreeze functionality\n",
    "    print(\"🔧 DEBUG: Before unfreeze()\")\n",
    "    frozen_before = sum(1 for p in model.parameters() if not p.requires_grad)\n",
    "    model.unfreeze()\n",
    "    frozen_after = sum(1 for p in model.parameters() if not p.requires_grad)\n",
    "    print(f\"   Frozen parameters before unfreeze: {frozen_before}\")\n",
    "    print(f\"   Frozen parameters after unfreeze: {frozen_after}\")\n",
    "    print(f\"   Total trainable parameters: {sum(1 for p in model.parameters() if p.requires_grad)}\")\n",
    "    print()\n",
    "\n",
    "    # 🔧 DEBUG: Monitor parameter changes during training\n",
    "    print(\"🔧 DEBUG: Starting full model finetuning\")\n",
    "    history = model.finetune(\n",
    "        epochs=epochs,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "    # 🔧 DEBUG: Check if parameters actually changed\n",
    "    print(\"🔧 DEBUG: Parameter change analysis after finetuning\")\n",
    "    changed_params = 0\n",
    "    unchanged_params = 0\n",
    "    max_change = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in initial_params:\n",
    "            param_change = (param - initial_params[name]).norm().item()\n",
    "            if param_change > 1e-8:  # Consider very small changes as unchanged\n",
    "                changed_params += 1\n",
    "                max_change = max(max_change, param_change)\n",
    "            else:\n",
    "                unchanged_params += 1\n",
    "                print(f\"   Parameter {name} didn't change during training!\")\n",
    "\n",
    "    print(f\"   Parameters that changed: {changed_params}\")\n",
    "    print(f\"   Parameters that didn't change: {unchanged_params}\")\n",
    "    print(f\"   Maximum parameter change: {max_change:.6f}\")\n",
    "\n",
    "    if changed_params == 0:\n",
    "        print(\"   ⚠️ WARNING: No parameters changed during training!\")\n",
    "    elif max_change < 1e-6:\n",
    "        print(f\"   ⚠️ WARNING: Very small parameter changes (max: {max_change:.8f})\")\n",
    "    else:\n",
    "        print(\"   ✅ Parameters updated successfully\")\n",
    "    print()\n",
    "\n",
    "    # 🔧 DEBUG: Check training history\n",
    "    if history:\n",
    "        print(\"🔧 DEBUG: Training history analysis\")\n",
    "        if 'train_loss' in history:\n",
    "            train_losses = history['train_loss']\n",
    "            print(f\"   Training losses: {train_losses[:5]}...{train_losses[-5:] if len(train_losses) > 5 else train_losses}\")\n",
    "            print(f\"   Loss range: {min(train_losses):.6f} - {max(train_losses):.6f}\")\n",
    "            if len(train_losses) > 1:\n",
    "                loss_change = abs(train_losses[-1] - train_losses[0])\n",
    "                print(f\"   Total loss change: {loss_change:.6f}\")\n",
    "                if loss_change < 1e-6:\n",
    "                    print(\"   ⚠️ WARNING: Training loss barely changed!\")\n",
    "        else:\n",
    "            print(\"   ⚠️ No 'train_loss' found in history\")\n",
    "        print(f\"   History keys: {list(history.keys()) if history else 'None'}\")\n",
    "    else:\n",
    "        print(\"🔧 DEBUG: No training history returned\")\n",
    "    print()\n",
    "\n",
    "    # Save the finetuned model's state_dict\n",
    "    torch.save(model.encoder, filename)\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned\",\n",
    "        model_metrics,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090233",
   "metadata": {},
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWIN UNETR Task Vectors\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from monai.networks.nets.swin_unetr import BasicLayer\n",
    "from monai.networks.nets.swin_unetr import SwinTransformerBlock\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from monai.networks.nets.swin_unetr import WindowAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules.linear import Identity\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from torch.nn.modules.activation import GELU\n",
    "from monai.networks.nets.swin_unetr import PatchMerging\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from monai.networks.blocks.unetr_block import UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from torch.nn.modules.conv import ConvTranspose3d\n",
    "\n",
    "safe_globals = [\n",
    "    SwinUNETR,\n",
    "    SwinTransformer,\n",
    "    PatchEmbed,\n",
    "    Conv3d,\n",
    "    Dropout,\n",
    "    ModuleList,\n",
    "    BasicLayer,\n",
    "    SwinTransformerBlock,\n",
    "    LayerNorm,\n",
    "    WindowAttention,\n",
    "    Linear,\n",
    "    Softmax,\n",
    "    Identity,\n",
    "    MLPBlock,\n",
    "    GELU,\n",
    "    PatchMerging,\n",
    "    UnetrBasicBlock,\n",
    "    UnetResBlock,\n",
    "    Convolution,\n",
    "    LeakyReLU,\n",
    "    InstanceNorm3d,\n",
    "    UnetrUpBlock,\n",
    "    ConvTranspose3d,\n",
    "    UnetOutBlock,\n",
    "]\n",
    "##\n",
    "\n",
    "## CLIPSeg Task Vectors\n",
    "from src.CLIPSeg import CLIPSeg\n",
    "from clipseg.clipseg import CLIPDensePredT\n",
    "from clip.model import (\n",
    "    CLIP,\n",
    "    VisionTransformer,\n",
    "    LayerNorm,\n",
    "    Transformer,\n",
    "    ResidualAttentionBlock,\n",
    "    QuickGELU,\n",
    ")\n",
    "from torch.nn.modules.conv import Conv2d, ConvTranspose2d\n",
    "from torch.nn.modules.container import Sequential\n",
    "from torch.nn.modules.activation import MultiheadAttention, ReLU\n",
    "from torch.nn.modules.linear import NonDynamicallyQuantizableLinear\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.nn.modules.transformer import (\n",
    "    TransformerEncoderLayer,\n",
    "    TransformerEncoder,\n",
    "    TransformerDecoderLayer,\n",
    "    TransformerDecoder,\n",
    ")\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn.modules.container import ModuleDict\n",
    "\n",
    "safe_globals.extend(\n",
    "    [\n",
    "        CLIPSeg,\n",
    "        CLIPDensePredT,\n",
    "        CLIP,\n",
    "        VisionTransformer,\n",
    "        Conv2d,\n",
    "        LayerNorm,\n",
    "        Transformer,\n",
    "        Sequential,\n",
    "        ResidualAttentionBlock,\n",
    "        MultiheadAttention,\n",
    "        NonDynamicallyQuantizableLinear,\n",
    "        QuickGELU,\n",
    "        Embedding,\n",
    "        ReLU,\n",
    "        ConvTranspose2d,\n",
    "        TransformerEncoderLayer,\n",
    "        TransformerEncoder,\n",
    "        TransformerDecoderLayer,\n",
    "        TransformerDecoder,\n",
    "        relu,\n",
    "        ModuleDict,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for domain in DOMAINS:\n",
    "        print(\n",
    "            f\"Building task vector for {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images\"\n",
    "        )\n",
    "        baseline_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        finetuned_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned.pth\"\n",
    "        )\n",
    "        if not baseline_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Baseline checkpoint for {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "        if not finetuned_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Finetuned checkpoint {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        with torch.serialization.safe_globals(\n",
    "            safe_globals=safe_globals,\n",
    "        ):\n",
    "            task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "            # Remove keys associated with the output layers from the task vector\n",
    "            # For swin it's all layers starting with '.out'\n",
    "            # For clipseg it might not be necessary since the model architecture isn't dependent on the number of output features\n",
    "            if encoder_type == \"swin_unetr\":\n",
    "                for k in task_vector.keys():\n",
    "                    if k.startswith(\".out\"):\n",
    "                        del task_vector[k]\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS_CT\": task_vectors[\"MMWHS_MR\"]\n",
    "    + task_vectors[\"CHAOS_CT\"]\n",
    "    - task_vectors[\"CHAOS_MR\"],\n",
    "    \"MMWHS_MR\": task_vectors[\"MMWHS_CT\"]\n",
    "    + task_vectors[\"CHAOS_MR\"]\n",
    "    - task_vectors[\"CHAOS_CT\"],\n",
    "    \"CHAOS_CT\": task_vectors[\"CHAOS_MR\"]\n",
    "    + task_vectors[\"MMWHS_CT\"]\n",
    "    - task_vectors[\"MMWHS_MR\"],\n",
    "    \"CHAOS_MR\": task_vectors[\"CHAOS_CT\"]\n",
    "    + task_vectors[\"MMWHS_MR\"]\n",
    "    - task_vectors[\"MMWHS_CT\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 Task Vector Cross-Domain Adaptation Experiments\n",
    "print(\"🔄 Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for target_domain in DOMAINS:\n",
    "        print(f\"\\n{dataset_name}: {target_domain} adaptation\")\n",
    "\n",
    "        image_transform, seg_transform = get_preprocessing(\n",
    "            dataset_name, domain, is_training=False\n",
    "        )\n",
    "\n",
    "        dataset_kwargs = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"base_path\": DATA_PATH,\n",
    "            \"domain\": target_domain,\n",
    "            \"transform\": image_transform,  # Use transform instead of preprocess\n",
    "            \"seg_transform\": seg_transform,  # Pass seg_transform too\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"num_workers\": 0,\n",
    "            \"slide_2d\": not USE_3D,\n",
    "        }\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\":\n",
    "            extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "        # try:\n",
    "        target_dataset = get_dataset(**dataset_kwargs, **extra_kwargs)\n",
    "\n",
    "        composite_key = f\"{dataset_name}_{target_domain}\"\n",
    "        if composite_key in composite_task_vectors:\n",
    "            composite_task_vector = composite_task_vectors[composite_key]\n",
    "\n",
    "            target_model = target_dataset.get_model(encoder_type=encoder_type)\n",
    "            target_model.load_task_vector(composite_task_vector)\n",
    "\n",
    "            metrics = target_model.evaluate()\n",
    "            update_metrics(f\"{composite_key}_adaptation\", metrics)\n",
    "            print(\n",
    "                f\"   ✅ {composite_key}: Dice={metrics.get('train', {}).get('dice', 0):.3f}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   ⚠️ No composite task vector found for {composite_key}\")\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"   ❌ {dataset_name} {target_domain}: {str(e)[:100]}...\")\n",
    "        #     import traceback\n",
    "        #     traceback.print_exc()\n",
    "        #     # continue\n",
    "        #     break\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display all metrics\n",
    "metrics_file = OUTPUTS_PATH / \"metrics.json\"\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        all_metrics = json.load(f)\n",
    "\n",
    "    print(\"\\n📊 COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline performance\n",
    "    print(\"\\n🏁 Baseline Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"baseline\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # After Head-training performance\n",
    "    print(\"\\n🏋️‍♂️ After Head-Training Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"head\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # Finetuned performance\n",
    "    print(\"\\n🏆 Finetuned Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"finetuned\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # Cross-domain adaptation results\n",
    "    print(\"\\n🔄 Cross-Domain Adaptation Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"adaptation\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "else:\n",
    "    print(\"No metrics file found. Run the experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "\n",
    "    # Copy checkpoints.zip to Google Drive\n",
    "    !zip -r /content/checkpoints.zip /content/xai/checkpoints\n",
    "    shutil.copy(\n",
    "        \"/content/checkpoints.zip\", \"/content/drive/MyDrive/xai/checkpoints.zip\"\n",
    "    )\n",
    "\n",
    "    # Copy metrics.json to Google Drive\n",
    "    shutil.copy(\n",
    "        \"/content/xai/outputs/metrics.json\", \"/content/drive/MyDrive/xai/metrics.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    !zip -r /kaggle/working/checkpoints.zip /kaggle/working/xai/checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
