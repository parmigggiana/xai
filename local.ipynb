{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0bb3b4",
   "metadata": {},
   "source": [
    "In Kaggle, \n",
    "Enable file persistence and internet access.\n",
    "Remember that you can run the whole notebook and close the runtime without wasting resources by going to File > Save Version > Save & Run All (Double check that GPU is selected in the advanced settings).\n",
    "Later, by going to 'File' > 'Version history' you can view the full logs and download the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c8f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle\n",
    "import os\n",
    "\n",
    "IN_KAGGLE = False\n",
    "if os.environ.get(\"KAGGLE_URL_BASE\", \"\"):\n",
    "    IN_KAGGLE = True\n",
    "    !git clone https://github.com/parmigggiana/xai /kaggle/working/xai\n",
    "    %cd xai\n",
    "    !git fetch\n",
    "    !git reset --hard origin/main\n",
    "    %pip install 'monai[einops,itk,nibabel]>=1.5.0' git+https://github.com/timojl/clipseg.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "IN_COLAB = False\n",
    "if not IN_KAGGLE:\n",
    "    try:\n",
    "        import google.colab\n",
    "        from google.colab import drive\n",
    "\n",
    "        IN_COLAB = True\n",
    "        import os\n",
    "\n",
    "        drive.mount(\"/content/drive\")\n",
    "        os.makedirs(\"/content/drive/MyDrive/xai\", exist_ok=True)\n",
    "        !git clone https://github.com/parmigggiana/xai /content/xai\n",
    "        %cd /content/xai\n",
    "        !git fetch\n",
    "        !git reset --hard origin/main\n",
    "        %pip install -r requirements.txt\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_vector import TaskVector\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"CHAOS\", \"MMWHS\"]\n",
    "DOMAINS = [\"CT\", \"MR\"]\n",
    "DATA_PATH = \"data/\"\n",
    "CHECKPOINT_PATH = \"checkpoints/\"\n",
    "OUTPUTS_PATH = \"outputs/\"\n",
    "USE_3D = False\n",
    "TRAINING_EPOCHS = {\n",
    "    (\"CHAOS\", \"CT\"): 100,\n",
    "    (\"CHAOS\", \"MR\"): 100,\n",
    "    (\"MMWHS\", \"CT\"): 100,\n",
    "    (\"MMWHS\", \"MR\"): 100,\n",
    "}\n",
    "BATCH_SIZE = 8\n",
    "SPATIAL_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 5e-5\n",
    "# Number of DataLoader workers (set >0 to enable parallel data loading)\n",
    "NUM_WORKERS = 0 if not IN_KAGGLE and not IN_COLAB else 2\n",
    "# Set True to enable debug prints/timers/visualizations)\n",
    "DEBUG = False\n",
    "\n",
    "# Profiling controls: False | 'cprofile' | 'torch'\n",
    "PROFILE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(CHECKPOINT_PATH)\n",
    "OUTPUTS_PATH = Path(OUTPUTS_PATH)\n",
    "DATA_PATH = Path(DATA_PATH)\n",
    "PROFILE_DIR = OUTPUTS_PATH / \"profiling\"\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "PROFILE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if USE_3D:\n",
    "    encoder_type = \"swin_unetr\"\n",
    "else:\n",
    "    encoder_type = \"clipseg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai import transforms\n",
    "\n",
    "\n",
    "def update_metrics(name, new_metrics):\n",
    "    metrics_file = OUTPUTS_PATH / \"metrics.json\"\n",
    "\n",
    "    if not metrics_file.exists():\n",
    "        metrics = {}\n",
    "    else:\n",
    "        with open(metrics_file, \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "    metrics[name] = new_metrics\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "\n",
    "# Normalization stats (mean, std) per dataset/domain\n",
    "NORM_STATS = {\n",
    "    (\"MMWHS\", \"MR\"): (186.5875, 258.5917),\n",
    "    (\"MMWHS\", \"CT\"): (-745.0086, 1042.7251),\n",
    "    (\"CHAOS\", \"MR\"): (90.8292, 168.8922),\n",
    "    (\"CHAOS\", \"CT\"): (-478.1732, 476.7163),\n",
    "}\n",
    "\n",
    "# Optimized preprocessing: resize early\n",
    "\n",
    "\n",
    "def get_preprocessing(dataset_name: str, domain: str, is_training=True):\n",
    "    decode_func = get_decode_func(dataset_name, domain)\n",
    "    mean_std = NORM_STATS.get((dataset_name, domain))\n",
    "    mean, std = mean_std if mean_std is not None else (None, None)\n",
    "\n",
    "    # Image-specific transforms\n",
    "    if USE_3D:\n",
    "        image_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "    else:\n",
    "        image_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "\n",
    "    # Resize early to reduce compute\n",
    "    image_transforms.append(\n",
    "        transforms.Resize(\n",
    "            spatial_size=SPATIAL_SIZE,\n",
    "            size_mode=\"longest\",\n",
    "            mode=\"area\",\n",
    "            anti_aliasing=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert to tensor and ensure float32 for stable CPU ops\n",
    "    image_transforms.extend(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Normalize (still in float32)\n",
    "    if mean is not None and std is not None:\n",
    "        image_transforms.append(\n",
    "            transforms.NormalizeIntensity(\n",
    "                subtrahend=float(mean),\n",
    "                divisor=float(std),\n",
    "                channel_wise=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Augmentations (training only) — run in float32 on CPU\n",
    "    if is_training:\n",
    "        image_transforms.extend(\n",
    "            [\n",
    "                transforms.RandGaussianNoise(prob=0.15, std=0.05),\n",
    "                transforms.RandAdjustContrast(prob=0.15, gamma=(0.95, 1.05)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Repeat to 3 channels only at the end (2D only)\n",
    "    if not USE_3D:\n",
    "        image_transforms.append(transforms.RepeatChannel(repeats=3))\n",
    "\n",
    "    image_transform = transforms.Compose(image_transforms)\n",
    "\n",
    "    # Segmentation transforms\n",
    "    if not USE_3D:\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "    else:\n",
    "        seg_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "\n",
    "    seg_transforms.extend(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.long),\n",
    "            transforms.Lambda(\n",
    "                lambda x: decode_func(x)\n",
    "            ),  # decode after tensor conversion\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE, size_mode=\"longest\", mode=\"nearest\"\n",
    "            ),\n",
    "            # transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    seg_transform = transforms.Compose(seg_transforms)\n",
    "    return image_transform, seg_transform\n",
    "\n",
    "\n",
    "def get_decode_func(dataset_name, domain):\n",
    "    from src.datasets.mmwhs import mmwhs_labels\n",
    "\n",
    "    decode = None\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        if domain in [\"MR\", \"MRI\"]:\n",
    "\n",
    "            def decode(labels):\n",
    "                # Convert intensity values to class indices (keep as float32)\n",
    "                return labels // 63\n",
    "\n",
    "        elif domain == \"CT\":\n",
    "\n",
    "            def decode(labels):\n",
    "                return torch.where(labels > 0, 1.0, 0.0)\n",
    "\n",
    "    elif dataset_name == \"MMWHS\":\n",
    "\n",
    "        def decode(labels):\n",
    "            decoded_labels = torch.zeros_like(labels, dtype=torch.float32)\n",
    "            for i, label_val in enumerate(mmwhs_labels.keys()):\n",
    "                decoded_labels[labels == label_val] = i\n",
    "            return decoded_labels\n",
    "\n",
    "    if decode is None:\n",
    "\n",
    "        def decode(labels):\n",
    "            return labels\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in TRAINING_EPOCHS.items():\n",
    "    download_and_extract_dataset(dataset_name, DATA_PATH)\n",
    "\n",
    "    image_transform, seg_transform = get_preprocessing(\n",
    "        dataset_name, domain, is_training=True\n",
    "    )\n",
    "\n",
    "    filename = f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned.pth\"\n",
    "    filename = CHECKPOINT_PATH / filename\n",
    "    # Check if the finetuned checkpoint already exists\n",
    "    if filename.exists():\n",
    "        print(\n",
    "            f\"Finetuned model for {dataset_name} in {domain} domain with {'3d' if USE_3D else '2d'} images already exists at {filename}. Skipping finetuning.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        transform=image_transform,\n",
    "        seg_transform=seg_transform,\n",
    "        base_path=DATA_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        slice_2d=not USE_3D,\n",
    "    )\n",
    "\n",
    "    #  Ensure the dataset is loaded correctly\n",
    "    if not isinstance(dataset, BaseDataset):\n",
    "        raise TypeError(\n",
    "            f\"Expected dataset to be an instance of BaseDataset, got {type(dataset)}\"\n",
    "        )\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=encoder_type,\n",
    "    )\n",
    "\n",
    "    # Save the baseline model's state_dict before finetuning\n",
    "    baseline_filename = (\n",
    "        CHECKPOINT_PATH\n",
    "        / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "    )\n",
    "    torch.save(model.encoder, baseline_filename)\n",
    "\n",
    "    if USE_3D:\n",
    "        model.freeze_body()\n",
    "        model.finetune(\n",
    "            epochs=epochs,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            profile=PROFILE,\n",
    "            profile_dir=str(PROFILE_DIR),\n",
    "        )\n",
    "        metrics = model.evaluate(profile=PROFILE, profile_dir=str(PROFILE_DIR))\n",
    "        update_metrics(\n",
    "            f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_head\",\n",
    "            metrics,\n",
    "        )\n",
    "\n",
    "    # Train Only Segmentation Head\n",
    "    # pass\n",
    "\n",
    "    # Train Visual Encoder + Segmentation head\n",
    "    # model.unfreeze()\n",
    "    # model.freeze_text_encoder()\n",
    "\n",
    "    # Train Last 2 ResBlocks of Visual Encoder + Segmentation head\n",
    "    # for p in model.encoder.clipseg.reduce.parameters():  # Not in forward pass anyway\n",
    "    #     p.requires_grad = False\n",
    "    # for i in range(8, 10):\n",
    "    #     for p in model.encoder.clipseg.clip_model.visual.transformer.resblocks[\n",
    "    #         i\n",
    "    #     ].parameters():\n",
    "    #         p.requires_grad = True\n",
    "\n",
    "    # Train Only Visual Encoder\n",
    "    # for p in model.encoder.clipseg.model.parameters():\n",
    "    #     p.requires_grad_(not p.requires_grad)\n",
    "    # model.freeze_text_encoder()\n",
    "\n",
    "    history = model.finetune(\n",
    "        epochs=epochs,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        debug=DEBUG,\n",
    "        profile=PROFILE,\n",
    "        profile_dir=str(PROFILE_DIR),\n",
    "    )\n",
    "\n",
    "    torch.save(model.encoder, filename)\n",
    "    model_metrics = model.evaluate(profile=PROFILE, profile_dir=str(PROFILE_DIR))\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned\",\n",
    "        model_metrics,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090233",
   "metadata": {},
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWIN UNETR Task Vectors\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from monai.networks.nets.swin_unetr import BasicLayer\n",
    "from monai.networks.nets.swin_unetr import SwinTransformerBlock\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from monai.networks.nets.swin_unetr import WindowAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules.linear import Identity\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from torch.nn.modules.activation import GELU\n",
    "from monai.networks.nets.swin_unetr import PatchMerging\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from monai.networks.blocks.unetr_block import UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from torch.nn.modules.conv import ConvTranspose3d\n",
    "\n",
    "safe_globals = [\n",
    "    SwinUNETR,\n",
    "    SwinTransformer,\n",
    "    PatchEmbed,\n",
    "    Conv3d,\n",
    "    Dropout,\n",
    "    ModuleList,\n",
    "    BasicLayer,\n",
    "    SwinTransformerBlock,\n",
    "    LayerNorm,\n",
    "    WindowAttention,\n",
    "    Linear,\n",
    "    Softmax,\n",
    "    Identity,\n",
    "    MLPBlock,\n",
    "    GELU,\n",
    "    PatchMerging,\n",
    "    UnetrBasicBlock,\n",
    "    UnetResBlock,\n",
    "    Convolution,\n",
    "    LeakyReLU,\n",
    "    InstanceNorm3d,\n",
    "    UnetrUpBlock,\n",
    "    ConvTranspose3d,\n",
    "    UnetOutBlock,\n",
    "]\n",
    "##\n",
    "\n",
    "## CLIPSeg Task Vectors\n",
    "from src.CLIPSeg import CLIPSeg\n",
    "from clipseg.clipseg import CLIPDensePredT\n",
    "from clip.model import (\n",
    "    CLIP,\n",
    "    VisionTransformer,\n",
    "    LayerNorm,\n",
    "    Transformer,\n",
    "    ResidualAttentionBlock,\n",
    "    QuickGELU,\n",
    ")\n",
    "from torch.nn.modules.conv import Conv2d, ConvTranspose2d\n",
    "from torch.nn.modules.container import Sequential\n",
    "from torch.nn.modules.activation import MultiheadAttention, ReLU\n",
    "from torch.nn.modules.linear import NonDynamicallyQuantizableLinear\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.nn.modules.transformer import (\n",
    "    TransformerEncoderLayer,\n",
    "    TransformerEncoder,\n",
    "    TransformerDecoderLayer,\n",
    "    TransformerDecoder,\n",
    ")\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn.modules.container import ModuleDict\n",
    "\n",
    "from src.CLIPSeg import GradCLIPDensePredT\n",
    "safe_globals.extend(\n",
    "    [\n",
    "        CLIPSeg,\n",
    "        CLIPDensePredT,\n",
    "        GradCLIPDensePredT,\n",
    "        CLIP,\n",
    "        VisionTransformer,\n",
    "        Conv2d,\n",
    "        LayerNorm,\n",
    "        Transformer,\n",
    "        Sequential,\n",
    "        ResidualAttentionBlock,\n",
    "        MultiheadAttention,\n",
    "        NonDynamicallyQuantizableLinear,\n",
    "        QuickGELU,\n",
    "        Embedding,\n",
    "        ReLU,\n",
    "        ConvTranspose2d,\n",
    "        TransformerEncoderLayer,\n",
    "        TransformerEncoder,\n",
    "        TransformerDecoderLayer,\n",
    "        TransformerDecoder,\n",
    "        relu,\n",
    "        ModuleDict,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for domain in DOMAINS:\n",
    "        print(\n",
    "            f\"Building task vector for {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images\"\n",
    "        )\n",
    "        baseline_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        finetuned_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned.pth\"\n",
    "        )\n",
    "        if not baseline_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Baseline checkpoint for {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "        if not finetuned_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Finetuned checkpoint {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        with torch.serialization.safe_globals(\n",
    "            safe_globals=safe_globals,\n",
    "        ):\n",
    "            task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "            # Remove keys associated with the output layers from the task vector\n",
    "            # For swin it's all layers starting with '.out'\n",
    "            # For clipseg it might not be necessary since the model architecture isn't dependent on the number of output features\n",
    "            if encoder_type == \"swin_unetr\":\n",
    "                for k in task_vector.keys():\n",
    "                    if k.startswith(\".out\"):\n",
    "                        del task_vector[k]\n",
    "        # print(task_vector.keys())\n",
    "        # if encoder_type == \"clipseg\":\n",
    "        #     task_vector = task_vector.rename_prefix(\"clipseg.model\", \"clipseg.clip_model\")\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe4664",
   "metadata": {},
   "source": [
    "## Part 1: Improve robustness post-hoc with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS\": (task_vectors[\"MMWHS_MR\"])\n",
    "    + (task_vectors[\"MMWHS_CT\"]),\n",
    "    \"CHAOS\": (task_vectors[\"CHAOS_MR\"])\n",
    "    + (task_vectors[\"CHAOS_CT\"]),\n",
    "    \"MR\": (task_vectors[\"CHAOS_MR\"])\n",
    "     + (task_vectors[\"MMWHS_MR\"]),\n",
    "    \"CT\": (task_vectors[\"CHAOS_CT\"])\n",
    "     + (task_vectors[\"MMWHS_CT\"]),\n",
    "}\n",
    "alpha = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e029b51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train: 100%|██████████| 251/251 [01:00<00:00,  4.16it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train - Dice: 0.2025, Hausdorff: 30.04477310180664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating val: 100%|██████████| 54/54 [00:10<00:00,  5.08it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ val - Dice: 0.2028, Hausdorff: 29.97385025024414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test: 100%|██████████| 54/54 [00:11<00:00,  4.78it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ test - Dice: 0.2047, Hausdorff: 29.896242141723633\n",
      "   ✅ CHAOS at CT: Train Dice=0.203 | Val Dice=0.203\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "🔄 Loading CLIPSeg weights...\n",
      "🔄 Loading CLIPSeg weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train: 100%|██████████| 251/251 [00:45<00:00,  5.49it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train - Dice: 0.2165, Hausdorff: 26.05634117126465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating val: 100%|██████████| 54/54 [00:09<00:00,  5.54it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ val - Dice: 0.2302, Hausdorff: 26.043668746948242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test: 100%|██████████| 54/54 [00:09<00:00,  5.58it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ test - Dice: 0.2139, Hausdorff: 26.094341278076172\n",
      "   ✅ CT at CHAOS: Train Dice=0.217 | Val Dice=0.230\n",
      "\n",
      "CHAOS: MR adaptation\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "🔄 Loading CLIPSeg weights...\n",
      "🔄 Loading CLIPSeg weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train:   0%|          | 0/54 [00:00<?, ?it/s]the ground truth of class 1 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 1 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 2 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 2 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 3 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 3 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 1 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 1 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 2 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 2 is all 0, this may result in nan/inf distance.\n",
      "the ground truth of class 3 is all 0, this may result in nan/inf distance.\n",
      "the prediction of class 3 is all 0, this may result in nan/inf distance.\n",
      "Evaluating train: 100%|██████████| 54/54 [00:35<00:00,  1.52it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train - Dice: 0.0506, Hausdorff: 23.040422439575195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating val:  83%|████████▎ | 10/12 [00:06<00:01,  1.47it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Replace encoder with the adapted one\u001b[39;00m\n\u001b[32m     42\u001b[39m target_model.encoder = new_encoder\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m metrics = \u001b[43mtarget_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m update_metrics(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_composite_at_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_domain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, metrics)\n\u001b[32m     46\u001b[39m train_d = metrics.get(\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[33m\"\u001b[39m\u001b[33mdice\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\semantic_segmentation.py:1277\u001b[39m, in \u001b[36mMedicalSegmenter.evaluate\u001b[39m\u001b[34m(self, visualize, profile, profile_dir, max_batches_per_split, fast_metrics, compute_hausdorff)\u001b[39m\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device.type == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m amp_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, dtype=amp_dtype):\n\u001b[32m-> \u001b[39m\u001b[32m1277\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1279\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m(images)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\semantic_segmentation.py:292\u001b[39m, in \u001b[36mMedicalSegmenter.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    288\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    Override call method to handle both training and inference.\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[33;03m    This allows the model to be used seamlessly in training loops.\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\semantic_segmentation.py:277\u001b[39m, in \u001b[36mMedicalSegmenter.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encoder_type == \u001b[33m\"\u001b[39m\u001b[33mswin_unetr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    275\u001b[39m     x, original_shape = \u001b[38;5;28mself\u001b[39m._pad_input_for_swin_unetr(x)\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encoder_type == \u001b[33m\"\u001b[39m\u001b[33mswin_unetr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    280\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._crop_output_to_original_size(result, original_shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\CLIPSeg.py:331\u001b[39m, in \u001b[36mCLIPSeg.forward\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classes:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.medical_templates \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset_info:\n\u001b[32m    330\u001b[39m         \u001b[38;5;66;03m# Use cached medical prompt embedding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_single_class_with_medical_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         \u001b[38;5;66;03m# Fallback to original CLIPSeg behavior\u001b[39;00m\n\u001b[32m    334\u001b[39m         pred = \u001b[38;5;28mself\u001b[39m.clipseg(image, \u001b[38;5;28mcls\u001b[39m)[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (B, 1, H, W)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\CLIPSeg.py:298\u001b[39m, in \u001b[36mCLIPSeg.predict_single_class_with_medical_prompts\u001b[39m\u001b[34m(self, image, class_name)\u001b[39m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    294\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClass \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not in initialized classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    295\u001b[39m     )\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Get (cached) averaged embedding for this class\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m avg_embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_avg_prompt_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Run the segmentation forward pass and convert logits to probabilities\u001b[39;00m\n\u001b[32m    300\u001b[39m pred = \u001b[38;5;28mself\u001b[39m.clipseg(image, conditional=avg_embedding)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Matteo\\Desktop\\xai\\src\\CLIPSeg.py:269\u001b[39m, in \u001b[36mCLIPSeg._get_avg_prompt_embedding\u001b[39m\u001b[34m(self, class_name, device)\u001b[39m\n\u001b[32m    267\u001b[39m cached = \u001b[38;5;28mself\u001b[39m._avg_prompt_embed_cpu.get(class_name)\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m prompts = \u001b[38;5;28mself\u001b[39m.generate_medical_prompts(class_name)\n\u001b[32m    272\u001b[39m embeds: List[torch.Tensor] = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Task vector simple composition experiments\n",
    "print(\"🔄 Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for target_domain in DOMAINS:\n",
    "        print(f\"\\n{dataset_name}: {target_domain} adaptation\")\n",
    "\n",
    "        image_transform, seg_transform = get_preprocessing(\n",
    "            dataset_name, target_domain, is_training=False\n",
    "        )\n",
    "\n",
    "        dataset_kwargs = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"base_path\": DATA_PATH,\n",
    "            \"domain\": target_domain,\n",
    "            \"transform\": image_transform,  # Use transform instead of preprocess\n",
    "            \"seg_transform\": seg_transform,  # Pass seg_transform too\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"num_workers\": NUM_WORKERS,\n",
    "            \"slice_2d\": not USE_3D,\n",
    "        }\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\":\n",
    "            extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "        target_dataset = get_dataset(**dataset_kwargs, **extra_kwargs)\n",
    "\n",
    "        # Use dataset-specific composite vector first\n",
    "        composite_task_vector = composite_task_vectors[dataset_name]\n",
    "\n",
    "        # Build a new encoder by applying the task vector to the baseline checkpoint\n",
    "        baseline_ckpt = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{target_domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        with torch.serialization.safe_globals(safe_globals=safe_globals):\n",
    "            new_encoder = composite_task_vector.apply_to(str(baseline_ckpt), scaling_coef=alpha)\n",
    "\n",
    "        target_model = target_dataset.get_model(encoder_type=encoder_type)\n",
    "        # Replace encoder with the adapted one\n",
    "        target_model.encoder = new_encoder\n",
    "\n",
    "        metrics = target_model.evaluate()\n",
    "        update_metrics(f\"{dataset_name}_composite_at_{target_domain}\", metrics)\n",
    "        train_d = metrics.get(\"train\", {}).get(\"dice\", 0)\n",
    "        val_d = metrics.get(\"val\", {}).get(\"dice\")\n",
    "        if val_d is not None:\n",
    "            print(\n",
    "                f\"   ✅ {dataset_name} at {target_domain}: Train Dice={train_d:.3f} | Val Dice={val_d:.3f}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   ✅ {dataset_name} at {target_domain}: Train Dice={train_d:.3f}\")\n",
    "\n",
    "        # Now try domain-composite vector\n",
    "        composite_task_vector = composite_task_vectors[target_domain]\n",
    "\n",
    "        baseline_ckpt = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{target_domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        with torch.serialization.safe_globals(safe_globals=safe_globals):\n",
    "            new_encoder = composite_task_vector.apply_to(str(baseline_ckpt), scaling_coef=alpha)\n",
    "\n",
    "        target_model = target_dataset.get_model(encoder_type=encoder_type)\n",
    "        target_model.encoder = new_encoder\n",
    "\n",
    "        metrics = target_model.evaluate()\n",
    "        update_metrics(f\"{target_domain}_composite_at_{dataset_name}\", metrics)\n",
    "        train_d = metrics.get(\"train\", {}).get(\"dice\", 0)\n",
    "        val_d = metrics.get(\"val\", {}).get(\"dice\")\n",
    "        if val_d is not None:\n",
    "            print(\n",
    "                f\"   ✅ {target_domain} at {dataset_name}: Train Dice={train_d:.3f} | Val Dice={val_d:.3f}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   ✅ {target_domain} at {dataset_name}: Train Dice={train_d:.3f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS_CT\":  task_vectors[\"MMWHS_MR\"] \\\n",
    "               + task_vectors[\"CHAOS_CT\"] \\\n",
    "               - task_vectors[\"CHAOS_MR\"],\n",
    "\n",
    "    \"MMWHS_MR\":  task_vectors[\"MMWHS_CT\"] \\\n",
    "               + task_vectors[\"CHAOS_MR\"] \\\n",
    "               - task_vectors[\"CHAOS_CT\"],\n",
    "\n",
    "    \"CHAOS_CT\":  task_vectors[\"CHAOS_MR\"] \\\n",
    "               + task_vectors[\"MMWHS_CT\"] \\\n",
    "               - task_vectors[\"MMWHS_MR\"],\n",
    "\n",
    "    \"CHAOS_MR\":  task_vectors[\"CHAOS_CT\"] \\\n",
    "               + task_vectors[\"MMWHS_MR\"] \\\n",
    "               - task_vectors[\"MMWHS_CT\"],\n",
    "}\n",
    "alpha = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0784a95d",
   "metadata": {},
   "source": [
    "## Part 2: Improve robustness post-hoc without data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 Task Vector Cross-Domain Adaptation Experiments\n",
    "print(\"🔄 Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for target_domain in DOMAINS:\n",
    "        print(f\"\\n{dataset_name}: {target_domain} adaptation\")\n",
    "\n",
    "        image_transform, seg_transform = get_preprocessing(\n",
    "            dataset_name, target_domain, is_training=False\n",
    "        )\n",
    "\n",
    "        dataset_kwargs = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"base_path\": DATA_PATH,\n",
    "            \"domain\": target_domain,\n",
    "            \"transform\": image_transform,  # Use transform instead of preprocess\n",
    "            \"seg_transform\": seg_transform,  # Pass seg_transform too\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"num_workers\": NUM_WORKERS,\n",
    "            \"slice_2d\": not USE_3D,\n",
    "        }\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\":\n",
    "            extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "        # try:\n",
    "        target_dataset = get_dataset(**dataset_kwargs, **extra_kwargs)\n",
    "\n",
    "        composite_key = f\"{dataset_name}_{target_domain}\"\n",
    "        if composite_key in composite_task_vectors:\n",
    "            composite_task_vector = composite_task_vectors[composite_key]\n",
    "\n",
    "            # Apply vector to the correct baseline encoder checkpoint\n",
    "            baseline_ckpt = (\n",
    "                CHECKPOINT_PATH\n",
    "                / f\"{dataset_name}_{target_domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "            )\n",
    "            with torch.serialization.safe_globals(safe_globals=safe_globals):\n",
    "                new_encoder = composite_task_vector.apply_to(str(baseline_ckpt), scaling_coef=alpha)\n",
    "\n",
    "            target_model = target_dataset.get_model(encoder_type=encoder_type)\n",
    "            target_model.encoder = new_encoder\n",
    "\n",
    "            metrics = target_model.evaluate()\n",
    "            update_metrics(f\"{composite_key}_adaptation\", metrics)\n",
    "            train_d = metrics.get(\"train\", {}).get(\"dice\", 0)\n",
    "            val_d = metrics.get(\"val\", {}).get(\"dice\")\n",
    "            if val_d is not None:\n",
    "                print(\n",
    "                    f\"   ✅ {composite_key}: Train Dice={train_d:.3f} | Val Dice={val_d:.3f}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"   ✅ {composite_key}: Train Dice={train_d:.3f}\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ No composite task vector found for {composite_key}\")\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"   ❌ {dataset_name} {target_domain}: {str(e)[:100]}...\")\n",
    "        #     import traceback\n",
    "        #     traceback.print_exc()\n",
    "        #     # continue\n",
    "        #     break\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display all metrics\n",
    "metrics_file = OUTPUTS_PATH / \"metrics.json\"\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        all_metrics = json.load(f)\n",
    "\n",
    "    print(\"\\n📊 COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    def fmt_pair(m):\n",
    "        if not isinstance(m, dict):\n",
    "            return \"Dice=N/A\"\n",
    "        t = m.get(\"train\", {})\n",
    "        v = m.get(\"val\", {})\n",
    "        t_d = t.get(\"dice\")\n",
    "        v_d = v.get(\"dice\")\n",
    "        if t_d is not None and v_d is not None:\n",
    "            return f\"Train Dice={t_d:.3f}, Val Dice={v_d:.3f}\"\n",
    "        if t_d is not None:\n",
    "            return f\"Train Dice={t_d:.3f}\"\n",
    "        if v_d is not None:\n",
    "            return f\"Val Dice={v_d:.3f}\"\n",
    "        return \"Dice=N/A\"\n",
    "\n",
    "    # Baseline performance\n",
    "    print(\"\\nBaseline Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"baseline\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # After Head-training performance\n",
    "    print(\"\\n🏋️‍♂️ After Head-Training Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"head\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # Finetuned performance\n",
    "    print(\"\\n🏆 Finetuned Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"finetuned\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # Composite task vector results\n",
    "    print(\"\\n🧩 Composite Task Vector Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"composite_at\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "\n",
    "    # Dataless adaptation results\n",
    "    print(\"\\n🔄 Dataless Adaptation Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"adaptation\" in key:\n",
    "            print(f\"   {key}: {fmt_pair(metrics)}\")\n",
    "else:\n",
    "    print(\"No metrics file found. Run the experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "\n",
    "    # Copy checkpoints.zip to Google Drive\n",
    "    !zip -r /content/checkpoints.zip /content/xai/checkpoints\n",
    "    shutil.copy(\n",
    "        \"/content/checkpoints.zip\", \"/content/drive/MyDrive/xai/checkpoints.zip\"\n",
    "    )\n",
    "\n",
    "    # Copy metrics.json to Google Drive\n",
    "    shutil.copy(\n",
    "        \"/content/xai/outputs/metrics.json\", \"/content/drive/MyDrive/xai/metrics.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    !zip -r /kaggle/working/checkpoints.zip /kaggle/working/xai/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f3841",
   "metadata": {},
   "source": [
    "# Statistiche dei 4 dataset (CHAOS/MMWHS × CT/MR)\n",
    "Questo blocco calcola e visualizza statistiche per ciascuna combinazione dataset/dominio:\n",
    "- Dimensioni degli split (train/val/test)\n",
    "- Forma media di immagini e maschere\n",
    "- Statistiche di intensità (min/max/media/dev.std) su un sottoinsieme del train\n",
    "- Distribuzione delle classi (bar chart) sul sottoinsieme del train\n",
    "\n",
    "Nota: per rapidità, le statistiche vengono calcolate su un sottoinsieme dei primi N campioni del train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "SUBSET_N = 1e16 - 1  # numero massimo di campioni del train da usare per le statistiche\n",
    "PRINT_EVERY = 8\n",
    "\n",
    "# helper: estrai numpy dai MetaTensor o torch.Tensor\n",
    "\n",
    "\n",
    "def to_numpy(x):\n",
    "    if hasattr(x, \"detach\"):\n",
    "        x = x.detach()\n",
    "    if hasattr(x, \"cpu\"):\n",
    "        x = x.cpu()\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "def class_histogram(labels_np):\n",
    "    # considera solo valori >=0\n",
    "    flat = labels_np.astype(np.int64).ravel()\n",
    "    flat = flat[flat >= 0]\n",
    "    counts = Counter(flat.tolist())\n",
    "    return counts\n",
    "\n",
    "\n",
    "def summarize_split(loader, max_items=SUBSET_N):\n",
    "    n = 0\n",
    "    shapes_img, shapes_seg = [], []\n",
    "    stats = {\n",
    "        \"img_min\": [],\n",
    "        \"img_max\": [],\n",
    "        \"img_mean\": [],\n",
    "        \"img_std\": [],\n",
    "        \"class_counts\": Counter(),\n",
    "    }\n",
    "    if loader is None:\n",
    "        return {\n",
    "            \"n_seen\": 0,\n",
    "            \"img_shape_examples\": [],\n",
    "            \"seg_shape_examples\": [],\n",
    "            \"img_min\": None,\n",
    "            \"img_max\": None,\n",
    "            \"img_mean\": None,\n",
    "            \"img_std\": None,\n",
    "            \"class_hist\": {},\n",
    "        }\n",
    "    for batch in loader:\n",
    "        img = batch.get(\"image\")\n",
    "        seg = batch.get(\"label\")\n",
    "        if img is None:\n",
    "            continue\n",
    "        # img/seg possono essere MetaTensor con shape (B, C, H, W) o (B, C, H, W, D)\n",
    "        img_np = to_numpy(img)\n",
    "        stats[\"img_min\"].append(float(img_np.min()))\n",
    "        stats[\"img_max\"].append(float(img_np.max()))\n",
    "        stats[\"img_mean\"].append(float(img_np.mean()))\n",
    "        stats[\"img_std\"].append(float(img_np.std()))\n",
    "        shapes_img.append(tuple(img_np.shape))\n",
    "        if seg is not None:\n",
    "            seg_np = to_numpy(seg)\n",
    "            shapes_seg.append(tuple(seg_np.shape))\n",
    "            stats[\"class_counts\"].update(class_histogram(seg_np))\n",
    "        n += img_np.shape[0]\n",
    "        if n >= max_items:\n",
    "            break\n",
    "    # aggrega\n",
    "    agg = {\n",
    "        \"n_seen\": n,\n",
    "        \"img_shape_examples\": shapes_img[: min(3, len(shapes_img))],\n",
    "        \"seg_shape_examples\": shapes_seg[: min(3, len(shapes_seg))],\n",
    "        \"img_min\": float(np.mean(stats[\"img_min\"])) if stats[\"img_min\"] else None,\n",
    "        \"img_max\": float(np.mean(stats[\"img_max\"])) if stats[\"img_max\"] else None,\n",
    "        \"img_mean\": float(np.mean(stats[\"img_mean\"])) if stats[\"img_mean\"] else None,\n",
    "        \"img_std\": float(np.mean(stats[\"img_std\"])) if stats[\"img_std\"] else None,\n",
    "        \"class_hist\": dict(stats[\"class_counts\"]),\n",
    "    }\n",
    "    return agg\n",
    "\n",
    "\n",
    "def plot_histogram(hist_dict, title, classnames=None):\n",
    "    if not hist_dict:\n",
    "        print(f\"   Nessuna maschera/nessuna classe trovata per {title}\")\n",
    "        return\n",
    "    keys = sorted(hist_dict.keys())\n",
    "    vals = [hist_dict[k] for k in keys]\n",
    "    labels = [\n",
    "        classnames[k] if classnames and k < len(classnames) else str(k) for k in keys\n",
    "    ]\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.bar(range(len(keys)), vals)\n",
    "    plt.xticks(range(len(keys)), labels, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for domain in DOMAINS:\n",
    "        print(f\"\\n== {dataset_name} / {domain} ==\")\n",
    "        image_transform, seg_transform = get_preprocessing(\n",
    "            dataset_name, domain, is_training=False\n",
    "        )\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\" and domain == \"MR\":\n",
    "            # opzionale: limita a fegato\n",
    "            extra_kwargs[\"liver_only\"] = False\n",
    "\n",
    "        ds = get_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            base_path=DATA_PATH,\n",
    "            domain=domain,\n",
    "            transform=image_transform,\n",
    "            seg_transform=seg_transform,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            slice_2d=not USE_3D,\n",
    "            **extra_kwargs,\n",
    "        )\n",
    "\n",
    "        # dimensioni split\n",
    "        n_train = len(ds.train_dataset) if ds.train_dataset is not None else 0\n",
    "        n_val = len(ds.val_dataset) if ds.val_dataset is not None else 0\n",
    "        n_test = len(ds.test_dataset) if ds.test_dataset is not None else 0\n",
    "        print(f\"Split -> train: {n_train}, val: {n_val}, test: {n_test}\")\n",
    "        print(\n",
    "            f\"Num classi: {getattr(ds, 'num_classes', 'N/A')} | Classnames: {getattr(ds, 'classnames', None)}\"\n",
    "        )\n",
    "\n",
    "        # statistiche su subset del train\n",
    "        train_stats = summarize_split(ds.train_loader, SUBSET_N)\n",
    "        imin = train_stats[\"img_min\"]\n",
    "        imax = train_stats[\"img_max\"]\n",
    "        imean = train_stats[\"img_mean\"]\n",
    "        istd = train_stats[\"img_std\"]\n",
    "        fmt = lambda v: (f\"{v:.4f}\" if isinstance(v, (int, float)) else \"N/A\")\n",
    "        print(f\"   Visti nel subset: {train_stats['n_seen']}\")\n",
    "        print(f\"   Esempi img shape: {train_stats['img_shape_examples']}\")\n",
    "        print(f\"   Esempi seg shape: {train_stats['seg_shape_examples']}\")\n",
    "        print(\n",
    "            f\"   Intensità ~ min:{fmt(imin)} max:{fmt(imax)} \"\n",
    "            f\"mean:{fmt(imean)} std:{fmt(istd)}\"\n",
    "        )\n",
    "\n",
    "        all_stats[f\"{dataset_name}_{domain}\"] = {\n",
    "            \"splits\": {\"train\": n_train, \"val\": n_val, \"test\": n_test},\n",
    "            \"subset\": train_stats,\n",
    "            \"classnames\": getattr(ds, \"classnames\", None),\n",
    "        }\n",
    "\n",
    "        # bar chart distribuzione classi\n",
    "        plot_histogram(\n",
    "            train_stats[\"class_hist\"],\n",
    "            title=f\"Distribuzione classi (subset) - {dataset_name} {domain}\",\n",
    "            classnames=ds.classnames if hasattr(ds, \"classnames\") else None,\n",
    "        )\n",
    "\n",
    "# salva riepilogo su file\n",
    "try:\n",
    "    out_file = OUTPUTS_PATH / \"dataset_stats.json\"\n",
    "    with open(out_file, \"w\") as f:\n",
    "        json.dump(all_stats, f, indent=2)\n",
    "    print(f\"\\nSalvato riepilogo in: {out_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore salvataggio stats: {e}\")\n",
    "\n",
    "print(\"\\nRiepilogo sintetico:\")\n",
    "for k, v in all_stats.items():\n",
    "    s = v[\"splits\"]\n",
    "    print(\n",
    "        f\" - {k}: train={s['train']}, val={s['val']}, test={s['test']} | visti(subset)={v['subset']['n_seen']}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
