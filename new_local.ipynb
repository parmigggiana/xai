{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0bb3b4",
   "metadata": {},
   "source": [
    "In Kaggle, add the following to the dependencies:\n",
    "```\n",
    "pip install torch\n",
    "pip install torchvision\n",
    "pip install numpy\n",
    "pip install pydicom\n",
    "pip install PILlow\n",
    "pip install matplotlib\n",
    "```\n",
    "Enable file persistence and internet access.\n",
    "Remember that you can run the whole notebook and close the runtime without wasting resources by going to File > Save Version > Save & Run All (Double check that GPU is selected in the advanced settings).\n",
    "Later, by going to 'File' > 'Version history' you can view the full logs and download the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c8f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle\n",
    "import os\n",
    "\n",
    "IN_KAGGLE = False\n",
    "if os.environ.get(\"KAGGLE_URL_BASE\", \"\"):\n",
    "    IN_KAGGLE = True\n",
    "    !git clone https://github.com/parmigggiana/xai /kaggle/working/xai\n",
    "    %cd xai\n",
    "    !git fetch\n",
    "    !git reset --hard origin/main\n",
    "    %pip install 'napari[pyqt6,optional]==0.6.2a1' 'monai[einops,nibabel]>=1.1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c6af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "IN_COLAB = False\n",
    "if not IN_KAGGLE:\n",
    "    try:\n",
    "        import google.colab\n",
    "        from google.colab import drive\n",
    "        IN_COLAB = True\n",
    "        import os\n",
    "        drive.mount('/content/drive')\n",
    "        os.makedirs('/content/drive/MyDrive/xai', exist_ok=True)\n",
    "        !git clone https://github.com/parmigggiana/xai /content/xai\n",
    "        %cd /content/xai\n",
    "        !git fetch\n",
    "        !git reset --hard origin/main\n",
    "        %pip install -r requirements.txt\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_vector import TaskVector\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"CHAOS\", \"MMWHS\"]\n",
    "domains = [\"MR\", \"CT\"]\n",
    "data_path = \"data/\"\n",
    "checkpoint_path = \"checkpoints/\"\n",
    "outputs_path = \"outputs/\"\n",
    "use_3d = True\n",
    "training_epochs = {\n",
    "    (\"CHAOS\", \"MR\"): 30,\n",
    "    (\"CHAOS\", \"CT\"): 10,\n",
    "    (\"MMWHS\", \"MR\"): 30,\n",
    "    (\"MMWHS\", \"CT\"): 20,\n",
    "}\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(checkpoint_path)\n",
    "outputs_path = Path(outputs_path)\n",
    "data_path = Path(data_path)\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "outputs_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def update_metrics(name, new_metrics):\n",
    "    metrics_file = outputs_path / \"metrics.json\"\n",
    "\n",
    "    if not metrics_file.exists():\n",
    "        metrics = {}\n",
    "    else:\n",
    "        with open(metrics_file, \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "    metrics[name] = new_metrics\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "\n",
    "def get_preprocessing(domain):\n",
    "    if use_3d:\n",
    "\n",
    "        def preprocess(x):\n",
    "            # Only scale down to (96, 128, 128) if larger\n",
    "            target_shape = (96, 128, 128)\n",
    "            img = x[\"image\"].float()\n",
    "            lbl = x[\"label\"].float() if x[\"label\"] is not None else None\n",
    "\n",
    "            if img.shape[-3:] != target_shape:\n",
    "                img = torch.nn.functional.interpolate(\n",
    "                    img.unsqueeze(0),\n",
    "                    size=target_shape,\n",
    "                    mode=\"trilinear\",\n",
    "                    align_corners=False,\n",
    "                ).squeeze(0)\n",
    "                if lbl is not None:\n",
    "                    lbl = (\n",
    "                        torch.nn.functional.interpolate(\n",
    "                            lbl.unsqueeze(0), size=target_shape, mode=\"nearest\"\n",
    "                        )\n",
    "                        .squeeze(0)\n",
    "                        .long()\n",
    "                    )\n",
    "            else:\n",
    "                if lbl is not None:\n",
    "                    lbl = lbl.long()\n",
    "\n",
    "            return {\"image\": img, \"label\": lbl}\n",
    "\n",
    "    else:\n",
    "        preprocess = torch.nn.Identity\n",
    "\n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned model for CHAOS in MR domain with 3d images already exists at checkpoints/CHAOS_MR_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for CHAOS in CT domain with 3d images already exists at checkpoints/CHAOS_CT_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for MMWHS in MR domain with 3d images already exists at checkpoints/MMWHS_MR_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for MMWHS in CT domain with 3d images already exists at checkpoints/MMWHS_CT_3d_finetuned.pth. Skipping finetuning.\n"
     ]
    }
   ],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in training_epochs.items():\n",
    "    download_and_extract_dataset(dataset_name, data_path)\n",
    "    preprocess = get_preprocessing(domain)\n",
    "\n",
    "    filename = f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned.pth\"\n",
    "    filename = checkpoint_path / filename\n",
    "    # Check if the finetuned checkpoint already exists\n",
    "    if filename.exists():\n",
    "        print(\n",
    "            f\"Finetuned model for {dataset_name} in {domain} domain with {'3d' if use_3d else '2d'} images already exists at {filename}. Skipping finetuning.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if use_3d else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        preprocess=preprocess,\n",
    "        base_path=data_path,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        slice_2d=not use_3d,\n",
    "    )\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=\"swin_unetr\",\n",
    "    )\n",
    "\n",
    "    # Save the baseline model's state_dict before finetuning\n",
    "    baseline_filename = (\n",
    "        checkpoint_path\n",
    "        / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline.pth\"\n",
    "    )\n",
    "    torch.save(model.encoder, baseline_filename)\n",
    "    print(\n",
    "        f\"Processing {dataset_name} in {domain} domain with {'3d' if use_3d else '2d'} images\"\n",
    "    )\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline\",\n",
    "        model_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the segmentation head\n",
    "    # For the proposal of the paper this part should not be done like this!\n",
    "    # This requires data - the original task arithmetic paper builds classification heads using no data, only templates\n",
    "    # if we have time this point should be addressed, otherwise at least mention that the technical problem\n",
    "    # requires more developement time and the proof of concept should still be somewhat solid.\n",
    "    model.freeze_body()\n",
    "    model.finetune(epochs=epochs, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    # Save the head\n",
    "    torch.save(model.head, checkpoint_path / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_head.pth\")\n",
    "\n",
    "    metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_head\",\n",
    "        metrics,\n",
    "    )\n",
    "\n",
    "    # Finetune the encoder-decoder\n",
    "    model.unfreeze()\n",
    "    model.freeze_head()\n",
    "    history = model.finetune(\n",
    "        epochs=epochs,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "    # Save the finetuned model's state_dict\n",
    "\n",
    "    torch.save(model.encoder, filename)\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned\",\n",
    "        model_metrics,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090233",
   "metadata": {},
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f1df630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector for CHAOS dataset in MR domain with 3d images\n",
      "Building task vector for CHAOS dataset in CT domain with 3d images\n",
      "Building task vector for MMWHS dataset in MR domain with 3d images\n",
      "Building task vector for MMWHS dataset in CT domain with 3d images\n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from monai.networks.nets.swin_unetr import BasicLayer\n",
    "from monai.networks.nets.swin_unetr import SwinTransformerBlock\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from monai.networks.nets.swin_unetr import WindowAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules.linear import Identity\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from torch.nn.modules.activation import GELU\n",
    "from monai.networks.nets.swin_unetr import PatchMerging\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from monai.networks.blocks.unetr_block import UnetrUpBlock\n",
    "from torch.nn.modules.conv import ConvTranspose3d\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "\n",
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in dataset_names:\n",
    "    for domain in domains:\n",
    "        print(\n",
    "            f\"Building task vector for {dataset_name} dataset in {domain} domain with {'3d' if use_3d else '2d'} images\"\n",
    "        )\n",
    "        baseline_checkpoint = (\n",
    "            checkpoint_path\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        finetuned_checkpoint = (\n",
    "            checkpoint_path\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned.pth\"\n",
    "        )\n",
    "        with torch.serialization.safe_globals(\n",
    "            [\n",
    "                SwinUNETR,\n",
    "                SwinTransformer,\n",
    "                PatchEmbed,\n",
    "                Conv3d,\n",
    "                Dropout,\n",
    "                ModuleList,\n",
    "                BasicLayer,\n",
    "                SwinTransformerBlock,\n",
    "                LayerNorm,\n",
    "                WindowAttention,\n",
    "                Linear,\n",
    "                Softmax,\n",
    "                Identity,\n",
    "                MLPBlock,\n",
    "                GELU,\n",
    "                PatchMerging,\n",
    "                UnetrBasicBlock,\n",
    "                UnetResBlock,\n",
    "                Convolution,\n",
    "                LeakyReLU,\n",
    "                InstanceNorm3d,\n",
    "                UnetrUpBlock,\n",
    "                ConvTranspose3d,\n",
    "                UnetOutBlock,\n",
    "            ]\n",
    "        ):\n",
    "            task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "            # Remove keys associated with the .out layer from the task vector\n",
    "            out_layer_keys = [k for k in task_vector.keys() if \"out.\" in k]\n",
    "            for k in out_layer_keys:\n",
    "                del task_vector[k]\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb6d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS_CT\": task_vectors[\"MMWHS_MR\"] + task_vectors[\"CHAOS_CT\"] - task_vectors[\"CHAOS_MR\"],\n",
    "    \"MMWHS_MR\": task_vectors[\"MMWHS_CT\"] + task_vectors[\"CHAOS_MR\"] - task_vectors[\"CHAOS_CT\"],\n",
    "    \"CHAOS_CT\": task_vectors[\"CHAOS_MR\"] + task_vectors[\"MMWHS_CT\"] - task_vectors[\"MMWHS_MR\"],\n",
    "    \"CHAOS_MR\": task_vectors[\"CHAOS_CT\"] + task_vectors[\"MMWHS_MR\"] - task_vectors[\"MMWHS_CT\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Task Vector Cross-Domain Adaptation Experiments\n",
      "================================================================================\n",
      "\n",
      "CHAOS: MR adaptation\n",
      "2025-07-26 22:05:29,222 - INFO - Expected md5 is None, skip md5 check for file data/ssl_pretrained_weights.pth.\n",
      "2025-07-26 22:05:29,223 - INFO - File exists: data/ssl_pretrained_weights.pth, skipped downloading.\n",
      "Total updated layers 159 / 159\n",
      "Pretrained Weights Succesfully Loaded !\n",
      "üîç Evaluating train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Task Vector Cross-Domain Evaluation (merged version, with nested loops for all configs)\n",
    "print(\"\\nüîÑ Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    for target_domain in domains:\n",
    "\n",
    "        # Only add extra_kwarg 'liver_only' for CHAOS dataset\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\":\n",
    "            extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "        try:\n",
    "            composite_key = f\"{dataset_name}_{target_domain}\"\n",
    "            if composite_key not in composite_task_vectors:\n",
    "                # If the composite task vector does not exist, skip this iteration\n",
    "                print(\n",
    "                    f\"   ‚ùó Warning: Composite task vector for {composite_key} does not exist. Skipping evaluation.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            print(\n",
    "                f\"\\n{dataset_name}: {target_domain} adaptation\"\n",
    "            )\n",
    "            # Load target domain dataset\n",
    "            dataset_kwargs = dict(\n",
    "                dataset_name=dataset_name,\n",
    "                domain=target_domain,\n",
    "                base_path=data_path,\n",
    "                preprocess=preprocess,\n",
    "                batch_size=1,\n",
    "                num_workers=1,\n",
    "                slice_2d=False,\n",
    "            )\n",
    "            dataset_kwargs.update(extra_kwargs)\n",
    "            target_dataset = get_dataset(**dataset_kwargs)\n",
    "            target_model = target_dataset.get_model(encoder_type=\"swin_unetr\")\n",
    "\n",
    "            # Apply composite task vector for target domain\n",
    "            composite_task_vector = composite_task_vectors[composite_key]\n",
    "            target_model.load_task_vector(composite_task_vector)\n",
    "\n",
    "            # Overwrite the model's head with the saved one from the same task\n",
    "            head_filename = checkpoint_path / f\"{dataset_name}_{target_domain}_{'3d' if use_3d else '2d'}_head.pth\"\n",
    "            if head_filename.exists():\n",
    "                with torch.serialization.safe_globals([UnetOutBlock, Convolution, Conv3d]):\n",
    "                    target_model.head.load_state_dict(torch.load(head_filename, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\").state_dict())\n",
    "            else:\n",
    "                print(\n",
    "                    f\"   ‚ùó Warning: Head file {head_filename} does not exist. Skipping evaluation.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Evaluate cross-domain performance\n",
    "            metrics = target_model.evaluate()\n",
    "            update_metrics(f\"{composite_key}_adaptation\", metrics)\n",
    "\n",
    "            print(\n",
    "                f\"   ‚úÖ {dataset_name} {target_domain}: Dice={metrics.get('dice', 0):.3f}, Hausdorff={metrics.get('hausdorff', 0):.3f}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"   ‚ùå {dataset_name} {target_domain} error: {e}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d41e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä COMPREHENSIVE RESULTS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üèÅ Baseline Performance:\n",
      "   CHAOS_MR_3d_baseline: Dice=0.000, HD=0.000\n",
      "   CHAOS_CT_3d_baseline: Dice=0.000, HD=0.000\n",
      "   MMWHS_MR_3d_baseline: Dice=0.000, HD=0.000\n",
      "   MMWHS_CT_3d_baseline: Dice=0.000, HD=0.000\n",
      "\n",
      "üèãÔ∏è‚Äç‚ôÇÔ∏è After Head-Training Performance:\n",
      "\n",
      "üèÜ Finetuned Performance:\n",
      "   CHAOS_MR_3d_finetuned: Dice=0.000, HD=0.000\n",
      "   CHAOS_CT_3d_finetuned: Dice=0.000, HD=0.000\n",
      "   MMWHS_MR_3d_finetuned: Dice=0.000, HD=0.000\n",
      "   MMWHS_CT_3d_finetuned: Dice=0.000, HD=0.000\n",
      "\n",
      "üîÑ Cross-Domain Adaptation Results:\n"
     ]
    }
   ],
   "source": [
    "# Load and display all metrics\n",
    "metrics_file = outputs_path / \"metrics.json\"\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        all_metrics = json.load(f)\n",
    "\n",
    "    print(\"\\nüìä COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline performance\n",
    "    print(\"\\nüèÅ Baseline Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"baseline\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # After Head-training performance\n",
    "    print(\"\\nüèãÔ∏è‚Äç‚ôÇÔ∏è After Head-Training Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"head\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # Finetuned performance\n",
    "    print(\"\\nüèÜ Finetuned Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"finetuned\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # Cross-domain adaptation results\n",
    "    print(\"\\nüîÑ Cross-Domain Adaptation Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"adaptation\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "else:\n",
    "    print(\"No metrics file found. Run the experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe0a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "\n",
    "\n",
    "    # Copy checkpoints.zip to Google Drive\n",
    "    !zip -r /content/checkpoints.zip /content/xai/checkpoints\n",
    "    shutil.copy('/content/checkpoints.zip', '/content/drive/MyDrive/xai/checkpoints.zip')\n",
    "\n",
    "    # Copy metrics.json to Google Drive\n",
    "    shutil.copy('/content/xai/outputs/metrics.json', '/content/drive/MyDrive/xai/metrics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    !zip -r /kaggle/working/checkpoints.zip /kaggle/working/xai/checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
