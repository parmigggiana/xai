{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0bb3b4",
   "metadata": {},
   "source": [
    "In Kaggle, add the following to the dependencies:\n",
    "```\n",
    "pip install torch\n",
    "pip install torchvision\n",
    "pip install numpy\n",
    "pip install pydicom\n",
    "pip install PILlow\n",
    "pip install matplotlib\n",
    "```\n",
    "Enable file persistence and internet access.\n",
    "Remember that you can run the whole notebook and close the runtime without wasting resources by going to File > Save Version > Save & Run All (Double check that GPU is selected in the advanced settings).\n",
    "Later, by going to 'File' > 'Version history' you can view the full logs and download the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60c8f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle\n",
    "import os\n",
    "\n",
    "IN_KAGGLE = False\n",
    "if os.environ.get(\"KAGGLE_URL_BASE\", \"\"):\n",
    "    IN_KAGGLE = True\n",
    "    !git clone https://github.com/parmigggiana/xai /kaggle/working/xai\n",
    "    %cd xai\n",
    "    !git fetch\n",
    "    !git reset --hard origin/main\n",
    "    %pip install 'monai[einops,itk]>=1.5.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52c6af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "IN_COLAB = False\n",
    "if not IN_KAGGLE:\n",
    "    try:\n",
    "        import google.colab\n",
    "        from google.colab import drive\n",
    "\n",
    "        IN_COLAB = True\n",
    "        import os\n",
    "\n",
    "        drive.mount(\"/content/drive\")\n",
    "        os.makedirs(\"/content/drive/MyDrive/xai\", exist_ok=True)\n",
    "        !git clone https://github.com/parmigggiana/xai /content/xai\n",
    "        %cd /content/xai\n",
    "        !git fetch\n",
    "        !git reset --hard origin/main\n",
    "        %pip install -r requirements.txt\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_vector import TaskVector\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"CHAOS\", \"MMWHS\"]\n",
    "domains = [\"MR\", \"CT\"]\n",
    "data_path = \"data/\"\n",
    "checkpoint_path = \"checkpoints/\"\n",
    "outputs_path = \"outputs/\"\n",
    "use_3d = True\n",
    "training_epochs = {\n",
    "    (\"CHAOS\", \"MR\"): 30,\n",
    "    (\"CHAOS\", \"CT\"): 10,\n",
    "    (\"MMWHS\", \"MR\"): 30,\n",
    "    (\"MMWHS\", \"CT\"): 20,\n",
    "}\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(checkpoint_path)\n",
    "outputs_path = Path(outputs_path)\n",
    "data_path = Path(data_path)\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "outputs_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai import transforms\n",
    "from monai.data.image_reader import ITKReader\n",
    "from pathlib import Path\n",
    "from src.datasets.volumetricPNGReader import VolumetricPNGReader\n",
    "\n",
    "\n",
    "def update_metrics(name, new_metrics):\n",
    "    metrics_file = outputs_path / \"metrics.json\"\n",
    "\n",
    "    if not metrics_file.exists():\n",
    "        metrics = {}\n",
    "    else:\n",
    "        with open(metrics_file, \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "    metrics[name] = new_metrics\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "\n",
    "def get_preprocessing(domain, is_training=True, dataset_name=None):\n",
    "    \"\"\"\n",
    "    Get comprehensive preprocessing pipeline for volumetric medical data.\n",
    "\n",
    "    Returns separate transforms for images and segmentations to work with ImageDataset.\n",
    "    Handles different file formats based on dataset:\n",
    "    - CHAOS: DICOM images (directories), PNG labels (directories)\n",
    "    - MMWHS: NIfTI images and labels\n",
    "    \"\"\"\n",
    "    if use_3d:\n",
    "\n",
    "        # Image-specific transforms (applied to image files)\n",
    "        image_transforms = [\n",
    "            transforms.EnsureChannelFirst(\n",
    "                channel_dim=\"no_channel\"\n",
    "            ),  # Ensure channel-first format\n",
    "            transforms.Orientation(axcodes=\"RAS\"),  # Standardize spatial orientation\n",
    "            transforms.Spacing(\n",
    "                pixdim=(1.5, 1.5, 2.0), mode=\"bilinear\"\n",
    "            ),  # Consistent voxel spacing\n",
    "        ]\n",
    "\n",
    "        # Domain-specific intensity normalization for images\n",
    "        if domain == \"CT\":\n",
    "            # CT-specific intensity normalization\n",
    "            image_transforms.extend(\n",
    "                [\n",
    "                    transforms.ScaleIntensityRange(\n",
    "                        a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True\n",
    "                    ),  # CT-specific clipping\n",
    "                ]\n",
    "            )\n",
    "        else:  # MR\n",
    "            # MR intensity normalization\n",
    "            image_transforms.extend(\n",
    "                [\n",
    "                    transforms.NormalizeIntensity(nonzero=True, channel_wise=True),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Spatial resizing for images\n",
    "        image_transforms.append(\n",
    "            transforms.Resize(spatial_size=96, size_mode=\"longest\", mode=\"area\")\n",
    "        )\n",
    "\n",
    "        # Training-specific augmentations for images only\n",
    "        if is_training:\n",
    "            # Image-only augmentations (safe for ImageDataset)\n",
    "            augmentation_transforms = [\n",
    "                transforms.RandGaussianNoise(prob=0.2, std=0.05),\n",
    "                transforms.RandAdjustContrast(prob=0.2, gamma=(0.9, 1.1)),\n",
    "            ]\n",
    "            image_transforms.extend(augmentation_transforms)\n",
    "\n",
    "        # Final conversion to tensor for images\n",
    "        image_transforms.extend(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.EnsureType(dtype=torch.float32),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Segmentation-specific transforms (applied to segmentation files)\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: print(x.shape) or x),\n",
    "            transforms.EnsureChannelFirst(\n",
    "                channel_dim=\"no_channel\"\n",
    "            ),  # Ensure channel-first format\n",
    "            transforms.Orientation(axcodes=\"RAS\"),  # Standardize spatial orientation\n",
    "            transforms.Spacing(\n",
    "                pixdim=(1.5, 1.5, 2.0), mode=\"nearest\"\n",
    "            ),  # Use nearest for labels\n",
    "            transforms.Resize(\n",
    "                spatial_size=96, size_mode=\"longest\", mode=\"nearest\"\n",
    "            ),  # Resize labels\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "\n",
    "        # Create separate transform pipelines\n",
    "        image_transform = transforms.Compose(image_transforms)\n",
    "        seg_transform = transforms.Compose(seg_transforms)\n",
    "\n",
    "        return image_transform, seg_transform\n",
    "\n",
    "    else:\n",
    "        # 2D preprocessing (if needed)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned model for CHAOS in MR domain with 3d images already exists at checkpoints/CHAOS_MR_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for CHAOS in CT domain with 3d images already exists at checkpoints/CHAOS_CT_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for MMWHS in MR domain with 3d images already exists at checkpoints/MMWHS_MR_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for MMWHS in CT domain with 3d images already exists at checkpoints/MMWHS_CT_3d_finetuned.pth. Skipping finetuning.\n"
     ]
    }
   ],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in training_epochs.items():\n",
    "    download_and_extract_dataset(dataset_name, data_path)\n",
    "\n",
    "    # Get separate transforms for images and segmentations\n",
    "    image_transform, seg_transform = get_preprocessing(\n",
    "        domain, is_training=True, dataset_name=dataset_name\n",
    "    )\n",
    "\n",
    "    filename = f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned.pth\"\n",
    "    filename = checkpoint_path / filename\n",
    "    # Check if the finetuned checkpoint already exists\n",
    "    if filename.exists():\n",
    "        print(\n",
    "            f\"Finetuned model for {dataset_name} in {domain} domain with {'3d' if use_3d else '2d'} images already exists at {filename}. Skipping finetuning.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if use_3d else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        transform=image_transform,  # Use transform instead of preprocess\n",
    "        seg_transform=seg_transform,  # Pass seg_transform too\n",
    "        base_path=data_path,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        slice_2d=not use_3d,\n",
    "    )\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=\"swin_unetr\",\n",
    "    )\n",
    "\n",
    "    # Save the baseline model's state_dict before finetuning\n",
    "    baseline_filename = (\n",
    "        checkpoint_path\n",
    "        / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline.pth\"\n",
    "    )\n",
    "    torch.save(model.encoder, baseline_filename)\n",
    "    print(\n",
    "        f\"Processing {dataset_name} in {domain} domain with {'3d' if use_3d else '2d'} images\"\n",
    "    )\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline\",\n",
    "        model_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the segmentation head\n",
    "    # For the proposal of the paper this part should not be done like this!\n",
    "    # This requires data - the original task arithmetic paper builds classification heads using no data, only templates\n",
    "    # if we have time this point should be addressed, otherwise at least mention that the technical problem\n",
    "    # requires more developement time and the proof of concept should still be somewhat solid.\n",
    "    model.freeze_body()\n",
    "    model.finetune(\n",
    "        epochs=epochs, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    # Save the head\n",
    "    torch.save(\n",
    "        model.head,\n",
    "        checkpoint_path\n",
    "        / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_head.pth\",\n",
    "    )\n",
    "\n",
    "    metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_head\",\n",
    "        metrics,\n",
    "    )\n",
    "\n",
    "    # Finetune the encoder-decoder\n",
    "    model.unfreeze()\n",
    "    model.freeze_head()\n",
    "    history = model.finetune(\n",
    "        epochs=epochs,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "    # Save the finetuned model's state_dict\n",
    "\n",
    "    torch.save(model.encoder, filename)\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned\",\n",
    "        model_metrics,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090233",
   "metadata": {},
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54f4f17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector for CHAOS dataset in MR domain with 3d images\n",
      "Building task vector for CHAOS dataset in CT domain with 3d images\n",
      "Building task vector for MMWHS dataset in MR domain with 3d images\n",
      "Building task vector for MMWHS dataset in CT domain with 3d images\n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from monai.networks.nets.swin_unetr import BasicLayer\n",
    "from monai.networks.nets.swin_unetr import SwinTransformerBlock\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from monai.networks.nets.swin_unetr import WindowAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules.linear import Identity\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from torch.nn.modules.activation import GELU\n",
    "from monai.networks.nets.swin_unetr import PatchMerging\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from monai.networks.blocks.unetr_block import UnetrUpBlock\n",
    "from torch.nn.modules.conv import ConvTranspose3d\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "\n",
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in dataset_names:\n",
    "    for domain in domains:\n",
    "        print(\n",
    "            f\"Building task vector for {dataset_name} dataset in {domain} domain with {'3d' if use_3d else '2d'} images\"\n",
    "        )\n",
    "        baseline_checkpoint = (\n",
    "            checkpoint_path\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        finetuned_checkpoint = (\n",
    "            checkpoint_path\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned.pth\"\n",
    "        )\n",
    "        with torch.serialization.safe_globals(\n",
    "            [\n",
    "                SwinUNETR,\n",
    "                SwinTransformer,\n",
    "                PatchEmbed,\n",
    "                Conv3d,\n",
    "                Dropout,\n",
    "                ModuleList,\n",
    "                BasicLayer,\n",
    "                SwinTransformerBlock,\n",
    "                LayerNorm,\n",
    "                WindowAttention,\n",
    "                Linear,\n",
    "                Softmax,\n",
    "                Identity,\n",
    "                MLPBlock,\n",
    "                GELU,\n",
    "                PatchMerging,\n",
    "                UnetrBasicBlock,\n",
    "                UnetResBlock,\n",
    "                Convolution,\n",
    "                LeakyReLU,\n",
    "                InstanceNorm3d,\n",
    "                UnetrUpBlock,\n",
    "                ConvTranspose3d,\n",
    "                UnetOutBlock,\n",
    "            ]\n",
    "        ):\n",
    "            task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "            # Remove keys associated with the .out layer from the task vector\n",
    "            out_layer_keys = [k for k in task_vector.keys() if \"out.\" in k]\n",
    "            for k in out_layer_keys:\n",
    "                del task_vector[k]\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebb6d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS_CT\": task_vectors[\"MMWHS_MR\"]\n",
    "    + task_vectors[\"CHAOS_CT\"]\n",
    "    - task_vectors[\"CHAOS_MR\"],\n",
    "    \"MMWHS_MR\": task_vectors[\"MMWHS_CT\"]\n",
    "    + task_vectors[\"CHAOS_MR\"]\n",
    "    - task_vectors[\"CHAOS_CT\"],\n",
    "    \"CHAOS_CT\": task_vectors[\"CHAOS_MR\"]\n",
    "    + task_vectors[\"MMWHS_CT\"]\n",
    "    - task_vectors[\"MMWHS_MR\"],\n",
    "    \"CHAOS_MR\": task_vectors[\"CHAOS_CT\"]\n",
    "    + task_vectors[\"MMWHS_MR\"]\n",
    "    - task_vectors[\"MMWHS_CT\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c57ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Task Vector Cross-Domain Adaptation Experiments\n",
      "================================================================================\n",
      "\n",
      "CHAOS: MR adaptation\n",
      "Loaded ['data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/1/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/10/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/13/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/15/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/19/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/2/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/20/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/21/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/22/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/3/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/31/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/32/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/33/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/34/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/36/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/37/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/38/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/39/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/5/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/8/T2SPIR/DICOM_anon'] and ['data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/1/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/10/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/13/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/15/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/19/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/2/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/20/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/21/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/22/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/3/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/31/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/32/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/33/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/34/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/36/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/37/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/38/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/39/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/5/T2SPIR/Ground', 'data/CHAOS/CHAOS_Train_Sets/Train_Sets/MR/8/T2SPIR/Ground'] for MR train\n",
      "Loaded ['data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/11/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/12/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/14/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/16/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/17/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/18/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/23/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/24/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/25/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/26/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/27/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/28/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/29/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/30/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/35/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/4/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/40/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/6/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/7/T2SPIR/DICOM_anon', 'data/CHAOS/CHAOS_Test_Sets/Test_Sets/MR/9/T2SPIR/DICOM_anon'] and [] for MR test\n",
      "2025-07-29 20:08:21,660 - INFO - Expected md5 is None, skip md5 check for file data/ssl_pretrained_weights.pth.\n",
      "2025-07-29 20:08:21,661 - INFO - File exists: data/ssl_pretrained_weights.pth, skipped downloading.\n",
      "Total updated layers 159 / 159\n",
      "Pretrained Weights Succesfully Loaded !\n",
      "üîç Evaluating train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "applying transform <monai.transforms.compose.Compose object at 0x7f224d7726c0>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/transforms/transform.py:150\u001b[39m, in \u001b[36mapply_transform\u001b[39m\u001b[34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[39m\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    147\u001b[39m             apply_transform(transform, item, map_items_ - \u001b[32m1\u001b[39m, unpack_items, log_stats, lazy, overrides)\n\u001b[32m    148\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[32m    149\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/transforms/transform.py:98\u001b[39m, in \u001b[36m_apply_transform\u001b[39m\u001b[34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(*data, lazy=lazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(*data)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m transform(data, lazy=lazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/transforms/utility/array.py:220\u001b[39m, in \u001b[36mEnsureChannelFirst.__call__\u001b[39m\u001b[34m(self, img, meta_dict)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strict_check:\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    221\u001b[39m warnings.warn(msg)\n",
      "\u001b[31mValueError\u001b[39m: Unknown original_channel_dim in the MetaTensor meta dict or `meta_dict` or `channel_dim`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/transforms/transform.py:150\u001b[39m, in \u001b[36mapply_transform\u001b[39m\u001b[34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[39m\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    147\u001b[39m             apply_transform(transform, item, map_items_ - \u001b[32m1\u001b[39m, unpack_items, log_stats, lazy, overrides)\n\u001b[32m    148\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[32m    149\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/transforms/transform.py:98\u001b[39m, in \u001b[36m_apply_transform\u001b[39m\u001b[34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(*data, lazy=lazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(*data)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/transforms/compose.py:346\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, input_, start, end, threading, lazy)\u001b[39m\n\u001b[32m    345\u001b[39m _lazy = \u001b[38;5;28mself\u001b[39m._lazy \u001b[38;5;28;01mif\u001b[39;00m lazy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lazy\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m result = \u001b[43mexecute_compose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_lazy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreading\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/transforms/compose.py:116\u001b[39m, in \u001b[36mexecute_compose\u001b[39m\u001b[34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[39m\n\u001b[32m    115\u001b[39m         _transform = deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     data = \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m data = apply_pending_transforms(data, \u001b[38;5;28;01mNone\u001b[39;00m, overrides, logger_name=log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/transforms/transform.py:180\u001b[39m, in \u001b[36mapply_transform\u001b[39m\u001b[34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[39m\n\u001b[32m    179\u001b[39m         _log_stats(data=data)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mapplying transform \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransform\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: applying transform <monai.transforms.utility.array.EnsureChannelFirst object at 0x7f21be3f4a10>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m target_model.load_task_vector(composite_task_vector)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# target_model.setup_for_dataset(target_dataset)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m metrics = \u001b[43mtarget_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m update_metrics(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomposite_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_adaptation\u001b[39m\u001b[33m\"\u001b[39m, metrics)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomposite_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Dice=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics.get(\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m{}).get(\u001b[33m'\u001b[39m\u001b[33mdice\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/semantic_segmentation.py:504\u001b[39m, in \u001b[36mMedical3DSegmenter.evaluate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    502\u001b[39m has_labels = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluating \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/y5fslgbwfjvkczl1g3ynkd0pp95j3rwg-python3.12-tqdm-4.67.1/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/datasets/custom_imageDataset.py:147\u001b[39m, in \u001b[36mImageDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    140\u001b[39m         seg, seg_meta_data = apply_transform(\n\u001b[32m    141\u001b[39m             \u001b[38;5;28mself\u001b[39m.seg_transform,\n\u001b[32m    142\u001b[39m             (seg, seg_meta_data),\n\u001b[32m    143\u001b[39m             map_items=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    144\u001b[39m             unpack_items=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    145\u001b[39m         )\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         seg = \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseg_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    150\u001b[39m     label = \u001b[38;5;28mself\u001b[39m.labels[index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/transforms/transform.py:180\u001b[39m, in \u001b[36mapply_transform\u001b[39m\u001b[34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[39m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    179\u001b[39m         _log_stats(data=data)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mapplying transform \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransform\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: applying transform <monai.transforms.compose.Compose object at 0x7f224d7726c0>"
     ]
    }
   ],
   "source": [
    "# üîÑ Task Vector Cross-Domain Adaptation Experiments\n",
    "print(\"üîÑ Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    for target_domain in domains:\n",
    "        print(f\"\\n{dataset_name}: {target_domain} adaptation\")\n",
    "\n",
    "        # Get separate transforms for target domain, passing dataset_name for correct readers\n",
    "        image_transform, seg_transform = get_preprocessing(\n",
    "            target_domain, is_training=False, dataset_name=dataset_name\n",
    "        )\n",
    "        # image_transform = None\n",
    "        # seg_transform = None\n",
    "\n",
    "        dataset_kwargs = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"base_path\": data_path,\n",
    "            \"domain\": target_domain,\n",
    "            \"transform\": image_transform,  # Use transform instead of preprocess\n",
    "            \"seg_transform\": seg_transform,  # Pass seg_transform too\n",
    "            \"batch_size\": 1,\n",
    "        }\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\":\n",
    "            extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "        # try:\n",
    "        target_dataset = get_dataset(**dataset_kwargs, **extra_kwargs)\n",
    "\n",
    "        composite_key = f\"{dataset_name}_{target_domain}\"\n",
    "        if composite_key in composite_task_vectors:\n",
    "            composite_task_vector = composite_task_vectors[composite_key]\n",
    "\n",
    "            target_model = target_dataset.get_model()\n",
    "            target_model.load_task_vector(composite_task_vector)\n",
    "            # target_model.setup_for_dataset(target_dataset)\n",
    "\n",
    "            metrics = target_model.evaluate()\n",
    "            update_metrics(f\"{composite_key}_adaptation\", metrics)\n",
    "            print(\n",
    "                f\"   ‚úÖ {composite_key}: Dice={metrics.get('train', {}).get('dice', 0):.3f}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No composite task vector found for {composite_key}\")\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"   ‚ùå {dataset_name} {target_domain}: {str(e)[:100]}...\")\n",
    "        #     import traceback\n",
    "        #     traceback.print_exc()\n",
    "        #     # continue\n",
    "        #     break\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display all metrics\n",
    "metrics_file = outputs_path / \"metrics.json\"\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        all_metrics = json.load(f)\n",
    "\n",
    "    print(\"\\nüìä COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline performance\n",
    "    print(\"\\nüèÅ Baseline Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"baseline\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # After Head-training performance\n",
    "    print(\"\\nüèãÔ∏è‚Äç‚ôÇÔ∏è After Head-Training Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"head\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # Finetuned performance\n",
    "    print(\"\\nüèÜ Finetuned Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"finetuned\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # Cross-domain adaptation results\n",
    "    print(\"\\nüîÑ Cross-Domain Adaptation Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"adaptation\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "else:\n",
    "    print(\"No metrics file found. Run the experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "\n",
    "    # Copy checkpoints.zip to Google Drive\n",
    "    !zip -r /content/checkpoints.zip /content/xai/checkpoints\n",
    "    shutil.copy(\n",
    "        \"/content/checkpoints.zip\", \"/content/drive/MyDrive/xai/checkpoints.zip\"\n",
    "    )\n",
    "\n",
    "    # Copy metrics.json to Google Drive\n",
    "    shutil.copy(\n",
    "        \"/content/xai/outputs/metrics.json\", \"/content/drive/MyDrive/xai/metrics.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    !zip -r /kaggle/working/checkpoints.zip /kaggle/working/xai/checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
