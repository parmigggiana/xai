{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0bb3b4",
   "metadata": {},
   "source": [
    "In Kaggle, add the following to the dependencies:\n",
    "```\n",
    "pip install torch\n",
    "pip install torchvision\n",
    "pip install numpy\n",
    "pip install pydicom\n",
    "pip install PILlow\n",
    "pip install matplotlib\n",
    "```\n",
    "Enable file persistence and internet access.\n",
    "Remember that you can run the whole notebook and close the runtime without wasting resources by going to File > Save Version > Save & Run All (Double check that GPU is selected in the advanced settings).\n",
    "Later, by going to 'File' > 'Version history' you can view the full logs and download the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c8f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle\n",
    "import os\n",
    "\n",
    "IN_KAGGLE = False\n",
    "if os.environ.get(\"KAGGLE_URL_BASE\", \"\"):\n",
    "    IN_KAGGLE = True\n",
    "    !git clone https://github.com/parmigggiana/xai /kaggle/working/xai\n",
    "    %cd xai\n",
    "    !git fetch\n",
    "    !git reset --hard origin/main\n",
    "    %pip install 'napari[pyqt6,optional]==0.6.2a1' 'monai[einops,nibabel]>=1.1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c6af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "IN_COLAB = False\n",
    "if not IN_KAGGLE:\n",
    "    try:\n",
    "        import google.colab\n",
    "        from google.colab import drive\n",
    "        IN_COLAB = True\n",
    "        import os\n",
    "        drive.mount('/content/drive')\n",
    "        os.makedirs('/content/drive/MyDrive/xai', exist_ok=True)\n",
    "        !git clone https://github.com/parmigggiana/xai /content/xai\n",
    "        %cd /content/xai\n",
    "        !git fetch\n",
    "        !git reset --hard origin/main\n",
    "        %pip install -r requirements.txt\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.task_vector import TaskVector\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"CHAOS\", \"MMWHS\"]\n",
    "domains = [\"MR\", \"CT\"]\n",
    "data_path = \"data/\"\n",
    "checkpoint_path = \"checkpoints/\"\n",
    "outputs_path = \"outputs/\"\n",
    "use_3d = True\n",
    "training_epochs = {\n",
    "    (\"CHAOS\", \"MR\"): 30,\n",
    "    (\"CHAOS\", \"CT\"): 10,\n",
    "    (\"MMWHS\", \"MR\"): 30,\n",
    "    (\"MMWHS\", \"CT\"): 20,\n",
    "}\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(checkpoint_path)\n",
    "outputs_path = Path(outputs_path)\n",
    "data_path = Path(data_path)\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "outputs_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai import transforms\n",
    "\n",
    "def update_metrics(name, new_metrics):\n",
    "    metrics_file = outputs_path / \"metrics.json\"\n",
    "\n",
    "    if not metrics_file.exists():\n",
    "        metrics = {}\n",
    "    else:\n",
    "        with open(metrics_file, \"r\") as f:\n",
    "            metrics = json.load(f)\n",
    "\n",
    "    metrics[name] = new_metrics\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "\n",
    "def get_preprocessing(domain, is_training=True):\n",
    "    \"\"\"\n",
    "    Get comprehensive preprocessing pipeline for volumetric medical data.\n",
    "\n",
    "    Args:\n",
    "        domain: 'MR' or 'CT'\n",
    "        is_training: Whether this is for training (includes augmentations)\n",
    "    \"\"\"\n",
    "    if use_3d:\n",
    "        # Base preprocessing steps (applied to all data)\n",
    "        base_transforms = [\n",
    "            transforms.Orientation(axcodes=\"RAS\"),  # Standardize spatial orientation (D, H, W)\n",
    "            transforms.Spacing(pixdim=(1.5, 1.5, 2.0), mode=\"trilinear\"),  # Consistent voxel spacing\n",
    "        ]\n",
    "\n",
    "        # Domain-specific intensity normalization\n",
    "        if domain.upper() in ['CT']:\n",
    "            # CT: Clip HU values and normalize\n",
    "            base_transforms.extend([\n",
    "                transforms.ScaleIntensityRange(\n",
    "                    a_min=-200, a_max=300, b_min=0.0, b_max=1.0, clip=True\n",
    "                ),\n",
    "            ])\n",
    "        else:  # MR/MRI\n",
    "            # MR: Z-score normalization (handles varying intensity ranges)\n",
    "            base_transforms.extend([\n",
    "                transforms.NormalizeIntensity(nonzero=True, channel_wise=True),\n",
    "            ])\n",
    "\n",
    "        # Spatial resizing\n",
    "        base_transforms.append(\n",
    "            transforms.Resize(spatial_size=(96, 96, 96), mode=\"trilinear\")\n",
    "        )\n",
    "\n",
    "        # Training augmentations\n",
    "        if is_training:\n",
    "            augmentation_transforms = [\n",
    "                transforms.RandRotate90(prob=0.3, spatial_axes=(0, 1)),\n",
    "                transforms.RandFlip(prob=0.3, spatial_axis=0),\n",
    "                transforms.RandAffine(\n",
    "                    prob=0.3,\n",
    "                    rotate_range=0.1,\n",
    "                    translate_range=5,\n",
    "                    scale_range=0.1,\n",
    "                    mode=\"trilinear\"\n",
    "                ),\n",
    "                transforms.RandGaussianNoise(prob=0.2, std=0.05),\n",
    "                transforms.RandAdjustContrast(prob=0.2, gamma=(0.9, 1.1)),\n",
    "            ]\n",
    "            base_transforms.extend(augmentation_transforms)\n",
    "\n",
    "        # Final conversion to tensor\n",
    "        base_transforms.extend([\n",
    "            # transforms.EnsureChannelFirst(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ])\n",
    "\n",
    "        return transforms.Compose(base_transforms)\n",
    "    else:\n",
    "        # 2D preprocessing (if needed)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned model for CHAOS in MR domain with 3d images already exists at checkpoints/CHAOS_MR_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for CHAOS in CT domain with 3d images already exists at checkpoints/CHAOS_CT_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for MMWHS in MR domain with 3d images already exists at checkpoints/MMWHS_MR_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for MMWHS in CT domain with 3d images already exists at checkpoints/MMWHS_CT_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for MMWHS in MR domain with 3d images already exists at checkpoints/MMWHS_MR_3d_finetuned.pth. Skipping finetuning.\n",
      "Finetuned model for MMWHS in CT domain with 3d images already exists at checkpoints/MMWHS_CT_3d_finetuned.pth. Skipping finetuning.\n"
     ]
    }
   ],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in training_epochs.items():\n",
    "    download_and_extract_dataset(dataset_name, data_path)\n",
    "    preprocess = get_preprocessing(domain, is_training=True)\n",
    "\n",
    "    filename = f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned.pth\"\n",
    "    filename = checkpoint_path / filename\n",
    "    # Check if the finetuned checkpoint already exists\n",
    "    if filename.exists():\n",
    "        print(\n",
    "            f\"Finetuned model for {dataset_name} in {domain} domain with {'3d' if use_3d else '2d'} images already exists at {filename}. Skipping finetuning.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if use_3d else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        preprocess=preprocess,\n",
    "        base_path=data_path,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        slice_2d=not use_3d,\n",
    "    )\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=\"swin_unetr\",\n",
    "    )\n",
    "\n",
    "    # Save the baseline model's state_dict before finetuning\n",
    "    baseline_filename = (\n",
    "        checkpoint_path\n",
    "        / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline.pth\"\n",
    "    )\n",
    "    torch.save(model.encoder, baseline_filename)\n",
    "    print(\n",
    "        f\"Processing {dataset_name} in {domain} domain with {'3d' if use_3d else '2d'} images\"\n",
    "    )\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline\",\n",
    "        model_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the segmentation head\n",
    "    # For the proposal of the paper this part should not be done like this!\n",
    "    # This requires data - the original task arithmetic paper builds classification heads using no data, only templates\n",
    "    # if we have time this point should be addressed, otherwise at least mention that the technical problem\n",
    "    # requires more developement time and the proof of concept should still be somewhat solid.\n",
    "    model.freeze_body()\n",
    "    model.finetune(epochs=epochs, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    # Save the head\n",
    "    torch.save(model.head, checkpoint_path / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_head.pth\")\n",
    "\n",
    "    metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_head\",\n",
    "        metrics,\n",
    "    )\n",
    "\n",
    "    # Finetune the encoder-decoder\n",
    "    model.unfreeze()\n",
    "    model.freeze_head()\n",
    "    history = model.finetune(\n",
    "        epochs=epochs,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "    # Save the finetuned model's state_dict\n",
    "\n",
    "    torch.save(model.encoder, filename)\n",
    "    model_metrics = model.evaluate()\n",
    "    update_metrics(\n",
    "        f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned\",\n",
    "        model_metrics,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090233",
   "metadata": {},
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f1df630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector for CHAOS dataset in MR domain with 3d images\n",
      "Building task vector for CHAOS dataset in CT domain with 3d images\n",
      "Building task vector for CHAOS dataset in CT domain with 3d images\n",
      "Building task vector for MMWHS dataset in MR domain with 3d images\n",
      "Building task vector for MMWHS dataset in MR domain with 3d images\n",
      "Building task vector for MMWHS dataset in CT domain with 3d images\n",
      "Building task vector for MMWHS dataset in CT domain with 3d images\n"
     ]
    }
   ],
   "source": [
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from monai.networks.nets.swin_unetr import BasicLayer\n",
    "from monai.networks.nets.swin_unetr import SwinTransformerBlock\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from monai.networks.nets.swin_unetr import WindowAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules.linear import Identity\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from torch.nn.modules.activation import GELU\n",
    "from monai.networks.nets.swin_unetr import PatchMerging\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from monai.networks.blocks.unetr_block import UnetrUpBlock\n",
    "from torch.nn.modules.conv import ConvTranspose3d\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "\n",
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in dataset_names:\n",
    "    for domain in domains:\n",
    "        print(\n",
    "            f\"Building task vector for {dataset_name} dataset in {domain} domain with {'3d' if use_3d else '2d'} images\"\n",
    "        )\n",
    "        baseline_checkpoint = (\n",
    "            checkpoint_path\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        finetuned_checkpoint = (\n",
    "            checkpoint_path\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if use_3d else '2d'}_finetuned.pth\"\n",
    "        )\n",
    "        with torch.serialization.safe_globals(\n",
    "            [\n",
    "                SwinUNETR,\n",
    "                SwinTransformer,\n",
    "                PatchEmbed,\n",
    "                Conv3d,\n",
    "                Dropout,\n",
    "                ModuleList,\n",
    "                BasicLayer,\n",
    "                SwinTransformerBlock,\n",
    "                LayerNorm,\n",
    "                WindowAttention,\n",
    "                Linear,\n",
    "                Softmax,\n",
    "                Identity,\n",
    "                MLPBlock,\n",
    "                GELU,\n",
    "                PatchMerging,\n",
    "                UnetrBasicBlock,\n",
    "                UnetResBlock,\n",
    "                Convolution,\n",
    "                LeakyReLU,\n",
    "                InstanceNorm3d,\n",
    "                UnetrUpBlock,\n",
    "                ConvTranspose3d,\n",
    "                UnetOutBlock,\n",
    "            ]\n",
    "        ):\n",
    "            task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "            # Remove keys associated with the .out layer from the task vector\n",
    "            out_layer_keys = [k for k in task_vector.keys() if \"out.\" in k]\n",
    "            for k in out_layer_keys:\n",
    "                del task_vector[k]\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb6d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS_CT\": task_vectors[\"MMWHS_MR\"] + task_vectors[\"CHAOS_CT\"] - task_vectors[\"CHAOS_MR\"],\n",
    "    \"MMWHS_MR\": task_vectors[\"MMWHS_CT\"] + task_vectors[\"CHAOS_MR\"] - task_vectors[\"CHAOS_CT\"],\n",
    "    \"CHAOS_CT\": task_vectors[\"CHAOS_MR\"] + task_vectors[\"MMWHS_CT\"] - task_vectors[\"MMWHS_MR\"],\n",
    "    \"CHAOS_MR\": task_vectors[\"CHAOS_CT\"] + task_vectors[\"MMWHS_MR\"] - task_vectors[\"MMWHS_CT\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c57ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Task Vector Cross-Domain Adaptation Experiments\n",
      "================================================================================\n",
      "\n",
      "CHAOS: MR adaptation\n",
      "Split 'test' - Found 0 segmentation files for patient 11\n",
      "\n",
      "=== DEBUG: Metadata Analysis for Patient 11 ===\n",
      "Image metadata keys: ['spacing', original_affine, space, affine, spatial_shape, original_channel_dim]\n",
      "Image shape: (256, 256, 26)\n",
      "No segmentation data (test split or no files found)\n",
      "=== END DEBUG ===\n",
      "\n",
      "Split 'test' - Found 0 segmentation files for patient 11\n",
      "\n",
      "=== DEBUG: Metadata Analysis for Patient 11 ===\n",
      "Image metadata keys: ['spacing', original_affine, space, affine, spatial_shape, original_channel_dim]\n",
      "Image shape: (256, 256, 26)\n",
      "No segmentation data (test split or no files found)\n",
      "=== END DEBUG ===\n",
      "\n",
      "2025-07-28 12:55:58,428 - INFO - Expected md5 is None, skip md5 check for file data/ssl_pretrained_weights.pth.\n",
      "2025-07-28 12:55:58,429 - INFO - File exists: data/ssl_pretrained_weights.pth, skipped downloading.\n",
      "2025-07-28 12:55:58,428 - INFO - Expected md5 is None, skip md5 check for file data/ssl_pretrained_weights.pth.\n",
      "2025-07-28 12:55:58,429 - INFO - File exists: data/ssl_pretrained_weights.pth, skipped downloading.\n",
      "Total updated layers 159 / 159\n",
      "Pretrained Weights Succesfully Loaded !\n",
      "🔍 Evaluating train split...\n",
      "Total updated layers 159 / 159\n",
      "Pretrained Weights Succesfully Loaded !\n",
      "🔍 Evaluating train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train' - Found 36 segmentation files for patient 1\n",
      "\n",
      "\n",
      "=== DEBUG: Metadata Analysis for Patient 1 ===\n",
      "Image metadata keys: ['spacing', original_affine, space, affine, spatial_shape, original_channel_dim]\n",
      "\n",
      "=== DEBUG: Metadata Analysis for Patient 1 ===\n",
      "Image metadata keys: ['spacing', original_affine, space, affine, spatial_shape, original_channel_dim]\n",
      "Image shape: (256, 256, 36)\n",
      "Segmentation metadata keys: ['format', 'mode', 'width', 'height', spatial_shape, original_channel_dim]\n",
      "Segmentation shape: (256, 256, 36)Image shape: (256, 256, 36)\n",
      "Segmentation metadata keys: ['format', 'mode', 'width', 'height', spatial_shape, original_channel_dim]\n",
      "Segmentation shape: (256, 256, 36)\n",
      "\n",
      "Total seg slices loaded: 36\n",
      "Checking if segmentation metadata varies across slices:Total seg slices loaded: 36\n",
      "Checking if segmentation metadata varies across slices:\n",
      "\n",
      "  Slice 1: original_channel_dim differs from first slice\n",
      "    First: nan  Slice 1: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "\n",
      "    Slice 1: nan\n",
      "  Slice 2: original_channel_dim differs from first slice\n",
      "    First: nan    Slice 1: nan\n",
      "  Slice 2: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 2: nan\n",
      "  Slice 3: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 2: nan\n",
      "  Slice 3: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "\n",
      "    Slice 3: nan\n",
      "  Slice 4: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 3: nan\n",
      "  Slice 4: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 4: nan\n",
      "  Slice 5: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 5: nan    Slice 4: nan\n",
      "  Slice 5: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 5: nan\n",
      "  Slice 6: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 6: nan\n",
      "  Slice 7: original_channel_dim differs from first slice\n",
      "  Slice 6: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 6: nan\n",
      "  Slice 7: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 7: nan\n",
      "\n",
      "    First: nan\n",
      "    Slice 7: nan\n",
      "  Slice 8: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 8: nan\n",
      "  Slice 9: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "  Slice 8: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 8: nan\n",
      "  Slice 9: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 9: nan\n",
      "  Slice 10: original_channel_dim differs from first slice\n",
      "    First: nan    Slice 9: nan\n",
      "  Slice 10: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 10: nan\n",
      "  Slice 11: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 11: nan\n",
      "  Slice 12: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 12: nan\n",
      "\n",
      "    Slice 10: nan\n",
      "  Slice 11: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 11: nan\n",
      "  Slice 12: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 12: nan\n",
      "  Slice 13: original_channel_dim differs from first slice\n",
      "    First: nan  Slice 13: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 13: nan\n",
      "  Slice 14: original_channel_dim differs from first slice\n",
      "    Slice 13: nan\n",
      "  Slice 14: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 14: nan\n",
      "  Slice 15: original_channel_dim differs from first slice\n",
      "\n",
      "    First: nan\n",
      "    Slice 14: nan\n",
      "  Slice 15: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 15: nan\n",
      "  Slice 16: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 16: nan\n",
      "  Slice 17: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 17: nan\n",
      "  Slice 18: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 18: nan\n",
      "  Slice 19: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    First: nan\n",
      "    Slice 15: nan\n",
      "  Slice 16: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 16: nan\n",
      "  Slice 17: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 17: nan\n",
      "  Slice 18: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 18: nan\n",
      "  Slice 19: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 19: nan\n",
      "  Slice 20: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 20: nan\n",
      "  Slice 21: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 21: nan\n",
      "  Slice 22: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 22: nan\n",
      "  Slice 23: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 19: nan\n",
      "  Slice 20: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 20: nan\n",
      "  Slice 21: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 21: nan\n",
      "  Slice 22: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 22: nan\n",
      "  Slice 23: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 23: nan\n",
      "  Slice 24: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 24: nan\n",
      "    Slice 23: nan\n",
      "  Slice 24: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 24: nan\n",
      "  Slice 25: original_channel_dim differs from first slice\n",
      "    First: nan  Slice 25: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 25: nan\n",
      "  Slice 26: original_channel_dim differs from first slice\n",
      "\n",
      "    Slice 25: nan\n",
      "  Slice 26: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 26: nan\n",
      "  Slice 27: original_channel_dim differs from first slice\n",
      "    First: nan    First: nan\n",
      "    Slice 26: nan\n",
      "  Slice 27: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 27: nan\n",
      "\n",
      "    Slice 27: nan\n",
      "  Slice 28: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 28: nan  Slice 28: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 28: nan\n",
      "  Slice 29: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "  Slice 29: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 29: nan\n",
      "  Slice 30: original_channel_dim differs from first slice\n",
      "    Slice 29: nan\n",
      "  Slice 30: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "\n",
      "    First: nan\n",
      "    Slice 30: nan\n",
      "  Slice 31: original_channel_dim differs from first slice    Slice 30: nan\n",
      "  Slice 31: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 31: nan\n",
      "  Slice 32: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 31: nan\n",
      "  Slice 32: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 32: nan\n",
      "  Slice 33: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 33: nan\n",
      "    First: nan\n",
      "    Slice 32: nan\n",
      "  Slice 33: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 33: nan\n",
      "  Slice 34: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 34: nan\n",
      "\n",
      "  Slice 34: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 34: nan\n",
      "  Slice 35: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 35: nan\n",
      "=== END DEBUG ===\n",
      "  Slice 35: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 35: nan\n",
      "=== END DEBUG ===\n",
      "\n",
      "\n",
      "Split 'train' - Found 26 segmentation files for patient 15\n",
      "Split 'train' - Found 26 segmentation files for patient 15\n",
      "\n",
      "=== DEBUG: Metadata Analysis for Patient 15 ===\n",
      "Image metadata keys: ['spacing', original_affine, space, affine, spatial_shape, original_channel_dim]\n",
      "\n",
      "=== DEBUG: Metadata Analysis for Patient 15 ===\n",
      "Image metadata keys: ['spacing', original_affine, space, affine, spatial_shape, original_channel_dim]\n",
      "Image shape: (256, 256, 26)\n",
      "Segmentation metadata keys: ['format', 'mode', 'width', 'height', spatial_shape, original_channel_dim]\n",
      "Image shape: (256, 256, 26)\n",
      "Segmentation metadata keys: ['format', 'mode', 'width', 'height', spatial_shape, original_channel_dim]\n",
      "Segmentation shape: (256, 256, 26)\n",
      "Total seg slices loaded: 26\n",
      "Checking if segmentation metadata varies across slices:Segmentation shape: (256, 256, 26)\n",
      "Total seg slices loaded: 26\n",
      "Checking if segmentation metadata varies across slices:\n",
      "  Slice 1: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 1: nan\n",
      "  Slice 1: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 1: nan\n",
      "  Slice 2: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 2: nan\n",
      "  Slice 3: original_channel_dim differs from first slice\n",
      "  Slice 2: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 2: nan\n",
      "  Slice 3: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 3: nan\n",
      "  Slice 4: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 3: nan\n",
      "  Slice 4: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 4: nan\n",
      "\n",
      "    First: nan\n",
      "    Slice 4: nan\n",
      "  Slice 5: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 5: nan\n",
      "  Slice 6: original_channel_dim differs from first slice\n",
      "    First: nan  Slice 5: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 5: nan\n",
      "  Slice 6: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 6: nan\n",
      "  Slice 7: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 6: nan\n",
      "  Slice 7: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 7: nan\n",
      "  Slice 8: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 8: nan\n",
      "    Slice 7: nan\n",
      "  Slice 8: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 8: nan\n",
      "  Slice 9: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 9: nan\n",
      "  Slice 9: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 9: nan\n",
      "  Slice 10: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 10: nan\n",
      "  Slice 11: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "\n",
      "  Slice 10: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 10: nan\n",
      "  Slice 11: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 11: nan\n",
      "  Slice 12: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 12: nan\n",
      "    Slice 11: nan\n",
      "  Slice 12: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 12: nan\n",
      "  Slice 13: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 13: nan\n",
      "  Slice 14: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 14: nan  Slice 13: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 13: nan\n",
      "  Slice 14: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 14: nan\n",
      "  Slice 15: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 15: nan\n",
      "  Slice 15: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 15: nan\n",
      "  Slice 16: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 16: nan\n",
      "  Slice 17: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 17: nan\n",
      "  Slice 18: original_channel_dim differs from first slice\n",
      "  Slice 16: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 16: nan\n",
      "  Slice 17: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 17: nan\n",
      "  Slice 18: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 18: nan\n",
      "  Slice 19: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 19: nan\n",
      "  Slice 20: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 20: nan\n",
      "  Slice 21: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "\n",
      "    First: nan\n",
      "    Slice 18: nan\n",
      "  Slice 19: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 19: nan\n",
      "  Slice 20: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 20: nan\n",
      "  Slice 21: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 21: nan\n",
      "  Slice 22: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 22: nan\n",
      "  Slice 23: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 23: nan\n",
      "  Slice 24: original_channel_dim differs from first slice    Slice 21: nan\n",
      "  Slice 22: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 22: nan\n",
      "  Slice 23: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 23: nan\n",
      "  Slice 24: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 24: nan\n",
      "  Slice 25: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 25: nan\n",
      "=== END DEBUG ===\n",
      "\n",
      "\n",
      "    First: nan\n",
      "    Slice 24: nan\n",
      "  Slice 25: original_channel_dim differs from first slice\n",
      "    First: nan\n",
      "    Slice 25: nan\n",
      "=== END DEBUG ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train:   0%|          | 0/20 [00:03<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "   ❌ CHAOS MR",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Evaluate cross-domain performance\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m metrics = \u001b[43mtarget_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m update_metrics(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomposite_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_adaptation\u001b[39m\u001b[33m\"\u001b[39m, metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/semantic_segmentation.py:523\u001b[39m, in \u001b[36mMedical3DSegmenter.evaluate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(device.type):\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    525\u001b[39m preds = torch.argmax(outputs, dim=\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/semantic_segmentation.py:186\u001b[39m, in \u001b[36mMedical3DSegmenter.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03mOverride call method to handle both training and inference.\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03mThis allows the model to be used seamlessly in training loops.\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/semantic_segmentation.py:172\u001b[39m, in \u001b[36mMedical3DSegmenter.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    170\u001b[39m x, original_shape = \u001b[38;5;28mself\u001b[39m._pad_input_for_swin_unetr(x)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m result = \u001b[38;5;28mself\u001b[39m._crop_output_to_original_size(result, original_shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/networks/nets/swin_unetr.py:317\u001b[39m, in \u001b[36mSwinUNETR.forward\u001b[39m\u001b[34m(self, x_in)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_tracing():\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_input_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m hidden_states_out = \u001b[38;5;28mself\u001b[39m.swinViT(x_in, \u001b[38;5;28mself\u001b[39m.normalize)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/.venv/lib/python3.12/site-packages/monai/networks/nets/swin_unetr.py:310\u001b[39m, in \u001b[36mSwinUNETR._check_input_size\u001b[39m\u001b[34m(self, spatial_shape)\u001b[39m\n\u001b[32m    309\u001b[39m wrong_dims = (np.where(remainder)[\u001b[32m0\u001b[39m] + \u001b[32m2\u001b[39m).tolist()\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    311\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mspatial dimensions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwrong_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of input image (spatial shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspatial_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m must be divisible by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.patch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m**5.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    313\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: spatial dimensions [2, 3] of input image (spatial shape: torch.Size([1, 127, 96, 96])) must be divisible by 2**5.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     59\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_domain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Dice=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics.get(\u001b[33m'\u001b[39m\u001b[33mdice\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Hausdorff=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics.get(\u001b[33m'\u001b[39m\u001b[33mhausdorff\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     60\u001b[39m     )\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ❌ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_domain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m  \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m:    ❌ CHAOS MR"
     ]
    }
   ],
   "source": [
    "# Task Vector Cross-Domain Evaluation (merged version, with nested loops for all configs)\n",
    "print(\"\\n🔄 Task Vector Cross-Domain Adaptation Experiments\")\n",
    "print(\"=\" * 80)\n",
    "preprocess = get_preprocessing(domain, is_training=False)\n",
    "for dataset_name in dataset_names:\n",
    "    for target_domain in domains:\n",
    "\n",
    "        # Only add extra_kwarg 'liver_only' for CHAOS dataset\n",
    "        extra_kwargs = {}\n",
    "        if dataset_name == \"CHAOS\":\n",
    "            extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "        try:\n",
    "            composite_key = f\"{dataset_name}_{target_domain}\"\n",
    "            if composite_key not in composite_task_vectors:\n",
    "                # If the composite task vector does not exist, skip this iteration\n",
    "                print(\n",
    "                    f\"   ❗ Warning: Composite task vector for {composite_key} does not exist. Skipping evaluation.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            print(\n",
    "                f\"\\n{dataset_name}: {target_domain} adaptation\"\n",
    "            )\n",
    "            # Load target domain dataset\n",
    "            dataset_kwargs = dict(\n",
    "                dataset_name=dataset_name,\n",
    "                domain=target_domain,\n",
    "                base_path=data_path,\n",
    "                preprocess=preprocess,\n",
    "                batch_size=1,\n",
    "                num_workers=1,\n",
    "                slice_2d=False,\n",
    "            )\n",
    "            dataset_kwargs.update(extra_kwargs)\n",
    "            target_dataset = get_dataset(**dataset_kwargs)\n",
    "            target_model = target_dataset.get_model(encoder_type=\"swin_unetr\")\n",
    "\n",
    "            # Apply composite task vector for target domain\n",
    "            composite_task_vector = composite_task_vectors[composite_key]\n",
    "            target_model.load_task_vector(composite_task_vector)\n",
    "\n",
    "            # Overwrite the model's head with the saved one from the same task\n",
    "            head_filename = checkpoint_path / f\"{dataset_name}_{target_domain}_{'3d' if use_3d else '2d'}_head.pth\"\n",
    "            if head_filename.exists():\n",
    "                with torch.serialization.safe_globals([UnetOutBlock, Convolution, Conv3d]):\n",
    "                    target_model.head.load_state_dict(torch.load(head_filename, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\").state_dict())\n",
    "            else:\n",
    "                print(\n",
    "                    f\"   ❗ Warning: Head file {head_filename} does not exist. Skipping evaluation.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Evaluate cross-domain performance\n",
    "            metrics = target_model.evaluate()\n",
    "            update_metrics(f\"{composite_key}_adaptation\", metrics)\n",
    "\n",
    "            print(\n",
    "                f\"   ✅ {dataset_name} {target_domain}: Dice={metrics.get('dice', 0):.3f}, Hausdorff={metrics.get('hausdorff', 0):.3f}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"   ❌ {dataset_name} {target_domain}\") from  e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d41e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 COMPREHENSIVE RESULTS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "🏁 Baseline Performance:\n",
      "   CHAOS_MR_3d_baseline: Dice=0.241, HD=74.361\n",
      "   CHAOS_CT_3d_baseline: Dice=0.127, HD=100.085\n",
      "   MMWHS_MR_3d_baseline: Dice=0.001, HD=72.752\n",
      "   MMWHS_CT_3d_baseline: Dice=0.161, HD=62.818\n",
      "\n",
      "🏋️‍♂️ After Head-Training Performance:\n",
      "   CHAOS_MR_3d_head: Dice=0.229, HD=74.416\n",
      "   CHAOS_CT_3d_head: Dice=0.384, HD=46.022\n",
      "   MMWHS_MR_3d_head: Dice=0.223, HD=29.732\n",
      "   MMWHS_CT_3d_head: Dice=0.205, HD=37.370\n",
      "\n",
      "🏆 Finetuned Performance:\n",
      "   CHAOS_MR_3d_finetuned: Dice=0.753, HD=61.213\n",
      "   CHAOS_CT_3d_finetuned: Dice=0.518, HD=69.145\n",
      "   MMWHS_MR_3d_finetuned: Dice=0.777, HD=32.957\n",
      "   MMWHS_CT_3d_finetuned: Dice=0.790, HD=28.309\n",
      "\n",
      "🔄 Cross-Domain Adaptation Results:\n",
      "   CHAOS_MR_adaptation: Dice=0.229, HD=74.416\n",
      "   CHAOS_CT_adaptation: Dice=0.384, HD=46.022\n",
      "   MMWHS_MR_adaptation: Dice=0.223, HD=29.732\n",
      "   MMWHS_CT_adaptation: Dice=0.205, HD=37.370\n"
     ]
    }
   ],
   "source": [
    "# Load and display all metrics\n",
    "metrics_file = outputs_path / \"metrics.json\"\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        all_metrics = json.load(f)\n",
    "\n",
    "    print(\"\\n📊 COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Baseline performance\n",
    "    print(\"\\n🏁 Baseline Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"baseline\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # After Head-training performance\n",
    "    print(\"\\n🏋️‍♂️ After Head-Training Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"head\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # Finetuned performance\n",
    "    print(\"\\n🏆 Finetuned Performance:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"finetuned\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "\n",
    "    # Cross-domain adaptation results\n",
    "    print(\"\\n🔄 Cross-Domain Adaptation Results:\")\n",
    "    for key, metrics in all_metrics.items():\n",
    "        if \"adaptation\" in key:\n",
    "            dice = metrics.get(\"train\").get(\"dice\", 0)\n",
    "            hausdorff = metrics.get(\"train\").get(\"hausdorff\", 0)\n",
    "            print(f\"   {key}: Dice={dice:.3f}, HD={hausdorff:.3f}\")\n",
    "else:\n",
    "    print(\"No metrics file found. Run the experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import shutil\n",
    "\n",
    "\n",
    "    # Copy checkpoints.zip to Google Drive\n",
    "    !zip -r /content/checkpoints.zip /content/xai/checkpoints\n",
    "    shutil.copy('/content/checkpoints.zip', '/content/drive/MyDrive/xai/checkpoints.zip')\n",
    "\n",
    "    # Copy metrics.json to Google Drive\n",
    "    shutil.copy('/content/xai/outputs/metrics.json', '/content/drive/MyDrive/xai/metrics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    !zip -r /kaggle/working/checkpoints.zip /kaggle/working/xai/checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
