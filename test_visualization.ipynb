{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb29726d",
   "metadata": {},
   "source": [
    "# Visualization Function Testing Notebook\n",
    "\n",
    "This notebook tests the visualization functions `visualize_sample_slice` with configurable dataset and domain settings. It focuses on ground truth visualization and skips 3D visualization as requested.\n",
    "\n",
    "## Features:\n",
    "- Tests CHAOS and MMWHS datasets\n",
    "- Supports CT and MR domains  \n",
    "- Configurable visualization parameters\n",
    "- Ground truth visualization testing\n",
    "- Error handling and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804e8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.registry import get_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f9166",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Change these constants to test different datasets and domains:\n",
    "- **DATASET_NAME**: \"CHAOS\" or \"MMWHS\"\n",
    "- **DOMAIN**: \"CT\" or \"MR\" \n",
    "- **ENCODER_TYPE**: \"resnet\" or \"swin_unetr\"\n",
    "- **USE_SEMANTIC_HEAD**: Enable/disable semantic guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e714f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration constants\n",
    "DATASET_NAME = \"MMWHS\"  # Change to \"MMWHS\" if needed\n",
    "DOMAIN = \"CT\"  # Change to \"CT\" if needed\n",
    "ENCODER_TYPE = \"swin_unetr\"  # Change to \"swin_unetr\" if needed\n",
    "BATCH_SIZE = 1\n",
    "NUM_WORKERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efea59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    # Only scale down to (96, 128, 128) if larger\n",
    "    target_shape = (96, 128, 128)\n",
    "    img = x[\"image\"].float()\n",
    "    lbl = x[\"label\"].float() if x[\"label\"] is not None else None\n",
    "\n",
    "    if img.shape[-3:] != target_shape:\n",
    "        img = torch.nn.functional.interpolate(\n",
    "            img.unsqueeze(0),\n",
    "            size=target_shape,\n",
    "            mode=\"trilinear\",\n",
    "            align_corners=False,\n",
    "        ).squeeze(0)\n",
    "        if lbl is not None:\n",
    "            lbl = (\n",
    "                torch.nn.functional.interpolate(\n",
    "                    lbl.unsqueeze(0), size=target_shape, mode=\"nearest\"\n",
    "                )\n",
    "                .squeeze(0)\n",
    "                .long()\n",
    "            )\n",
    "    else:\n",
    "        if lbl is not None:\n",
    "            lbl = lbl.long()\n",
    "\n",
    "    return {\"image\": img, \"label\": lbl}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17164a2",
   "metadata": {},
   "source": [
    "## Ground Truth Visualization Testing\n",
    "\n",
    "This section loads the dataset and tests the `visualize_sample_slice` function with ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29855ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ground_truth_visualization():\n",
    "    \"\"\"Test visualization functions with ground truth data.\"\"\"\n",
    "    print(\"ðŸ” Testing Ground Truth Visualization...\")\n",
    "    print(f\"Dataset: {DATASET_NAME}, Domain: {DOMAIN}\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = get_dataset(\n",
    "        dataset_name=DATASET_NAME,\n",
    "        domain=DOMAIN,\n",
    "        base_path=Path(\"data\"),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        preprocess=preprocess,\n",
    "        slice_2d=False,  # Use 3D data\n",
    "    )\n",
    "\n",
    "    # Get a sample from the dataset\n",
    "    loader = dataset.train_loader\n",
    "    batch = next(iter(loader))\n",
    "\n",
    "    # Extract a single sample\n",
    "    sample = {\n",
    "        \"image\": batch[\"image\"],  # Keep batch dimension for inference\n",
    "        \"label\": batch[\"label\"],  # Keep batch dimension for inference\n",
    "    }\n",
    "    return dataset, sample, batch\n",
    "\n",
    "\n",
    "def test_inference_and_visualization(dataset, batch):\n",
    "    \"\"\"Test inference with semantic head training and visualize both GT and prediction.\"\"\"\n",
    "    print(\"ðŸš€ Testing Inference with Semantic Head Training...\")\n",
    "    print(f\"Dataset: {DATASET_NAME}, Domain: {DOMAIN}\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Get model with semantic head\n",
    "    model = dataset.get_model(ENCODER_TYPE)\n",
    "\n",
    "    # Restore the checkpoint\n",
    "    checkpoint_path = Path(\"checkpoints\", f\"{DATASET_NAME}_{DOMAIN}_3d_finetuned.pth\")\n",
    "    finetuned_state_dict = torch.load(checkpoint_path, map_location=device).state_dict()\n",
    "    model.encoder.load_state_dict(finetuned_state_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    batch[\"image\"] = batch[\"image\"].to(device)\n",
    "    batch[\"label\"] = batch[\"label\"].to(device)\n",
    "\n",
    "    # Run inference\n",
    "    print(\"ðŸ”® Running inference...\")\n",
    "    outputs = model(batch[\"image\"])\n",
    "    preds = torch.argmax(outputs, dim=1, keepdim=True)\n",
    "\n",
    "    print(f\"Prediction unique values: {torch.unique(preds)}\")\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2799974",
   "metadata": {},
   "source": [
    "## Dataset Visualization Method Test\n",
    "\n",
    "Test the `dataset.visualize_sample_slice()` method with the loaded data. This section will show both ground truth and prediction visualizations for the **same sample** to enable direct comparison. The dataset-specific implementation automatically applies the correct rotation and flip parameters for each dataset type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cecbce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing Ground Truth Visualization...\n",
      "Dataset: MMWHS, Domain: CT\n",
      "Sample image shape: torch.Size([1, 1, 96, 128, 128])\n",
      "Sample label shape: torch.Size([1, 1, 96, 128, 128])\n",
      "\n",
      "ðŸ“Š Testing dataset.visualize_sample_slice method...\n"
     ]
    }
   ],
   "source": [
    "## Dataset Visualization Method Test\n",
    "dataset, sample, batch = test_ground_truth_visualization()\n",
    "\n",
    "# Test dataset.visualize_sample_slice method\n",
    "print(\"\\nðŸ“Š Testing dataset.visualize_sample_slice method...\")\n",
    "# Squeeze batch and channel dimensions for visualization\n",
    "sample[\"image\"] = sample[\"image\"].squeeze().permute(2, 1, 0).cpu().numpy()\n",
    "sample[\"label\"] = sample[\"label\"].squeeze().permute(2, 1, 0).cpu().numpy()\n",
    "# Use the dataset method instead of standalone function\n",
    "# dataset.visualize_sample_slice(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196981ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”® Testing dataset.visualize_sample_slice method with PREDICTIONS...\n",
      "ðŸš€ Testing Inference with Semantic Head Training...\n",
      "Dataset: MMWHS, Domain: CT\n",
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MMWHS' object has no attribute 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ”® Testing dataset.visualize_sample_slice method with PREDICTIONS...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.serialization.safe_globals(\n\u001b[32m     32\u001b[39m     [\n\u001b[32m     33\u001b[39m         SwinUNETR,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m ):\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# Get predictions using the same batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     preds = \u001b[43mtest_inference_and_visualization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Prepare prediction sample for visualization\u001b[39;00m\n\u001b[32m     63\u001b[39m pred_sample = {\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m: sample[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}  \u001b[38;5;66;03m# Will be filled below\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mtest_inference_and_visualization\u001b[39m\u001b[34m(dataset, batch)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Get model with semantic head\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m model = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mENCODER_TYPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Restore the checkpoint\u001b[39;00m\n\u001b[32m     45\u001b[39m checkpoint_path = Path(\u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDOMAIN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_3d_finetuned.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/src/datasets/common.py:41\u001b[39m, in \u001b[36mBaseDataset.get_model\u001b[39m\u001b[34m(self, encoder_type)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder_type=\u001b[33m\"\u001b[39m\u001b[33mswin_unetr\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     32\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m        encoder_type (str): Type of encoder to use ('swin_unetr' or 'resnet')\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m \u001b[33;03m        Medical3DSegmenter: Model with semantic guidance capabilities\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     39\u001b[39m     model = Medical3DSegmenter(\n\u001b[32m     40\u001b[39m         encoder_type=encoder_type,\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         num_classes=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m,\n\u001b[32m     42\u001b[39m         pretrained=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     43\u001b[39m         dataset=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m     44\u001b[39m     )\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[31mAttributeError\u001b[39m: 'MMWHS' object has no attribute 'num_classes'"
     ]
    }
   ],
   "source": [
    "## Prediction Visualization\n",
    "\n",
    "# Import required modules for safe loading\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from monai.networks.nets.swin_unetr import BasicLayer\n",
    "from monai.networks.nets.swin_unetr import SwinTransformerBlock\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from monai.networks.nets.swin_unetr import WindowAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules.linear import Identity\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from torch.nn.modules.activation import GELU\n",
    "from monai.networks.nets.swin_unetr import PatchMerging\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from monai.networks.blocks.unetr_block import UnetrUpBlock\n",
    "from torch.nn.modules.conv import ConvTranspose3d\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "\n",
    "# Run inference on the SAME sample\n",
    "print(\"\\nðŸ”® Testing dataset.visualize_sample_slice method with PREDICTIONS...\")\n",
    "with torch.serialization.safe_globals(\n",
    "    [\n",
    "        SwinUNETR,\n",
    "        SwinTransformer,\n",
    "        PatchEmbed,\n",
    "        Conv3d,\n",
    "        Dropout,\n",
    "        ModuleList,\n",
    "        BasicLayer,\n",
    "        SwinTransformerBlock,\n",
    "        LayerNorm,\n",
    "        WindowAttention,\n",
    "        Linear,\n",
    "        Softmax,\n",
    "        Identity,\n",
    "        MLPBlock,\n",
    "        GELU,\n",
    "        PatchMerging,\n",
    "        UnetrBasicBlock,\n",
    "        UnetResBlock,\n",
    "        Convolution,\n",
    "        LeakyReLU,\n",
    "        InstanceNorm3d,\n",
    "        UnetrUpBlock,\n",
    "        ConvTranspose3d,\n",
    "        UnetOutBlock,\n",
    "    ]\n",
    "):\n",
    "    # Get predictions using the same batch\n",
    "    preds = test_inference_and_visualization(dataset, batch)\n",
    "\n",
    "# Prepare prediction sample for visualization\n",
    "pred_sample = {\"image\": sample[\"image\"], \"label\": None}  # Will be filled below\n",
    "\n",
    "# Apply dataset-specific label mapping for predictions\n",
    "preds = dataset.encode(preds)\n",
    "print(\"Prediction unique values:\", np.unique(pred_sample[\"label\"]))\n",
    "# Use the dataset method for prediction visualization\n",
    "dataset.visualize_sample_slice(sample)\n",
    "dataset.visualize_sample_slice(pred_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
