{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a1e6e6",
   "metadata": {},
   "source": [
    "# Conflict minimization when combining task vectors (AWD + TATR)\n",
    "\n",
    "This notebook scaffolds experiments to reduce conflicts when combining **task vectors** (parameter deltas).\n",
    "\n",
    "We implement:\n",
    "- **Adaptive Weight Disentanglement (AWD)**: edit each task vector by *removing* components that are aligned with the (conflicting) subspace induced by other tasks.\n",
    "- **Task Arithmetic in Trust Region (TATR)**: combine (possibly edited) vectors, then **scale** the result to remain inside a trust region (norm bound + optional cosine constraints vs each task).\n",
    "\n",
    "We test effectiveness by:\n",
    "1) Comparing task vectors before/after editing (cosine/conflict matrices, norms)\n",
    "2) Comparing combined vector alignment to each task\n",
    "3) Running a small sample evaluation on 4 datasets\n",
    "\n",
    "> You will need to plug in how your repo loads models and datasets. The vector logic is framework-agnostic beyond PyTorch state_dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fed0d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /home/basilef/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project\n"
     ]
    }
   ],
   "source": [
    "# If needed (uncomment):\n",
    "# %pip install -r ../requirements.txt\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def _find_repo_root(start: Path) -> Path:\n",
    "    p = start.resolve()\n",
    "    for _ in range(6):\n",
    "        if (p / \"src\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return start.resolve()\n",
    "\n",
    "REPO_ROOT = _find_repo_root(Path.cwd())\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "print(\"Repo root:\", REPO_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6591bd",
   "metadata": {},
   "source": [
    "## 0) Configuration\n",
    "\n",
    "Define where your **base model** and **task-finetuned models** live. You can use either:\n",
    "- paths to checkpoints (preferred), or\n",
    "- in-memory model objects.\n",
    "\n",
    "The only hard requirement is that you can obtain `state_dict()` for base and each task model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "095a0af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentConfig(device='cpu', vector_device='cpu', dtype=torch.float32, use_3d=False, encoder_type='clipseg', task_names=('CHAOS_CT', 'CHAOS_MR', 'MMWHS_CT', 'MMWHS_MR'), repo_root=PosixPath('/home/basilef/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project'), data_path=PosixPath('/home/basilef/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/data'), checkpoint_path=PosixPath('/home/basilef/Documents/Magistrale/Anno2_Semestre2/Explainable_and_Trustworthy_AI/project/checkpoints'), batch_size=4, spatial_size=128, num_workers=0, max_eval_batches=10, addition_alpha=0.8, trust_radius=5.0, min_cos_to_each_task=-0.2, tatr_max_steps=30, tatr_shrink=0.85, awd_strength=1.0, awd_k=6.0, eps=1e-08)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "def _find_repo_root(start: Path) -> Path:\n",
    "    p = start.resolve()\n",
    "    for _ in range(6):\n",
    "        if (p / \"src\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return start.resolve()\n",
    "\n",
    "REPO_ROOT = _find_repo_root(Path.cwd())\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    # Evaluation device (models will be moved here for forward passes)\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Vector arithmetic device (keep on CPU to avoid GPU OOM during cosine/stacking)\n",
    "    vector_device: str = \"cpu\"\n",
    "    dtype: torch.dtype = torch.float32\n",
    "\n",
    "    # 2D pipeline only\n",
    "    use_3d: bool = False\n",
    "    encoder_type: str = \"clipseg\"\n",
    "\n",
    "    # The 4 tasks in this repo = (dataset, domain) pairs\n",
    "    task_names: Tuple[str, str, str, str] = (\"CHAOS_CT\", \"CHAOS_MR\", \"MMWHS_CT\", \"MMWHS_MR\")\n",
    "\n",
    "    # Paths (absolute, derived from repo root)\n",
    "    repo_root: Path = REPO_ROOT\n",
    "    data_path: Path = REPO_ROOT / \"data\"\n",
    "    checkpoint_path: Path = REPO_ROOT / \"checkpoints\"\n",
    "\n",
    "    # Data / eval knobs\n",
    "    batch_size: int = 4\n",
    "    spatial_size: int = 128\n",
    "    num_workers: int = 0\n",
    "    max_eval_batches: int = 10\n",
    "\n",
    "    # Task addition scaling (mirrors local.ipynb usage of alpha)\n",
    "    addition_alpha: float = 0.8\n",
    "\n",
    "    # Trust region parameters (TATR)\n",
    "    trust_radius: float = 5.0\n",
    "    min_cos_to_each_task: float = -0.2\n",
    "    tatr_max_steps: int = 30\n",
    "    tatr_shrink: float = 0.85\n",
    "\n",
    "    # AWD parameters\n",
    "    awd_strength: float = 1.0\n",
    "    awd_k: float = 6.0\n",
    "    eps: float = 1e-8\n",
    "\n",
    "CFG = ExperimentConfig()\n",
    "CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf1580",
   "metadata": {},
   "source": [
    "## 1) Task-vector utilities\n",
    "\n",
    "A task vector is the parameter delta: `Î”_task = Î¸_task - Î¸_base`.\n",
    "\n",
    "We keep deltas as `Dict[str, Tensor]` keyed by parameter names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af1c344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "StateDict = Dict[str, torch.Tensor]\n",
    "TaskVector = Dict[str, torch.Tensor]\n",
    "\n",
    "def to_device(sd: StateDict, device: str, dtype: torch.dtype) -> StateDict:\n",
    "    out = {}\n",
    "    for k, v in sd.items():\n",
    "        if torch.is_tensor(v):\n",
    "            out[k] = v.detach().to(device=device, dtype=dtype)\n",
    "    return out\n",
    "\n",
    "def compute_task_vector(base_sd: StateDict, task_sd: StateDict) -> TaskVector:\n",
    "    # Î”_task = Î¸_task - Î¸_base, on matching tensor keys/shapes\n",
    "    delta = {}\n",
    "    for k in base_sd.keys():\n",
    "        if k in task_sd and torch.is_tensor(base_sd[k]) and torch.is_tensor(task_sd[k]):\n",
    "            if base_sd[k].shape == task_sd[k].shape:\n",
    "                if getattr(base_sd[k].dtype, \"is_floating_point\", False):\n",
    "                    delta[k] = task_sd[k] - base_sd[k]\n",
    "    return delta\n",
    "\n",
    "def apply_task_vector_(model: torch.nn.Module, delta: TaskVector, scale: float = 1.0) -> None:\n",
    "    # In-place add to parameters; avoids materializing full state_dict copies.\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in delta and param.shape == delta[name].shape:\n",
    "                param.add_(delta[name].to(device=param.device, dtype=param.dtype), alpha=float(scale))\n",
    "\n",
    "def tv_dot(a: TaskVector, b: TaskVector) -> torch.Tensor:\n",
    "    # Streaming dot product over intersection of keys.\n",
    "    keys = set(a.keys()) & set(b.keys())\n",
    "    acc = None\n",
    "    for k in keys:\n",
    "        av = a[k]\n",
    "        bv = b[k]\n",
    "        if av.shape != bv.shape:\n",
    "            continue\n",
    "        term = (av.reshape(-1) * bv.reshape(-1)).sum()\n",
    "        acc = term if acc is None else (acc + term)\n",
    "    if acc is None:\n",
    "        return torch.tensor(0.0)\n",
    "    return acc\n",
    "\n",
    "def tv_norm2(a: TaskVector) -> torch.Tensor:\n",
    "    acc = None\n",
    "    for v in a.values():\n",
    "        term = (v.reshape(-1) * v.reshape(-1)).sum()\n",
    "        acc = term if acc is None else (acc + term)\n",
    "    if acc is None:\n",
    "        return torch.tensor(0.0)\n",
    "    return acc\n",
    "\n",
    "def tv_norm(a: TaskVector, eps: float = 1e-8) -> float:\n",
    "    return float(torch.sqrt(tv_norm2(a) + eps).detach().cpu())\n",
    "\n",
    "def tv_cosine(a: TaskVector, b: TaskVector, eps: float = 1e-8) -> float:\n",
    "    dot = tv_dot(a, b)\n",
    "    na2 = tv_norm2(a)\n",
    "    nb2 = tv_norm2(b)\n",
    "    denom = torch.sqrt((na2 + eps) * (nb2 + eps))\n",
    "    if float(denom.detach().cpu()) == 0.0:\n",
    "        return float(\"nan\")\n",
    "    return float((dot / denom).detach().cpu())\n",
    "\n",
    "def pairwise_cosine_matrix(deltas: List[TaskVector], eps: float = 1e-8) -> np.ndarray:\n",
    "    n = len(deltas)\n",
    "    M = np.zeros((n, n), dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            M[i, j] = tv_cosine(deltas[i], deltas[j], eps=eps)\n",
    "    return M\n",
    "\n",
    "def conflict_score_from_cos(M: np.ndarray) -> float:\n",
    "    # Average negative cosine off-diagonal (how much tasks point against each other)\n",
    "    n = M.shape[0]\n",
    "    vals = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                vals.append(min(0.0, float(M[i, j])))\n",
    "    return float(np.mean(vals)) if vals else float(\"nan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adece66",
   "metadata": {},
   "source": [
    "## 2) AWD â€” Adaptive Weight Disentanglement\n",
    "\n",
    "Intuition: for each task vector and each parameter tensor, if it is **anti-aligned** with the mean direction of other tasks (conflict), remove the component along that conflicting mean.\n",
    "\n",
    "We do it **adaptively** with a gate `g âˆˆ [0,1]` that grows when cosine is negative.\n",
    "\n",
    "**Result**: edited deltas `Î”'_t` with reduced negative interference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c12d5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 1.0 / (1.0 + torch.exp(-x))\n",
    "\n",
    "def awd_edit(\n",
    "    deltas: List[TaskVector],\n",
    "    strength: float = 1.0,\n",
    "    k: float = 6.0,\n",
    "    eps: float = 1e-8,\n",
    ") -> Tuple[List[TaskVector], Dict[str, float]]:\n",
    "    \"\"\"Adaptive Weight Disentanglement (AWD).\n",
    "\n",
    "    For each task t and param key:\n",
    "      m = mean_{u!=t}(Î”_u)\n",
    "      cos = <Î”_t, m> / (||Î”_t|| ||m||)\n",
    "      gate = sigmoid(k * (-cos))  # ~1 when cos negative (conflict), ~0 when positive\n",
    "      Î”'_t = Î”_t - strength*gate*proj_m(Î”_t)\n",
    "    \"\"\"\n",
    "    T = len(deltas)\n",
    "    if T < 2:\n",
    "        return deltas, {\"note\": \"AWD skipped (need >=2 tasks)\"}\n",
    "\n",
    "    keys = sorted(set().union(*[d.keys() for d in deltas]))\n",
    "    edited: List[TaskVector] = [dict() for _ in range(T)]\n",
    "\n",
    "    gates_accum = []\n",
    "    cos_accum = []\n",
    "    removed_energy = []\n",
    "\n",
    "    for key in keys:\n",
    "        tensors = [d.get(key, None) for d in deltas]\n",
    "        if any(t is None for t in tensors):\n",
    "            for t in range(T):\n",
    "                if tensors[t] is not None:\n",
    "                    edited[t][key] = tensors[t]\n",
    "            continue\n",
    "\n",
    "        # Flatten each tensor for this key -> [T, D]\n",
    "        stacked = torch.stack([tt.reshape(-1) for tt in tensors], dim=0)\n",
    "        sum_all = stacked.sum(dim=0)  # [D]\n",
    "        for t in range(T):\n",
    "            dt = stacked[t]  # [D]\n",
    "            m = (sum_all - dt) / (T - 1)  # [D]\n",
    "            dn = dt.norm() + eps\n",
    "            mn = m.norm() + eps\n",
    "            c = torch.dot(dt, m) / (dn * mn)\n",
    "            gate = _sigmoid(k * (-c))\n",
    "            proj = (torch.dot(dt, m) / (mn * mn)) * m\n",
    "            dt_new = dt - (strength * gate) * proj\n",
    "\n",
    "            gates_accum.append(float(gate.detach().cpu()))\n",
    "            cos_accum.append(float(c.detach().cpu()))\n",
    "            removed_energy.append(float(((strength * gate) * proj).norm().detach().cpu()))\n",
    "\n",
    "            edited[t][key] = dt_new.reshape(tensors[t].shape)\n",
    "\n",
    "    stats = {\n",
    "        \"avg_gate\": float(np.mean(gates_accum)) if gates_accum else float(\"nan\"),\n",
    "        \"avg_cos_to_others_mean\": float(np.mean(cos_accum)) if cos_accum else float(\"nan\"),\n",
    "        \"avg_removed_norm\": float(np.mean(removed_energy)) if removed_energy else float(\"nan\"),\n",
    "    }\n",
    "    return edited, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32cc1ab",
   "metadata": {},
   "source": [
    "## 3) TATR â€” Task Arithmetic in Trust Region\n",
    "\n",
    "Combine deltas (e.g., sum or weighted sum), then **shrink** the combined update until it satisfies:\n",
    "- global norm bound: `||Î”_comb|| â‰¤ trust_radius`\n",
    "- optional alignment constraint: `cos(Î”_comb, Î”_task_i) â‰¥ min_cos_to_each_task`\n",
    "\n",
    "This is a lightweight trust region that does not require extra gradient steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adf93c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(deltas: List[TaskVector], weights: List[float]) -> TaskVector:\n",
    "    assert len(deltas) == len(weights)\n",
    "    keys = sorted(set().union(*[d.keys() for d in deltas]))\n",
    "    out: TaskVector = {}\n",
    "    for k in keys:\n",
    "        acc = None\n",
    "        for d, w in zip(deltas, weights):\n",
    "            if k not in d:\n",
    "                continue\n",
    "            term = d[k] * float(w)\n",
    "            acc = term if acc is None else (acc + term)\n",
    "        if acc is not None:\n",
    "            out[k] = acc\n",
    "    return out\n",
    "\n",
    "def scale_delta(delta: TaskVector, s: float) -> TaskVector:\n",
    "    return {k: v * float(s) for k, v in delta.items()}\n",
    "\n",
    "def tatr_combine(\n",
    "    deltas: List[TaskVector],\n",
    "    weights: Optional[List[float]] = None,\n",
    "    trust_radius: float = 5.0,\n",
    "    min_cos_to_each_task: Optional[float] = None,\n",
    "    max_steps: int = 30,\n",
    "    shrink: float = 0.85,\n",
    "    eps: float = 1e-8,\n",
    ") -> Tuple[TaskVector, Dict[str, float]]:\n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(deltas)\n",
    "    comb0 = weighted_sum(deltas, weights)\n",
    "    n0 = tv_norm(comb0, eps=eps)\n",
    "\n",
    "    def ok(delta: TaskVector) -> Tuple[bool, float, float]:\n",
    "        n = tv_norm(delta, eps=eps)\n",
    "        min_cos = float(\"inf\")\n",
    "        if min_cos_to_each_task is not None:\n",
    "            for d in deltas:\n",
    "                min_cos = min(min_cos, tv_cosine(delta, d, eps=eps))\n",
    "        else:\n",
    "            min_cos = float(\"nan\")\n",
    "        norm_ok = (n <= trust_radius + 1e-12)\n",
    "        cos_ok = True if min_cos_to_each_task is None else (min_cos >= min_cos_to_each_task)\n",
    "        return (norm_ok and cos_ok), n, min_cos\n",
    "\n",
    "    s = 1.0\n",
    "    best = comb0\n",
    "    best_s = 1.0\n",
    "    last_min_cos = float(\"nan\")\n",
    "    for _step in range(max_steps + 1):\n",
    "        cand = scale_delta(comb0, s)\n",
    "        good, n, min_cos = ok(cand)\n",
    "        last_min_cos = min_cos\n",
    "        if good:\n",
    "            best, best_s = cand, s\n",
    "            break\n",
    "        s *= shrink\n",
    "\n",
    "    out_stats = {\n",
    "        \"initial_norm\": float(n0),\n",
    "        \"final_scale\": float(best_s),\n",
    "        \"final_norm\": float(tv_norm(best, eps=eps)),\n",
    "    }\n",
    "    if min_cos_to_each_task is not None:\n",
    "        out_stats[\"final_min_cos_to_tasks\"] = float(last_min_cos)\n",
    "    return best, out_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95dd0c",
   "metadata": {},
   "source": [
    "## 4) Vector-level diagnostics (before/after AWD, and after combining)\n",
    "\n",
    "These are quick checks to quantify conflicts:\n",
    "- pairwise cosine matrix\n",
    "- conflict score (mean negative cosine off-diagonal)\n",
    "- norms\n",
    "- alignment of combined vector to each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c4b0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_vectors(task_names: List[str], deltas: List[TaskVector], eps: float = 1e-8) -> Dict[str, object]:\n",
    "    M = pairwise_cosine_matrix(deltas, eps=eps)\n",
    "    norms = [tv_norm(d, eps=eps) for d in deltas]\n",
    "    return {\n",
    "        \"task_names\": task_names,\n",
    "        \"cosine_matrix\": M,\n",
    "        \"conflict_score\": conflict_score_from_cos(M),\n",
    "        \"norms\": norms,\n",
    "    }\n",
    "\n",
    "def combined_alignment(task_names: List[str], comb: TaskVector, deltas: List[TaskVector], eps: float = 1e-8) -> Dict[str, float]:\n",
    "    out: Dict[str, float] = {}\n",
    "    for name, d in zip(task_names, deltas):\n",
    "        out[f\"cos_to_{name}\"] = tv_cosine(comb, d, eps=eps)\n",
    "    out[\"combined_norm\"] = tv_norm(comb, eps=eps)\n",
    "    return out\n",
    "\n",
    "def print_matrix(names: List[str], M: np.ndarray, fmt: str = \"{:+.3f}\") -> None:\n",
    "    header = \" \" * 12 + \" \".join([f\"{n:>10}\" for n in names])\n",
    "    print(header)\n",
    "    for i, ni in enumerate(names):\n",
    "        row = \" \".join([f\"{fmt.format(M[i,j]):>10}\" for j in range(len(names))])\n",
    "        print(f\"{ni:>10}  {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c7ddb",
   "metadata": {},
   "source": [
    "## 5) Minimal evaluation harness on 4 datasets\n",
    "\n",
    "This is intentionally pluggable, because your repo likely has its own data + evaluation.\n",
    "\n",
    "You provide:\n",
    "- `load_base_model() -> nn.Module`\n",
    "- `load_task_model(task_name) -> nn.Module` or task checkpoint\n",
    "- `get_eval_loader(task_name) -> iterable` yielding batches\n",
    "- `eval_step(model, batch) -> Dict[str,float]` (e.g., loss, accuracy)\n",
    "\n",
    "Then we evaluate:\n",
    "- base model\n",
    "- per-task edited model (base + edited Î”_task)\n",
    "- combined model (base + combined Î”)\n",
    "\n",
    "Use small sample sizes to keep this notebook fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4693f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_loader(\n",
    "    model: torch.nn.Module,\n",
    "    loader,\n",
    "    eval_step: Callable[[torch.nn.Module, object], Dict[str, float]],\n",
    "    max_batches: int = 10,\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    agg = {}\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "            out = eval_step(model, batch)\n",
    "            for k, v in out.items():\n",
    "                agg[k] = agg.get(k, 0.0) + float(v)\n",
    "            n += 1\n",
    "    if n == 0:\n",
    "        return {\"note\": \"no batches\"}\n",
    "    return {k: v / n for k, v in agg.items()}\n",
    "\n",
    "def clone_model(model: torch.nn.Module) -> torch.nn.Module:\n",
    "    # Generic cloning: reinstantiate is repo-specific; here we deep-copy.\n",
    "    return copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3bfac",
   "metadata": {},
   "source": [
    "## 6) Experiment runner (fill in model/dataset hooks)\n",
    "\n",
    "Steps:\n",
    "1) Load base + 4 task-finetuned models\n",
    "2) Compute task deltas\n",
    "3) AWD edit deltas\n",
    "4) Combine with TATR\n",
    "5) Run vector diagnostics + small-sample evaluation\n",
    "\n",
    "### You must edit the TODO hooks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2D pipeline hooks (derived from local.ipynb/local.py) ---\n",
    "import gc\n",
    "\n",
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from src.utils import download_and_extract_dataset\n",
    "from monai import transforms\n",
    "\n",
    "def _cleanup_memory() -> None:\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def parse_task_name(task_name: str) -> Tuple[str, str]:\n",
    "    dataset_name, domain = task_name.split(\"_\", 1)\n",
    "    return dataset_name, domain\n",
    "\n",
    "def encoder_ckpt_path(task_name: str, kind: str) -> Path:\n",
    "    # kind in {'baseline','finetuned'}\n",
    "    ds, dom = parse_task_name(task_name)\n",
    "    return CFG.checkpoint_path / f\"{ds}_{dom}_{'3d' if CFG.use_3d else '2d'}_{kind}.pth\"\n",
    "\n",
    "def load_encoder_checkpoint(path: Path) -> torch.nn.Module:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing checkpoint: {path}\")\n",
    "    # Checkpoints were saved with torch.save(model.encoder, path)\n",
    "    return torch.load(path, map_location=CFG.vector_device, weights_only=False)\n",
    "\n",
    "def load_baseline_encoder() -> torch.nn.Module:\n",
    "    # Baselines are identical across tasks in this repo; pick a canonical baseline.\n",
    "    base_path = encoder_ckpt_path(CFG.task_names[0], kind=\"baseline\")\n",
    "    return load_encoder_checkpoint(base_path)\n",
    "\n",
    "def load_finetuned_encoder(task_name: str) -> torch.nn.Module:\n",
    "    fin_path = encoder_ckpt_path(task_name, kind=\"finetuned\")\n",
    "    return load_encoder_checkpoint(fin_path)\n",
    "\n",
    "# Normalization stats (mean, std) per dataset/domain\n",
    "NORM_STATS = {\n",
    "    (\"MMWHS\", \"MR\"): (186.5875, 258.5917),\n",
    "    (\"MMWHS\", \"CT\"): (-745.0086, 1042.7251),\n",
    "    (\"CHAOS\", \"MR\"): (90.8292, 168.8922),\n",
    "    (\"CHAOS\", \"CT\"): (-478.1732, 476.7163),\n",
    "}\n",
    "\n",
    "def get_decode_func(dataset_name: str, domain: str):\n",
    "    from src.datasets.mmwhs import mmwhs_labels\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        if domain in [\"MR\", \"MRI\"]:\n",
    "            return lambda labels: labels // 63\n",
    "        if domain == \"CT\":\n",
    "            return lambda labels: torch.where(labels > 0, 1.0, 0.0)\n",
    "    if dataset_name == \"MMWHS\":\n",
    "        def decode(labels):\n",
    "            decoded_labels = torch.zeros_like(labels, dtype=torch.float32)\n",
    "            for i, label_val in enumerate(mmwhs_labels.keys()):\n",
    "                decoded_labels[labels == label_val] = i\n",
    "            return decoded_labels\n",
    "        return decode\n",
    "    return lambda labels: labels\n",
    "\n",
    "def get_preprocessing(dataset_name: str, domain: str, is_training: bool):\n",
    "    decode_func = get_decode_func(dataset_name, domain)\n",
    "    mean, std = NORM_STATS.get((dataset_name, domain), (None, None))\n",
    "\n",
    "    image_transforms = [\n",
    "        transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "        transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        transforms.Resize(\n",
    "            spatial_size=CFG.spatial_size,\n",
    "            size_mode=\"longest\",\n",
    "            mode=\"area\",\n",
    "            anti_aliasing=True,\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.EnsureType(dtype=torch.float32),\n",
    "    ]\n",
    "    if mean is not None and std is not None:\n",
    "        image_transforms.append(\n",
    "            transforms.NormalizeIntensity(\n",
    "                subtrahend=float(mean),\n",
    "                divisor=float(std),\n",
    "                channel_wise=False,\n",
    "            )\n",
    "        )\n",
    "    if is_training:\n",
    "        image_transforms.extend([\n",
    "            transforms.RandGaussianNoise(prob=0.15, std=0.05),\n",
    "            transforms.RandAdjustContrast(prob=0.15, gamma=(0.95, 1.05)),\n",
    "        ])\n",
    "    image_transforms.append(transforms.RepeatChannel(repeats=3))\n",
    "    image_transform = transforms.Compose(image_transforms)\n",
    "\n",
    "    seg_transforms = [\n",
    "        transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "        transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.EnsureType(dtype=torch.long),\n",
    "        transforms.Lambda(lambda x: decode_func(x)),\n",
    "        transforms.Resize(\n",
    "            spatial_size=CFG.spatial_size,\n",
    "            size_mode=\"longest\",\n",
    "            mode=\"nearest\",\n",
    "        ),\n",
    "    ]\n",
    "    seg_transform = transforms.Compose(seg_transforms)\n",
    "    return image_transform, seg_transform\n",
    "\n",
    "def build_dataset_for_task(task_name: str, is_training: bool = False) -> BaseDataset:\n",
    "    dataset_name, domain = parse_task_name(task_name)\n",
    "    download_and_extract_dataset(dataset_name, CFG.data_path)\n",
    "    image_t, seg_t = get_preprocessing(dataset_name, domain, is_training=is_training)\n",
    "    ds: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        transform=image_t,\n",
    "        seg_transform=seg_t,\n",
    "        base_path=CFG.data_path,\n",
    "        batch_size=CFG.batch_size,\n",
    "        num_workers=CFG.num_workers,\n",
    "        slice_2d=True,\n",
    "    )\n",
    "    if not isinstance(ds, BaseDataset):\n",
    "        raise TypeError(f\"Expected BaseDataset, got {type(ds)}\")\n",
    "    return ds\n",
    "\n",
    "def unpack_batch(batch):\n",
    "    if isinstance(batch, dict):\n",
    "        return batch.get(\"image\"), batch.get(\"label\")\n",
    "    if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "        return batch[0], batch[1]\n",
    "    return None, None\n",
    "\n",
    "def eval_step(model: torch.nn.Module, batch) -> Dict[str, float]:\n",
    "    images, labels = unpack_batch(batch)\n",
    "    if images is None or labels is None:\n",
    "        return {}\n",
    "    images = images.to(CFG.device)\n",
    "    labels = labels.to(CFG.device)\n",
    "    try:\n",
    "        labels = labels.long()\n",
    "    except Exception:\n",
    "        pass\n",
    "    logits = model(images)  # (B, C, H, W) for 2D clipseg\n",
    "    preds = torch.argmax(logits, dim=1)  # (B, H, W)\n",
    "    y = labels.squeeze(1) if labels.ndim == 4 else labels\n",
    "    # mean Dice over foreground classes (exclude background=0)\n",
    "    eps = 1e-8\n",
    "    num_classes = logits.shape[1]\n",
    "    dices = []\n",
    "    for c in range(1, num_classes):\n",
    "        p = (preds == c)\n",
    "        g = (y == c)\n",
    "        inter = (p & g).sum().float()\n",
    "        denom = p.sum().float() + g.sum().float()\n",
    "        dice = (2.0 * inter + eps) / (denom + eps)\n",
    "        dices.append(dice)\n",
    "    mean_dice = (\n",
    "        torch.stack(dices).mean()\n",
    "        if len(dices)\n",
    "        else torch.tensor(float(\"nan\"), device=CFG.device)\n",
    "    )\n",
    "    return {\"mean_dice_fg\": float(mean_dice.detach().cpu())}\n",
    "\n",
    "def evaluate_encoder_on_task(encoder: torch.nn.Module, task_name: str, max_batches: int) -> Dict[str, float]:\n",
    "    ds = None\n",
    "    model = None\n",
    "    try:\n",
    "        ds = build_dataset_for_task(task_name, is_training=False)\n",
    "        model = ds.get_model(base_model=CFG.encoder_type).to(CFG.device)\n",
    "        model.encoder = encoder.to(CFG.device)\n",
    "        return evaluate_on_loader(model.encoder, ds.test_loader, eval_step, max_batches=max_batches)\n",
    "    finally:\n",
    "        # Important: free dataset/model between tasks to avoid RAM creep\n",
    "        try:\n",
    "            if model is not None:\n",
    "                model.to(\"cpu\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        del model, ds\n",
    "        _cleanup_memory()\n",
    "\n",
    "def evaluate_baseline_plus_delta(delta: Optional[TaskVector], task_name: str, max_batches: int, scale: float = 1.0) -> Dict[str, float]:\n",
    "    base_encoder = None\n",
    "    try:\n",
    "        base_encoder = load_baseline_encoder()\n",
    "        if delta is not None:\n",
    "            apply_task_vector_(base_encoder, delta, scale=scale)\n",
    "        return evaluate_encoder_on_task(base_encoder, task_name=task_name, max_batches=max_batches)\n",
    "    finally:\n",
    "        del base_encoder\n",
    "        _cleanup_memory()\n",
    "\n",
    "def build_raw_task_deltas(task_names: List[str]) -> Tuple[torch.nn.Module, List[torch.nn.Module], List[TaskVector]]:\n",
    "    base_encoder = load_baseline_encoder()\n",
    "    finetuned_encoders = [load_finetuned_encoder(tn) for tn in task_names]\n",
    "    base_sd = to_device(base_encoder.state_dict(), CFG.vector_device, CFG.dtype)\n",
    "    fin_sds = [to_device(m.state_dict(), CFG.vector_device, CFG.dtype) for m in finetuned_encoders]\n",
    "    deltas = [compute_task_vector(base_sd, sd) for sd in fin_sds]\n",
    "    return base_encoder, finetuned_encoders, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1b74992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Important fix: baseline checkpoints are per-task ---\n",
    "# local.ipynb applies each task vector to the *matching* baseline checkpoint (dataset+domain).\n",
    "# For sanity checks and fair evaluation, we also load the baseline per task here.\n",
    "\n",
    "def load_baseline_encoder(task_name: Optional[str] = None) -> torch.nn.Module:\n",
    "    if task_name is None:\n",
    "        task_name = CFG.task_names[0]\n",
    "    base_path = encoder_ckpt_path(task_name, kind=\"baseline\")\n",
    "    return load_encoder_checkpoint(base_path)\n",
    "\n",
    "def evaluate_baseline_plus_delta(\n",
    "    delta: Optional[TaskVector], task_name: str, max_batches: int, scale: float = 1.0\n",
    " ) -> Dict[str, float]:\n",
    "    base_encoder = load_baseline_encoder(task_name)\n",
    "    if delta is not None:\n",
    "        apply_task_vector_(base_encoder, delta, scale=scale)\n",
    "    return evaluate_encoder_on_task(base_encoder, task_name=task_name, max_batches=max_batches)\n",
    "\n",
    "def build_raw_task_deltas(task_names: List[str]) -> Tuple[List[torch.nn.Module], List[torch.nn.Module], List[TaskVector]]:\n",
    "    baseline_encoders = [load_baseline_encoder(tn) for tn in task_names]\n",
    "    finetuned_encoders = [load_finetuned_encoder(tn) for tn in task_names]\n",
    "    base_sds = [to_device(m.state_dict(), CFG.vector_device, CFG.dtype) for m in baseline_encoders]\n",
    "    fin_sds = [to_device(m.state_dict(), CFG.vector_device, CFG.dtype) for m in finetuned_encoders]\n",
    "    deltas = [compute_task_vector(bs, fs) for bs, fs in zip(base_sds, fin_sds)]\n",
    "    return baseline_encoders, finetuned_encoders, deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d666999",
   "metadata": {},
   "source": [
    "## 6A) Reference: finetuned vs baseline, and raw task vectors\n",
    "We first compute the 4 raw task vectors (finetuned âˆ’ baseline) and evaluate:\n",
    "- **finetuned encoder** on its own task (reference you care about)\n",
    "- **baseline** (to know the gap)\n",
    "- **baseline + raw Î”_task** (sanity check: should match finetuned, up to noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be4cc6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector diagnostics (raw deltas):\n",
      "conflict_score: 0.0\n",
      "norms: [57.1956901550293, 34.02861022949219, 84.9603042602539, 68.02688598632812]\n",
      "              CHAOS_CT   CHAOS_MR   MMWHS_CT   MMWHS_MR\n",
      "  CHAOS_CT      +1.000     +0.350     +0.342     +0.343\n",
      "  CHAOS_MR      +0.350     +1.000     +0.271     +0.278\n",
      "  MMWHS_CT      +0.342     +0.271     +1.000     +0.304\n",
      "  MMWHS_MR      +0.343     +0.278     +0.304     +1.000\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dice_finetuned</th>\n",
       "      <th>dice_baseline</th>\n",
       "      <th>dice_base_plus_raw_delta</th>\n",
       "      <th>delta_norm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHAOS_CT</th>\n",
       "      <td>0.971517</td>\n",
       "      <td>0.132276</td>\n",
       "      <td>0.971529</td>\n",
       "      <td>57.195690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAOS_MR</th>\n",
       "      <td>0.821681</td>\n",
       "      <td>0.018059</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>34.028610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMWHS_CT</th>\n",
       "      <td>0.936441</td>\n",
       "      <td>0.021665</td>\n",
       "      <td>0.936432</td>\n",
       "      <td>84.960304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMWHS_MR</th>\n",
       "      <td>0.871081</td>\n",
       "      <td>0.010305</td>\n",
       "      <td>0.871078</td>\n",
       "      <td>68.026886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dice_finetuned  dice_baseline  dice_base_plus_raw_delta  delta_norm\n",
       "task                                                                         \n",
       "CHAOS_CT        0.971517       0.132276                  0.971529   57.195690\n",
       "CHAOS_MR        0.821681       0.018059                  0.821549   34.028610\n",
       "MMWHS_CT        0.936441       0.021665                  0.936432   84.960304\n",
       "MMWHS_MR        0.871081       0.010305                  0.871078   68.026886"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "task_names = list(CFG.task_names)\n",
    "baseline_encoders, finetuned_encoders, deltas = build_raw_task_deltas(task_names)\n",
    "\n",
    "print(\"Vector diagnostics (raw deltas):\")\n",
    "orig_summary = summarize_vectors(task_names, deltas, eps=CFG.eps)\n",
    "print(\"conflict_score:\", orig_summary[\"conflict_score\"])\n",
    "print(\"norms:\", orig_summary[\"norms\"])\n",
    "print_matrix(task_names, orig_summary[\"cosine_matrix\"])\n",
    "\n",
    "rows = []\n",
    "for tn, fin_enc, dt in zip(task_names, finetuned_encoders, deltas):\n",
    "    s_finetuned = evaluate_encoder_on_task(fin_enc, tn, max_batches=CFG.max_eval_batches)\n",
    "    s_baseline = evaluate_baseline_plus_delta(None, tn, max_batches=CFG.max_eval_batches)\n",
    "    s_base_plus_raw = evaluate_baseline_plus_delta(dt, tn, max_batches=CFG.max_eval_batches, scale=1.0)\n",
    "    rows.append({\n",
    "        \"task\": tn,\n",
    "        \"dice_finetuned\": s_finetuned.get(\"mean_dice_fg\"),\n",
    "        \"dice_baseline\": s_baseline.get(\"mean_dice_fg\"),\n",
    "        \"dice_base_plus_raw_delta\": s_base_plus_raw.get(\"mean_dice_fg\"),\n",
    "        \"delta_norm\": tv_norm(dt, eps=CFG.eps),\n",
    "    })\n",
    "\n",
    "ref_df = pd.DataFrame(rows).set_index(\"task\")\n",
    "display(ref_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1311688c",
   "metadata": {},
   "source": [
    "## 6B) AWD test (edit each task vector, no trust region)\n",
    "This block isolates AWD: we edit each $\\Delta_{task}$ and compare against the **finetuned** model (not baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e2ad470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWD stats: {'avg_gate': 0.46810992170249516, 'avg_cos_to_others_mean': 0.04080290108022237, 'avg_removed_norm': 0.012791253246163366}\n",
      "Vector diagnostics (after AWD):\n",
      "conflict_score: 0.0\n",
      "norms: [56.936527252197266, 33.85821533203125, 84.52819061279297, 67.6814193725586]\n",
      "              CHAOS_CT   CHAOS_MR   MMWHS_CT   MMWHS_MR\n",
      "  CHAOS_CT      +1.000     +0.331     +0.313     +0.317\n",
      "  CHAOS_MR      +0.331     +1.000     +0.242     +0.252\n",
      "  MMWHS_CT      +0.313     +0.242     +1.000     +0.264\n",
      "  MMWHS_MR      +0.317     +0.252     +0.264     +1.000\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dice_finetuned</th>\n",
       "      <th>dice_base_plus_AWD_delta</th>\n",
       "      <th>cos(AWD_delta, raw_delta)</th>\n",
       "      <th>||AWD_delta||</th>\n",
       "      <th>||raw_delta||</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHAOS_CT</th>\n",
       "      <td>0.971517</td>\n",
       "      <td>0.971293</td>\n",
       "      <td>0.999634</td>\n",
       "      <td>56.936527</td>\n",
       "      <td>57.195690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAOS_MR</th>\n",
       "      <td>0.821681</td>\n",
       "      <td>0.821197</td>\n",
       "      <td>0.999548</td>\n",
       "      <td>33.858215</td>\n",
       "      <td>34.028610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMWHS_CT</th>\n",
       "      <td>0.936441</td>\n",
       "      <td>0.918757</td>\n",
       "      <td>0.999529</td>\n",
       "      <td>84.528191</td>\n",
       "      <td>84.960304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMWHS_MR</th>\n",
       "      <td>0.871081</td>\n",
       "      <td>0.867409</td>\n",
       "      <td>0.999524</td>\n",
       "      <td>67.681419</td>\n",
       "      <td>68.026886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dice_finetuned  dice_base_plus_AWD_delta  cos(AWD_delta, raw_delta)  \\\n",
       "task                                                                            \n",
       "CHAOS_CT        0.971517                  0.971293                   0.999634   \n",
       "CHAOS_MR        0.821681                  0.821197                   0.999548   \n",
       "MMWHS_CT        0.936441                  0.918757                   0.999529   \n",
       "MMWHS_MR        0.871081                  0.867409                   0.999524   \n",
       "\n",
       "          ||AWD_delta||  ||raw_delta||  \n",
       "task                                    \n",
       "CHAOS_CT      56.936527      57.195690  \n",
       "CHAOS_MR      33.858215      34.028610  \n",
       "MMWHS_CT      84.528191      84.960304  \n",
       "MMWHS_MR      67.681419      68.026886  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "awd_deltas, awd_stats = awd_edit(deltas, strength=CFG.awd_strength, k=CFG.awd_k, eps=CFG.eps)\n",
    "print(\"AWD stats:\", awd_stats)\n",
    "\n",
    "print(\"Vector diagnostics (after AWD):\")\n",
    "awd_summary = summarize_vectors(task_names, awd_deltas, eps=CFG.eps)\n",
    "print(\"conflict_score:\", awd_summary[\"conflict_score\"])\n",
    "print(\"norms:\", awd_summary[\"norms\"])\n",
    "print_matrix(task_names, awd_summary[\"cosine_matrix\"])\n",
    "\n",
    "rows = []\n",
    "for tn, fin_enc, dt_raw, dt_awd in zip(task_names, finetuned_encoders, deltas, awd_deltas):\n",
    "    s_finetuned = evaluate_encoder_on_task(fin_enc, tn, max_batches=CFG.max_eval_batches)\n",
    "    s_base_plus_awd = evaluate_baseline_plus_delta(dt_awd, tn, max_batches=CFG.max_eval_batches, scale=1.0)\n",
    "    rows.append({\n",
    "        \"task\": tn,\n",
    "        \"dice_finetuned\": s_finetuned.get(\"mean_dice_fg\"),\n",
    "        \"dice_base_plus_AWD_delta\": s_base_plus_awd.get(\"mean_dice_fg\"),\n",
    "        \"cos(AWD_delta, raw_delta)\": tv_cosine(dt_awd, dt_raw, eps=CFG.eps),\n",
    "        \"||AWD_delta||\": tv_norm(dt_awd, eps=CFG.eps),\n",
    "        \"||raw_delta||\": tv_norm(dt_raw, eps=CFG.eps),\n",
    "    })\n",
    "\n",
    "awd_df = pd.DataFrame(rows).set_index(\"task\")\n",
    "display(awd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6d940",
   "metadata": {},
   "source": [
    "## 6C) TATR test (trust region on task addition)\n",
    "This block isolates TATR: we **add** task vectors (like in local.ipynb) but shrink the combined update into a trust region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "423600bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TATR stats: {'initial_norm': 138.55642700195312, 'final_scale': 0.032945601421837174, 'final_norm': 4.564825057983398, 'final_min_cos_to_tasks': 0.5542226433753967}\n",
      "Combined alignment (TATR vs each raw task delta):\n",
      "{'cos_to_CHAOS_CT': 0.7016519904136658, 'cos_to_CHAOS_MR': 0.5542226433753967, 'cos_to_MMWHS_CT': 0.7761434316635132, 'cos_to_MMWHS_MR': 0.7094733119010925, 'combined_norm': 4.564825057983398}\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dice_finetuned</th>\n",
       "      <th>dice_baseline_plus_TATR_add</th>\n",
       "      <th>cos(comb_tatr, delta_task)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHAOS_CT</th>\n",
       "      <td>0.971517</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.701652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAOS_MR</th>\n",
       "      <td>0.821681</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.554223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMWHS_CT</th>\n",
       "      <td>0.936441</td>\n",
       "      <td>0.114579</td>\n",
       "      <td>0.776143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMWHS_MR</th>\n",
       "      <td>0.871081</td>\n",
       "      <td>0.086661</td>\n",
       "      <td>0.709473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dice_finetuned  dice_baseline_plus_TATR_add  \\\n",
       "task                                                    \n",
       "CHAOS_CT        0.971517                     0.000513   \n",
       "CHAOS_MR        0.821681                     0.003378   \n",
       "MMWHS_CT        0.936441                     0.114579   \n",
       "MMWHS_MR        0.871081                     0.086661   \n",
       "\n",
       "          cos(comb_tatr, delta_task)  \n",
       "task                                  \n",
       "CHAOS_CT                    0.701652  \n",
       "CHAOS_MR                    0.554223  \n",
       "MMWHS_CT                    0.776143  \n",
       "MMWHS_MR                    0.709473  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Addition weights mirror local.ipynb: alpha * (sum of selected task vectors)\n",
    "add_weights = [CFG.addition_alpha] * len(task_names)\n",
    "\n",
    "comb_tatr, tatr_stats = tatr_combine(\n",
    "    deltas,\n",
    "    weights=add_weights,\n",
    "    trust_radius=CFG.trust_radius,\n",
    "    min_cos_to_each_task=CFG.min_cos_to_each_task,\n",
    "    max_steps=CFG.tatr_max_steps,\n",
    "    shrink=CFG.tatr_shrink,\n",
    "    eps=CFG.eps,\n",
    " )\n",
    "print(\"TATR stats:\", tatr_stats)\n",
    "print(\"Combined alignment (TATR vs each raw task delta):\")\n",
    "print(combined_alignment(task_names, comb_tatr, deltas, eps=CFG.eps))\n",
    "\n",
    "rows = []\n",
    "for tn, fin_enc in zip(task_names, finetuned_encoders):\n",
    "    s_finetuned = evaluate_encoder_on_task(fin_enc, tn, max_batches=CFG.max_eval_batches)\n",
    "    s_comb = evaluate_baseline_plus_delta(comb_tatr, tn, max_batches=CFG.max_eval_batches, scale=1.0)\n",
    "    rows.append({\n",
    "        \"task\": tn,\n",
    "        \"dice_finetuned\": s_finetuned.get(\"mean_dice_fg\"),\n",
    "        \"dice_baseline_plus_TATR_add\": s_comb.get(\"mean_dice_fg\"),\n",
    "        \"cos(comb_tatr, delta_task)\": tv_cosine(comb_tatr, deltas[task_names.index(tn)], eps=CFG.eps),\n",
    "    })\n",
    "\n",
    "tatr_df = pd.DataFrame(rows).set_index(\"task\")\n",
    "display(tatr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e94ca",
   "metadata": {},
   "source": [
    "## 7) Task addition (local.ipynb-style composites): raw vs AWD vs TATR\n",
    "This reproduces the *task addition / composite task vectors* patterns from `local.ipynb`, but evaluates them with the notebook's quick Dice-on-test-loader metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "789515ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver', 'Right kidney', 'Left kidney', 'Spleen']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Left ventricle blood cavity', 'Right ventricle blood cavity', 'Left atrium blood cavity', 'Right atrium blood cavity', 'Myocardium of the left ventricle', 'Ascending aorta', 'Pulmonary artery']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>composite</th>\n",
       "      <th>dice_finetuned</th>\n",
       "      <th>dice_raw_add</th>\n",
       "      <th>dice_AWD_add</th>\n",
       "      <th>dice_TATR_add</th>\n",
       "      <th>gap_raw_vs_finetuned</th>\n",
       "      <th>gap_AWD_vs_finetuned</th>\n",
       "      <th>gap_TATR_vs_finetuned</th>\n",
       "      <th>tatr_final_scale</th>\n",
       "      <th>tatr_final_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHAOS_CT</td>\n",
       "      <td>dataset</td>\n",
       "      <td>0.971517</td>\n",
       "      <td>8.480276e-01</td>\n",
       "      <td>0.842589</td>\n",
       "      <td>7.522615e-05</td>\n",
       "      <td>-0.123489</td>\n",
       "      <td>-0.128928</td>\n",
       "      <td>-0.971442</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>4.520902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHAOS_CT</td>\n",
       "      <td>domain</td>\n",
       "      <td>0.971517</td>\n",
       "      <td>7.391409e-01</td>\n",
       "      <td>0.756405</td>\n",
       "      <td>2.223001e-02</td>\n",
       "      <td>-0.232376</td>\n",
       "      <td>-0.215112</td>\n",
       "      <td>-0.949287</td>\n",
       "      <td>0.045599</td>\n",
       "      <td>4.287944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHAOS_CT</td>\n",
       "      <td>cross</td>\n",
       "      <td>0.971517</td>\n",
       "      <td>2.213085e-12</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>2.213085e-12</td>\n",
       "      <td>-0.971517</td>\n",
       "      <td>-0.971462</td>\n",
       "      <td>-0.971517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.108269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHAOS_MR</td>\n",
       "      <td>dataset</td>\n",
       "      <td>0.821681</td>\n",
       "      <td>2.147809e-01</td>\n",
       "      <td>0.209067</td>\n",
       "      <td>2.628602e-02</td>\n",
       "      <td>-0.606900</td>\n",
       "      <td>-0.612614</td>\n",
       "      <td>-0.795395</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>4.520902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHAOS_MR</td>\n",
       "      <td>domain</td>\n",
       "      <td>0.821681</td>\n",
       "      <td>1.564909e-01</td>\n",
       "      <td>0.163393</td>\n",
       "      <td>4.492932e-03</td>\n",
       "      <td>-0.665190</td>\n",
       "      <td>-0.658287</td>\n",
       "      <td>-0.817188</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>4.994809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CHAOS_MR</td>\n",
       "      <td>cross</td>\n",
       "      <td>0.821681</td>\n",
       "      <td>2.904907e-10</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>2.904907e-10</td>\n",
       "      <td>-0.821681</td>\n",
       "      <td>-0.821658</td>\n",
       "      <td>-0.821681</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>83.711319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MMWHS_CT</td>\n",
       "      <td>dataset</td>\n",
       "      <td>0.936441</td>\n",
       "      <td>4.875830e-01</td>\n",
       "      <td>0.491014</td>\n",
       "      <td>5.182586e-03</td>\n",
       "      <td>-0.448858</td>\n",
       "      <td>-0.445427</td>\n",
       "      <td>-0.931259</td>\n",
       "      <td>0.045599</td>\n",
       "      <td>4.520413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MMWHS_CT</td>\n",
       "      <td>domain</td>\n",
       "      <td>0.936441</td>\n",
       "      <td>4.626066e-01</td>\n",
       "      <td>0.491676</td>\n",
       "      <td>6.149314e-03</td>\n",
       "      <td>-0.473835</td>\n",
       "      <td>-0.444765</td>\n",
       "      <td>-0.930292</td>\n",
       "      <td>0.045599</td>\n",
       "      <td>4.287944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MMWHS_CT</td>\n",
       "      <td>cross</td>\n",
       "      <td>0.936441</td>\n",
       "      <td>4.539433e-02</td>\n",
       "      <td>0.043245</td>\n",
       "      <td>8.305805e-03</td>\n",
       "      <td>-0.891047</td>\n",
       "      <td>-0.893197</td>\n",
       "      <td>-0.928136</td>\n",
       "      <td>0.063113</td>\n",
       "      <td>4.810473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MMWHS_MR</td>\n",
       "      <td>dataset</td>\n",
       "      <td>0.871081</td>\n",
       "      <td>1.223958e-01</td>\n",
       "      <td>0.134596</td>\n",
       "      <td>4.599404e-03</td>\n",
       "      <td>-0.748685</td>\n",
       "      <td>-0.736485</td>\n",
       "      <td>-0.866482</td>\n",
       "      <td>0.045599</td>\n",
       "      <td>4.520413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MMWHS_MR</td>\n",
       "      <td>domain</td>\n",
       "      <td>0.871081</td>\n",
       "      <td>3.864613e-01</td>\n",
       "      <td>0.372686</td>\n",
       "      <td>3.067301e-02</td>\n",
       "      <td>-0.484620</td>\n",
       "      <td>-0.498395</td>\n",
       "      <td>-0.840408</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>4.994809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MMWHS_MR</td>\n",
       "      <td>cross</td>\n",
       "      <td>0.871081</td>\n",
       "      <td>6.996986e-02</td>\n",
       "      <td>0.068523</td>\n",
       "      <td>9.684630e-03</td>\n",
       "      <td>-0.801111</td>\n",
       "      <td>-0.802558</td>\n",
       "      <td>-0.861397</td>\n",
       "      <td>0.063113</td>\n",
       "      <td>4.662174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        task composite  dice_finetuned  dice_raw_add  dice_AWD_add  \\\n",
       "0   CHAOS_CT   dataset        0.971517  8.480276e-01      0.842589   \n",
       "1   CHAOS_CT    domain        0.971517  7.391409e-01      0.756405   \n",
       "2   CHAOS_CT     cross        0.971517  2.213085e-12      0.000054   \n",
       "3   CHAOS_MR   dataset        0.821681  2.147809e-01      0.209067   \n",
       "4   CHAOS_MR    domain        0.821681  1.564909e-01      0.163393   \n",
       "5   CHAOS_MR     cross        0.821681  2.904907e-10      0.000023   \n",
       "6   MMWHS_CT   dataset        0.936441  4.875830e-01      0.491014   \n",
       "7   MMWHS_CT    domain        0.936441  4.626066e-01      0.491676   \n",
       "8   MMWHS_CT     cross        0.936441  4.539433e-02      0.043245   \n",
       "9   MMWHS_MR   dataset        0.871081  1.223958e-01      0.134596   \n",
       "10  MMWHS_MR    domain        0.871081  3.864613e-01      0.372686   \n",
       "11  MMWHS_MR     cross        0.871081  6.996986e-02      0.068523   \n",
       "\n",
       "    dice_TATR_add  gap_raw_vs_finetuned  gap_AWD_vs_finetuned  \\\n",
       "0    7.522615e-05             -0.123489             -0.128928   \n",
       "1    2.223001e-02             -0.232376             -0.215112   \n",
       "2    2.213085e-12             -0.971517             -0.971462   \n",
       "3    2.628602e-02             -0.606900             -0.612614   \n",
       "4    4.492932e-03             -0.665190             -0.658287   \n",
       "5    2.904907e-10             -0.821681             -0.821658   \n",
       "6    5.182586e-03             -0.448858             -0.445427   \n",
       "7    6.149314e-03             -0.473835             -0.444765   \n",
       "8    8.305805e-03             -0.891047             -0.893197   \n",
       "9    4.599404e-03             -0.748685             -0.736485   \n",
       "10   3.067301e-02             -0.484620             -0.498395   \n",
       "11   9.684630e-03             -0.801111             -0.802558   \n",
       "\n",
       "    gap_TATR_vs_finetuned  tatr_final_scale  tatr_final_norm  \n",
       "0               -0.971442          0.074251         4.520902  \n",
       "1               -0.949287          0.045599         4.287944  \n",
       "2               -0.971517          1.000000        79.108269  \n",
       "3               -0.795395          0.074251         4.520902  \n",
       "4               -0.817188          0.074251         4.994809  \n",
       "5               -0.821681          1.000000        83.711319  \n",
       "6               -0.931259          0.045599         4.520413  \n",
       "7               -0.930292          0.045599         4.287944  \n",
       "8               -0.928136          0.063113         4.810473  \n",
       "9               -0.866482          0.045599         4.520413  \n",
       "10              -0.840408          0.074251         4.994809  \n",
       "11              -0.861397          0.063113         4.662174  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_gap_raw</th>\n",
       "      <th>mean_gap_AWD</th>\n",
       "      <th>mean_gap_TATR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>composite</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cross</th>\n",
       "      <td>-0.871339</td>\n",
       "      <td>-0.872219</td>\n",
       "      <td>-0.895683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <td>-0.481983</td>\n",
       "      <td>-0.480864</td>\n",
       "      <td>-0.891144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain</th>\n",
       "      <td>-0.464005</td>\n",
       "      <td>-0.454140</td>\n",
       "      <td>-0.884294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean_gap_raw  mean_gap_AWD  mean_gap_TATR\n",
       "composite                                           \n",
       "cross         -0.871339     -0.872219      -0.895683\n",
       "dataset       -0.481983     -0.480864      -0.891144\n",
       "domain        -0.464005     -0.454140      -0.884294"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _idx(task_name: str) -> int:\n",
    "    return task_names.index(task_name)\n",
    "\n",
    "def build_local_composites() -> Dict[str, List[Tuple[str, float]]]:\n",
    "    # Mirrors local.ipynb composite_task_vectors definitions\n",
    "    comps: Dict[str, List[Tuple[str, float]]] = {}\n",
    "    # Dataset composites\n",
    "    comps[\"MMWHS\"] = [(\"MMWHS_MR\", +1.0), (\"MMWHS_CT\", +1.0)]\n",
    "    comps[\"CHAOS\"] = [(\"CHAOS_MR\", +1.0), (\"CHAOS_CT\", +1.0)]\n",
    "    # Domain composites\n",
    "    comps[\"MR\"] = [(\"CHAOS_MR\", +1.0), (\"MMWHS_MR\", +1.0)]\n",
    "    comps[\"CT\"] = [(\"CHAOS_CT\", +1.0), (\"MMWHS_CT\", +1.0)]\n",
    "    # Cross-domain arithmetic composites (Part 2 in local.ipynb)\n",
    "    comps[\"MMWHS_CT_cross\"] = [(\"MMWHS_MR\", +1.0), (\"CHAOS_CT\", +1.0), (\"CHAOS_MR\", -1.0)]\n",
    "    comps[\"MMWHS_MR_cross\"] = [(\"MMWHS_CT\", +1.0), (\"CHAOS_MR\", +1.0), (\"CHAOS_CT\", -1.0)]\n",
    "    comps[\"CHAOS_CT_cross\"] = [(\"CHAOS_MR\", +1.0), (\"MMWHS_CT\", +1.0), (\"MMWHS_MR\", -1.0)]\n",
    "    comps[\"CHAOS_MR_cross\"] = [(\"CHAOS_CT\", +1.0), (\"MMWHS_MR\", +1.0), (\"MMWHS_CT\", -1.0)]\n",
    "    return comps\n",
    "\n",
    "def combine_from_terms(delta_list: List[TaskVector], terms: List[Tuple[str, float]], scale: float) -> TaskVector:\n",
    "    chosen = [delta_list[_idx(tn)] for tn, _w in terms]\n",
    "    weights = [scale * float(w) for _tn, w in terms]\n",
    "    return weighted_sum(chosen, weights)\n",
    "\n",
    "def tatr_from_terms(delta_list: List[TaskVector], terms: List[Tuple[str, float]], scale: float) -> Tuple[TaskVector, Dict[str, float]]:\n",
    "    chosen = [delta_list[_idx(tn)] for tn, _w in terms]\n",
    "    weights = [scale * float(w) for _tn, w in terms]\n",
    "    return tatr_combine(\n",
    "        chosen,\n",
    "        weights=weights,\n",
    "        trust_radius=CFG.trust_radius,\n",
    "        min_cos_to_each_task=CFG.min_cos_to_each_task,\n",
    "        max_steps=CFG.tatr_max_steps,\n",
    "        shrink=CFG.tatr_shrink,\n",
    "        eps=CFG.eps,\n",
    "    )\n",
    "\n",
    "local_composites = build_local_composites()\n",
    "\n",
    "# Evaluate, for each target task: dataset composite, domain composite, and cross composite\n",
    "records = []\n",
    "for tn, fin_enc in zip(task_names, finetuned_encoders):\n",
    "    dataset_name, domain = parse_task_name(tn)\n",
    "    fin = evaluate_encoder_on_task(fin_enc, tn, max_batches=CFG.max_eval_batches).get(\"mean_dice_fg\")\n",
    "\n",
    "    # (1) dataset composite applied at (dataset, domain) baseline\n",
    "    ds_terms = local_composites[dataset_name]\n",
    "    # (2) domain composite applied at (dataset, domain) baseline\n",
    "    dom_terms = local_composites[domain]\n",
    "    # (3) cross composite for this task\n",
    "    cross_terms = local_composites[f\"{tn}_cross\"]\n",
    "\n",
    "    for comp_kind, terms in [(\"dataset\", ds_terms), (\"domain\", dom_terms), (\"cross\", cross_terms)]:\n",
    "        # Raw addition\n",
    "        d_raw = combine_from_terms(deltas, terms, scale=CFG.addition_alpha)\n",
    "        s_raw = evaluate_baseline_plus_delta(d_raw, tn, max_batches=CFG.max_eval_batches, scale=1.0).get(\"mean_dice_fg\")\n",
    "\n",
    "        # AWD addition (if available)\n",
    "        d_awd = combine_from_terms(awd_deltas, terms, scale=CFG.addition_alpha)\n",
    "        s_awd = evaluate_baseline_plus_delta(d_awd, tn, max_batches=CFG.max_eval_batches, scale=1.0).get(\"mean_dice_fg\")\n",
    "\n",
    "        # TATR addition (trust region on the combined update)\n",
    "        d_tatr, stats = tatr_from_terms(deltas, terms, scale=CFG.addition_alpha)\n",
    "        s_tatr = evaluate_baseline_plus_delta(d_tatr, tn, max_batches=CFG.max_eval_batches, scale=1.0).get(\"mean_dice_fg\")\n",
    "\n",
    "        records.append({\n",
    "            \"task\": tn,\n",
    "            \"composite\": comp_kind,\n",
    "            \"dice_finetuned\": fin,\n",
    "            \"dice_raw_add\": s_raw,\n",
    "            \"dice_AWD_add\": s_awd,\n",
    "            \"dice_TATR_add\": s_tatr,\n",
    "            \"gap_raw_vs_finetuned\": (s_raw - fin) if fin is not None else None,\n",
    "            \"gap_AWD_vs_finetuned\": (s_awd - fin) if fin is not None else None,\n",
    "            \"gap_TATR_vs_finetuned\": (s_tatr - fin) if fin is not None else None,\n",
    "            \"tatr_final_scale\": stats.get(\"final_scale\"),\n",
    "            \"tatr_final_norm\": stats.get(\"final_norm\"),\n",
    "        })\n",
    "\n",
    "add_df = pd.DataFrame(records)\n",
    "display(add_df)\n",
    "\n",
    "# Summary stats (mean over tasks) for each composite kind\n",
    "summary = (\n",
    "    add_df.groupby(\"composite\")[\n",
    "        [\"gap_raw_vs_finetuned\", \"gap_AWD_vs_finetuned\", \"gap_TATR_vs_finetuned\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .rename(columns={\n",
    "        \"gap_raw_vs_finetuned\": \"mean_gap_raw\",\n",
    "        \"gap_AWD_vs_finetuned\": \"mean_gap_AWD\",\n",
    "        \"gap_TATR_vs_finetuned\": \"mean_gap_TATR\",\n",
    "    })\n",
    ")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e506a22",
   "metadata": {},
   "source": [
    "## 8) Summary statistics (vs finetuned)\n",
    "This condenses the key comparisons you asked for into small tables: AWD-only, TATR-only, and task-addition composites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a99a8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_gap_vs_finetuned</th>\n",
       "      <th>mean_abs_gap_vs_finetuned</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AWD (per-task edit)</th>\n",
       "      <td>-0.005516</td>\n",
       "      <td>0.005516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TATR (sum all tasks)</th>\n",
       "      <td>-0.848897</td>\n",
       "      <td>0.848897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Addition: raw</th>\n",
       "      <td>-0.605776</td>\n",
       "      <td>0.605776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Addition: AWD</th>\n",
       "      <td>-0.602407</td>\n",
       "      <td>0.602407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Addition: TATR</th>\n",
       "      <td>-0.890374</td>\n",
       "      <td>0.890374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mean_gap_vs_finetuned  mean_abs_gap_vs_finetuned\n",
       "experiment                                                            \n",
       "AWD (per-task edit)               -0.005516                   0.005516\n",
       "TATR (sum all tasks)              -0.848897                   0.848897\n",
       "Addition: raw                     -0.605776                   0.605776\n",
       "Addition: AWD                     -0.602407                   0.602407\n",
       "Addition: TATR                    -0.890374                   0.890374"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>composite</th>\n",
       "      <th>best_method</th>\n",
       "      <th>best_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHAOS_CT</td>\n",
       "      <td>dataset</td>\n",
       "      <td>gap_raw_vs_finetuned</td>\n",
       "      <td>-0.123489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHAOS_CT</td>\n",
       "      <td>domain</td>\n",
       "      <td>gap_AWD_vs_finetuned</td>\n",
       "      <td>-0.215112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHAOS_CT</td>\n",
       "      <td>cross</td>\n",
       "      <td>gap_AWD_vs_finetuned</td>\n",
       "      <td>-0.971462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHAOS_MR</td>\n",
       "      <td>dataset</td>\n",
       "      <td>gap_raw_vs_finetuned</td>\n",
       "      <td>-0.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHAOS_MR</td>\n",
       "      <td>domain</td>\n",
       "      <td>gap_AWD_vs_finetuned</td>\n",
       "      <td>-0.658287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CHAOS_MR</td>\n",
       "      <td>cross</td>\n",
       "      <td>gap_AWD_vs_finetuned</td>\n",
       "      <td>-0.821658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MMWHS_CT</td>\n",
       "      <td>dataset</td>\n",
       "      <td>gap_AWD_vs_finetuned</td>\n",
       "      <td>-0.445427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MMWHS_CT</td>\n",
       "      <td>domain</td>\n",
       "      <td>gap_AWD_vs_finetuned</td>\n",
       "      <td>-0.444765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MMWHS_CT</td>\n",
       "      <td>cross</td>\n",
       "      <td>gap_raw_vs_finetuned</td>\n",
       "      <td>-0.891047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MMWHS_MR</td>\n",
       "      <td>dataset</td>\n",
       "      <td>gap_AWD_vs_finetuned</td>\n",
       "      <td>-0.736485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MMWHS_MR</td>\n",
       "      <td>domain</td>\n",
       "      <td>gap_raw_vs_finetuned</td>\n",
       "      <td>-0.484620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MMWHS_MR</td>\n",
       "      <td>cross</td>\n",
       "      <td>gap_raw_vs_finetuned</td>\n",
       "      <td>-0.801111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        task composite           best_method  best_gap\n",
       "0   CHAOS_CT   dataset  gap_raw_vs_finetuned -0.123489\n",
       "1   CHAOS_CT    domain  gap_AWD_vs_finetuned -0.215112\n",
       "2   CHAOS_CT     cross  gap_AWD_vs_finetuned -0.971462\n",
       "3   CHAOS_MR   dataset  gap_raw_vs_finetuned -0.606900\n",
       "4   CHAOS_MR    domain  gap_AWD_vs_finetuned -0.658287\n",
       "5   CHAOS_MR     cross  gap_AWD_vs_finetuned -0.821658\n",
       "6   MMWHS_CT   dataset  gap_AWD_vs_finetuned -0.445427\n",
       "7   MMWHS_CT    domain  gap_AWD_vs_finetuned -0.444765\n",
       "8   MMWHS_CT     cross  gap_raw_vs_finetuned -0.891047\n",
       "9   MMWHS_MR   dataset  gap_AWD_vs_finetuned -0.736485\n",
       "10  MMWHS_MR    domain  gap_raw_vs_finetuned -0.484620\n",
       "11  MMWHS_MR     cross  gap_raw_vs_finetuned -0.801111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _mean(series: pd.Series) -> float:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    return float(s.mean())\n",
    "\n",
    "def _mean_abs(series: pd.Series) -> float:\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    return float(s.abs().mean())\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# AWD-only (per-task edit): compare baseline+AWD_delta vs finetuned\n",
    "if \"awd_df\" in globals():\n",
    "    g = awd_df[\"dice_base_plus_AWD_delta\"] - awd_df[\"dice_finetuned\"]\n",
    "    summary_rows.append({\n",
    "        \"experiment\": \"AWD (per-task edit)\",\n",
    "        \"mean_gap_vs_finetuned\": _mean(g),\n",
    "        \"mean_abs_gap_vs_finetuned\": _mean_abs(g),\n",
    "    })\n",
    "\n",
    "# TATR-only (sum-all test from 6C): baseline+TATR_add vs finetuned\n",
    "if \"tatr_df\" in globals():\n",
    "    g = tatr_df[\"dice_baseline_plus_TATR_add\"] - tatr_df[\"dice_finetuned\"]\n",
    "    summary_rows.append({\n",
    "        \"experiment\": \"TATR (sum all tasks)\",\n",
    "        \"mean_gap_vs_finetuned\": _mean(g),\n",
    "        \"mean_abs_gap_vs_finetuned\": _mean_abs(g),\n",
    "    })\n",
    "\n",
    "# Task addition composites (section 7): compare each method vs finetuned, aggregated over (task, composite) rows\n",
    "if \"add_df\" in globals():\n",
    "    for method_col, name in [\n",
    "        (\"gap_raw_vs_finetuned\", \"Addition: raw\"),\n",
    "        (\"gap_AWD_vs_finetuned\", \"Addition: AWD\"),\n",
    "        (\"gap_TATR_vs_finetuned\", \"Addition: TATR\"),\n",
    "    ]:\n",
    "        summary_rows.append({\n",
    "            \"experiment\": name,\n",
    "            \"mean_gap_vs_finetuned\": _mean(add_df[method_col]),\n",
    "            \"mean_abs_gap_vs_finetuned\": _mean_abs(add_df[method_col]),\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).set_index(\"experiment\")\n",
    "display(summary_df)\n",
    "\n",
    "# Optional: best method per (task, composite) row\n",
    "if \"add_df\" in globals():\n",
    "    best = add_df.copy()\n",
    "    best[\"best_method\"] = best[[\"gap_raw_vs_finetuned\", \"gap_AWD_vs_finetuned\", \"gap_TATR_vs_finetuned\"]].idxmax(axis=1)\n",
    "    best[\"best_gap\"] = best[[\"gap_raw_vs_finetuned\", \"gap_AWD_vs_finetuned\", \"gap_TATR_vs_finetuned\"]].max(axis=1)\n",
    "    display(best[[\"task\", \"composite\", \"best_method\", \"best_gap\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
