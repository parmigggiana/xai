{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93571b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"CHAOS\", \"MMWHS\"]\n",
    "DOMAINS = [\"CT\", \"MR\"]\n",
    "DATA_PATH = \"../data/\"\n",
    "CHECKPOINT_PATH = \"../checkpoints/\"\n",
    "USE_3D = False\n",
    "TRAINING_EPOCHS = {\n",
    "    (\"CHAOS\", \"CT\"): 1,\n",
    "}\n",
    "BATCH_SIZE = 64\n",
    "SPATIAL_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# Set True to enable debug prints/timers/visualizations)\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe7c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_MAX_ITEMS = 32  # set the in-memory file cache size per dataset (images and segs)\n",
    "ENABLE_CACHE = True  # set to False to disable caching entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(CHECKPOINT_PATH)\n",
    "DATA_PATH = Path(DATA_PATH)\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if USE_3D:\n",
    "    encoder_type = \"swin_unetr\"\n",
    "else:\n",
    "    encoder_type = \"clipseg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai import transforms\n",
    "\n",
    "\n",
    "# Normalization stats (mean, std) per dataset/domain\n",
    "NORM_STATS = {\n",
    "    (\"MMWHS\", \"MR\"): (186.5875, 258.5917),\n",
    "    (\"MMWHS\", \"CT\"): (-745.0086, 1042.7251),\n",
    "    (\"CHAOS\", \"MR\"): (90.8292, 168.8922),\n",
    "    (\"CHAOS\", \"CT\"): (-478.1732, 476.7163),\n",
    "}\n",
    "\n",
    "# Optimized preprocessing: resize early\n",
    "\n",
    "\n",
    "def get_preprocessing(dataset_name: str, domain: str, is_training=True):\n",
    "    decode_func = get_decode_func(dataset_name, domain)\n",
    "    mean_std = NORM_STATS.get((dataset_name, domain))\n",
    "    mean, std = mean_std if mean_std is not None else (None, None)\n",
    "\n",
    "    # Image-specific transforms\n",
    "    if USE_3D:\n",
    "        image_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "    else:\n",
    "        image_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "\n",
    "    # Resize early to reduce compute\n",
    "    image_transforms.append(\n",
    "        transforms.Resize(\n",
    "            spatial_size=SPATIAL_SIZE,\n",
    "            size_mode=\"longest\",\n",
    "            mode=\"area\",\n",
    "            anti_aliasing=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert to tensor and ensure float32 for stable CPU ops\n",
    "    image_transforms.extend(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Augmentations (training only) ‚Äî run in float32 on CPU\n",
    "    if is_training:\n",
    "        image_transforms.extend(\n",
    "            [\n",
    "                transforms.RandGaussianNoise(prob=0.15, std=0.05),\n",
    "                transforms.RandAdjustContrast(prob=0.15, gamma=(0.95, 1.05)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Normalize (still in float32)\n",
    "    if mean is not None and std is not None:\n",
    "        image_transforms.append(\n",
    "            transforms.NormalizeIntensity(\n",
    "                subtrahend=float(mean),\n",
    "                divisor=float(std),\n",
    "                channel_wise=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Repeat to 3 channels only at the end (2D only)\n",
    "    if not USE_3D:\n",
    "        image_transforms.append(transforms.RepeatChannel(repeats=3))\n",
    "\n",
    "    image_transform = transforms.Compose(image_transforms)\n",
    "\n",
    "    # Segmentation transforms\n",
    "    if not USE_3D:\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "    else:\n",
    "        seg_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "\n",
    "    seg_transforms.extend(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "            transforms.Lambda(\n",
    "                lambda x: decode_func(x)\n",
    "            ),  # decode after tensor conversion\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE, size_mode=\"longest\", mode=\"nearest\"\n",
    "            ),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    seg_transform = transforms.Compose(seg_transforms)\n",
    "    return image_transform, seg_transform\n",
    "\n",
    "\n",
    "def get_decode_func(dataset_name, domain):\n",
    "    from src.datasets.mmwhs import mmwhs_labels\n",
    "\n",
    "    decode = None\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        if domain in [\"MR\", \"MRI\"]:\n",
    "\n",
    "            def decode(labels):\n",
    "                # Convert intensity values to class indices (keep as float32)\n",
    "                return labels // 63\n",
    "\n",
    "        elif domain == \"CT\":\n",
    "\n",
    "            def decode(labels):\n",
    "                return torch.where(labels > 0, 1.0, 0.0)\n",
    "\n",
    "    elif dataset_name == \"MMWHS\":\n",
    "\n",
    "        def decode(labels):\n",
    "            decoded_labels = torch.zeros_like(labels, dtype=torch.float32)\n",
    "            for i, label_val in enumerate(mmwhs_labels.keys()):\n",
    "                decoded_labels[labels == label_val] = i\n",
    "            return decoded_labels\n",
    "\n",
    "    if decode is None:\n",
    "\n",
    "        def decode(labels):\n",
    "            return labels\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee8505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad_status(model):\n",
    "    trainable = []\n",
    "    frozen = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            trainable.append(name)\n",
    "        else:\n",
    "            frozen.append(name)\n",
    "    print(f\"\\n=== Parameters requiring grad: {len(trainable)} ===\")\n",
    "    for n in trainable:\n",
    "    print(f\"  + {n}\")\n",
    "    print(f\"\\n=== Parameters frozen (no grad): {len(frozen)}===\")\n",
    "    for n in frozen:\n",
    "        print(f\"  - {n}\")\n",
    "\n",
    "    return trainable, frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f132bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_grad_status(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(not p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee31296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to snapshot parameters and report unchanged ones after finetuning\n",
    "import torch\n",
    "\n",
    "\n",
    "def snapshot_parameters(model):\n",
    "    \"\"\"Return a CPU snapshot of all parameter tensors by name.\"\"\"\n",
    "    return {name: p.detach().clone().cpu() for name, p in model.named_parameters()}\n",
    "\n",
    "\n",
    "def print_unchanged_parameters(model, snapshot, only_requires_grad: bool = True):\n",
    "    \"\"\"Print all parameter names whose values are identical to the snapshot.\n",
    "\n",
    "    Args:\n",
    "        model: The model to compare.\n",
    "        snapshot: Dict[name -> Tensor] with the pre-finetune parameter values.\n",
    "        only_requires_grad: If True, only consider parameters with requires_grad=True.\n",
    "    \"\"\"\n",
    "    unchanged = []\n",
    "    considered = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        if only_requires_grad and not p.requires_grad:\n",
    "            continue\n",
    "        prev = snapshot.get(name)\n",
    "        if prev is None:\n",
    "            continue\n",
    "        curr = p.detach().cpu()\n",
    "        considered += 1\n",
    "        if torch.equal(curr, prev):\n",
    "            unchanged.append(name)\n",
    "\n",
    "    print(\"\\n=== Parameters unchanged after finetuning ===\")\n",
    "    for n in unchanged:\n",
    "        print(f\"  - {n}\")\n",
    "    print(f\"Total unchanged: {len(unchanged)} of {considered} considered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning on CHAOS dataset in CT domain with 2d images \n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "üîÑ Loading CLIPSeg weights...\n",
      "model state keys: 508\n",
      "checkpoint state keys: 54\n",
      "CLIPSeg weight load summary: matched=54, missing=454, unexpected=0\n",
      "Missing keys (454):\n",
      "  - clip_model.positional_embedding\n",
      "  - clip_model.text_projection\n",
      "  - clip_model.logit_scale\n",
      "  - clip_model.visual.class_embedding\n",
      "  - clip_model.visual.positional_embedding\n",
      "  - clip_model.visual.proj\n",
      "  - clip_model.visual.conv1.weight\n",
      "  - clip_model.visual.ln_pre.weight\n",
      "  - clip_model.visual.ln_pre.bias\n",
      "  - clip_model.visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "  - clip_model.visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "  - clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "  - clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "  - clip_model.visual.transformer.resblocks.0.ln_1.weight\n",
      "  - clip_model.visual.transformer.resblocks.0.ln_1.bias\n",
      "  - clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "  - clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "  - clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "  - clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "  - clip_model.visual.transformer.resblocks.0.ln_2.weight\n",
      "  - clip_model.visual.transformer.resblocks.0.ln_2.bias\n",
      "  - clip_model.visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "  - clip_model.visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "  - clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "  - clip_model.visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "  ... and 429 more\n",
      "\n",
      "=== Parameters requiring grad: 206 ===\n",
      "\n",
      "=== Parameters frozen (no grad): 150===\n",
      "üöÄ Starting training for 1 epochs\n",
      "   Device: cpu\n",
      "   Learning Rate: 0.001\n",
      "   Weight Decay: 1e-05\n",
      "   Trainable params: 206 of 356\n",
      "   Params: total=150,796,962, trainable=87,368,865\n",
      "   Batches: train=31, val=7\n",
      "   Tracking params:\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "     - encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "\n",
      "üìñ Epoch 1/1\n",
      "   LR(s): 1.000000e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|‚ñé         | 1/31 [00:04<02:08,  4.27s/it, Loss=1.1433]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 0: 2.125s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|‚ñã         | 2/31 [00:07<01:38,  3.40s/it, Loss=1.1377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 1: 1.771s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|‚ñâ         | 3/31 [00:09<01:26,  3.09s/it, Loss=1.1396]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 2: 1.818s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|‚ñà‚ñé        | 4/31 [00:12<01:20,  2.99s/it, Loss=1.1294]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 3: 1.822s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|‚ñà‚ñå        | 5/31 [00:15<01:15,  2.89s/it, Loss=1.0921]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 4: 1.823s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|‚ñà‚ñâ        | 6/31 [00:18<01:17,  3.10s/it, Loss=1.0400]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 5: 2.412s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|‚ñà‚ñà‚ñé       | 7/31 [00:22<01:15,  3.17s/it, Loss=0.9836]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 6: 1.883s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|‚ñà‚ñà‚ñå       | 8/31 [00:25<01:12,  3.14s/it, Loss=0.9495]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 7: 2.007s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|‚ñà‚ñà‚ñâ       | 9/31 [00:28<01:09,  3.17s/it, Loss=0.9240]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 8: 2.054s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 10/31 [00:32<01:12,  3.47s/it, Loss=0.9158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 9: 1.987s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|‚ñà‚ñà‚ñà‚ñå      | 11/31 [00:43<01:53,  5.65s/it, Loss=0.9050]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 10: 9.505s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|‚ñà‚ñà‚ñà‚ñä      | 12/31 [01:02<03:06,  9.84s/it, Loss=0.8800]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 11: 12.412s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 13/31 [01:23<03:56, 13.12s/it, Loss=0.8630]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 12: 11.929s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 14/31 [01:31<03:18, 11.67s/it, Loss=0.8503]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 13: 1.759s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 15/31 [01:34<02:24,  9.03s/it, Loss=0.8348]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 14: 1.830s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 16/31 [01:37<01:47,  7.18s/it, Loss=0.8226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 15: 1.837s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 17/31 [01:40<01:22,  5.90s/it, Loss=0.8340]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 16: 1.915s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 18/31 [01:43<01:04,  4.99s/it, Loss=0.8480]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 17: 1.947s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 19/31 [01:45<00:51,  4.32s/it, Loss=0.8465]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 18: 1.744s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 20/31 [01:48<00:41,  3.79s/it, Loss=0.8151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 19: 1.751s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 21/31 [01:51<00:34,  3.47s/it, Loss=0.8063]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 20: 1.845s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 22/31 [01:53<00:28,  3.21s/it, Loss=0.8121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 21: 1.819s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 23/31 [01:56<00:25,  3.14s/it, Loss=0.8132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 22: 1.889s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 24/31 [01:59<00:21,  3.06s/it, Loss=0.8071]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 23: 1.870s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 25/31 [02:02<00:18,  3.10s/it, Loss=0.7884]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 24: 2.011s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 26/31 [02:06<00:15,  3.14s/it, Loss=0.7906]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 25: 1.918s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 27/31 [02:08<00:12,  3.02s/it, Loss=0.7826]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 26: 1.847s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 28/31 [02:11<00:08,  2.98s/it, Loss=0.8218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 27: 1.895s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 29/31 [02:15<00:06,  3.06s/it, Loss=0.8654]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 28: 2.267s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 30/31 [02:18<00:03,  3.24s/it, Loss=0.8768]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 29: 2.562s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [02:22<00:00,  4.59s/it, Loss=0.8418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[timer] batch 30: 2.519s\n",
      "Epoch 1 - Train Loss: 0.8909\n",
      "   Param norm deltas (after epoch):\n",
      "     encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.weight: Œînorm=+1.144306e+00 (before=8.586586e+00, after=9.730892e+00)\n",
      "     encoder.clipseg.clip_model.visual.transformer.resblocks.0.attn.out_proj.bias: Œînorm=+9.947181e-03 (before=1.183554e+00, after=1.193501e+00)\n",
      "     encoder.clipseg.clip_model.visual.transformer.resblocks.1.attn.out_proj.weight: Œînorm=+1.061639e+00 (before=8.633878e+00, after=9.695517e+00)\n",
      "   Grad non-zero in batches: 31/31 (100.0%)\n",
      "   Labels seen this epoch: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  14%|‚ñà‚ñç        | 1/7 [00:02<00:17,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Loss: 0.7672\n",
      "   New best Val Loss: 0.7672\n",
      "\n",
      "‚úÖ Training completed!\n",
      "\n",
      "=== Parameters unchanged after finetuning ===\n",
      "  - encoder.clipseg.clip_model.visual.proj\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_1.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_1.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_2.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.10.ln_2.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_1.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_1.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_2.weight\n",
      "  - encoder.clipseg.clip_model.visual.transformer.resblocks.11.ln_2.bias\n",
      "  - encoder.clipseg.clip_model.visual.ln_post.weight\n",
      "  - encoder.clipseg.clip_model.visual.ln_post.bias\n",
      "  - encoder.clipseg.reduce.weight\n",
      "  - encoder.clipseg.reduce.bias\n",
      "Total unchanged: 29 of 206 considered\n"
     ]
    }
   ],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in TRAINING_EPOCHS.items():\n",
    "    download_and_extract_dataset(dataset_name, DATA_PATH)\n",
    "\n",
    "    image_transform, seg_transform = get_preprocessing(\n",
    "        dataset_name, domain, is_training=True\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        transform=image_transform,  # Use transform instead of preprocess\n",
    "        seg_transform=seg_transform,  # Pass seg_transform too\n",
    "        base_path=DATA_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=0,\n",
    "        slice_2d=not USE_3D,\n",
    "        # new cache knobs\n",
    "        cache_max_items=CACHE_MAX_ITEMS,\n",
    "        enable_cache=ENABLE_CACHE,\n",
    "    )\n",
    "\n",
    "    #  Ensure the dataset is loaded correctly\n",
    "    if not isinstance(dataset, BaseDataset):\n",
    "        raise TypeError(\n",
    "            f\"Expected dataset to be an instance of BaseDataset, got {type(dataset)}\"\n",
    "        )\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=encoder_type,\n",
    "    )\n",
    "\n",
    "    # Train Only Segmentation Head\n",
    "    # pass\n",
    "\n",
    "    # Train Visual Encoder + Segmentation head\n",
    "    # model.unfreeze()\n",
    "    # model.freeze_text_encoder()\n",
    "\n",
    "    # Train Only Visual Encoder\n",
    "    for p in model.encoder.clipseg.model.parameters():\n",
    "        p.requires_grad_(not p.requires_grad)\n",
    "    model.freeze_text_encoder()\n",
    "\n",
    "    # Print grad status before finetuning (after final freeze mask)\n",
    "    print_grad_status(model)\n",
    "\n",
    "    # Snapshot parameters before finetuning (after setting requires_grad correctly)\n",
    "    before_snapshot = snapshot_parameters(model)\n",
    "\n",
    "    history = model.finetune(\n",
    "        epochs=1,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        val_max_batches=1,\n",
    "        fast_val_metrics=True,\n",
    "        debug=DEBUG,\n",
    "    )\n",
    "\n",
    "    # Print parameters that didn't change after finetuning (only those requiring grad)\n",
    "    print_unchanged_parameters(model, before_snapshot, only_requires_grad=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
