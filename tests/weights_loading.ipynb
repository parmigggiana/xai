{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5de70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79dc085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.registry import get_dataset\n",
    "from src.datasets.common import BaseDataset\n",
    "from pathlib import Path\n",
    "from src.utils import download_and_extract_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cba54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"CHAOS\", \"MMWHS\"]\n",
    "DOMAINS = [\"CT\", \"MR\"]\n",
    "DATA_PATH = \"../data/\"\n",
    "CHECKPOINT_PATH = \"../checkpoints/\"\n",
    "USE_3D = False\n",
    "TRAINING_EPOCHS = {\n",
    "    (\"CHAOS\", \"CT\"): 1,\n",
    "}\n",
    "BATCH_SIZE = 64\n",
    "SPATIAL_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# Set True to enable debug prints/timers/visualizations)\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe7c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_MAX_ITEMS = 32  # set the in-memory file cache size per dataset (images and segs)\n",
    "ENABLE_CACHE = True  # set to False to disable caching entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0801709",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(CHECKPOINT_PATH)\n",
    "DATA_PATH = Path(DATA_PATH)\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if USE_3D:\n",
    "    encoder_type = \"swin_unetr\"\n",
    "else:\n",
    "    encoder_type = \"clipseg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb09eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai import transforms\n",
    "\n",
    "\n",
    "# Normalization stats (mean, std) per dataset/domain\n",
    "NORM_STATS = {\n",
    "    (\"MMWHS\", \"MR\"): (186.5875, 258.5917),\n",
    "    (\"MMWHS\", \"CT\"): (-745.0086, 1042.7251),\n",
    "    (\"CHAOS\", \"MR\"): (90.8292, 168.8922),\n",
    "    (\"CHAOS\", \"CT\"): (-478.1732, 476.7163),\n",
    "}\n",
    "\n",
    "# Optimized preprocessing: resize early\n",
    "\n",
    "\n",
    "def get_preprocessing(dataset_name: str, domain: str, is_training=True):\n",
    "    decode_func = get_decode_func(dataset_name, domain)\n",
    "    mean_std = NORM_STATS.get((dataset_name, domain))\n",
    "    mean, std = mean_std if mean_std is not None else (None, None)\n",
    "\n",
    "    # Image-specific transforms\n",
    "    if USE_3D:\n",
    "        image_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "    else:\n",
    "        image_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "\n",
    "    # Resize early to reduce compute\n",
    "    image_transforms.append(\n",
    "        transforms.Resize(\n",
    "            spatial_size=SPATIAL_SIZE,\n",
    "            size_mode=\"longest\",\n",
    "            mode=\"area\",\n",
    "            anti_aliasing=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert to tensor and ensure float32 for stable CPU ops\n",
    "    image_transforms.extend(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Augmentations (training only) â€” run in float32 on CPU\n",
    "    if is_training:\n",
    "        image_transforms.extend(\n",
    "            [\n",
    "                transforms.RandGaussianNoise(prob=0.15, std=0.05),\n",
    "                transforms.RandAdjustContrast(prob=0.15, gamma=(0.95, 1.05)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Normalize (still in float32)\n",
    "    if mean is not None and std is not None:\n",
    "        image_transforms.append(\n",
    "            transforms.NormalizeIntensity(\n",
    "                subtrahend=float(mean),\n",
    "                divisor=float(std),\n",
    "                channel_wise=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Repeat to 3 channels only at the end (2D only)\n",
    "    if not USE_3D:\n",
    "        image_transforms.append(transforms.RepeatChannel(repeats=3))\n",
    "\n",
    "    image_transform = transforms.Compose(image_transforms)\n",
    "\n",
    "    # Segmentation transforms\n",
    "    if not USE_3D:\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "    else:\n",
    "        seg_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "\n",
    "    seg_transforms.extend(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "            transforms.Lambda(\n",
    "                lambda x: decode_func(x)\n",
    "            ),  # decode after tensor conversion\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE, size_mode=\"longest\", mode=\"nearest\"\n",
    "            ),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    seg_transform = transforms.Compose(seg_transforms)\n",
    "    return image_transform, seg_transform\n",
    "\n",
    "\n",
    "def get_decode_func(dataset_name, domain):\n",
    "    from src.datasets.mmwhs import mmwhs_labels\n",
    "\n",
    "    decode = None\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        if domain in [\"MR\", \"MRI\"]:\n",
    "\n",
    "            def decode(labels):\n",
    "                # Convert intensity values to class indices (keep as float32)\n",
    "                return labels // 63\n",
    "\n",
    "        elif domain == \"CT\":\n",
    "\n",
    "            def decode(labels):\n",
    "                return torch.where(labels > 0, 1.0, 0.0)\n",
    "\n",
    "    elif dataset_name == \"MMWHS\":\n",
    "\n",
    "        def decode(labels):\n",
    "            decoded_labels = torch.zeros_like(labels, dtype=torch.float32)\n",
    "            for i, label_val in enumerate(mmwhs_labels.keys()):\n",
    "                decoded_labels[labels == label_val] = i\n",
    "            return decoded_labels\n",
    "\n",
    "    if decode is None:\n",
    "\n",
    "        def decode(labels):\n",
    "            return labels\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c6dc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning on CHAOS dataset in CT domain with 2d images \n",
      "Found explicit background class in input. Treating it separately.\n",
      "Non-background classes: ['Liver']\n",
      "ðŸ”„ Loading CLIPSeg weights...\n",
      "model state keys: 508\n",
      "checkpoint state keys: 54\n",
      "CLIPSeg weight load summary: matched=54, missing=454, unexpected=0\n",
      "Missing keys (454):\n",
      "  - clip_model.positional_embedding\n",
      "  - clip_model.text_projection\n",
      "  - clip_model.logit_scale\n",
      "  - clip_model.visual.class_embedding\n",
      "  - clip_model.visual.positional_embedding\n",
      "  - clip_model.visual.proj\n",
      "  - clip_model.visual.conv1.weight\n",
      "  - clip_model.visual.ln_pre.weight\n",
      "  - clip_model.visual.ln_pre.bias\n",
      "  - clip_model.visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "  - clip_model.visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "  - clip_model.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "  - clip_model.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "  - clip_model.visual.transformer.resblocks.0.ln_1.weight\n",
      "  - clip_model.visual.transformer.resblocks.0.ln_1.bias\n",
      "  - clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "  - clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "  - clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "  - clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "  - clip_model.visual.transformer.resblocks.0.ln_2.weight\n",
      "  - clip_model.visual.transformer.resblocks.0.ln_2.bias\n",
      "  - clip_model.visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "  - clip_model.visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "  - clip_model.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "  - clip_model.visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "  ... and 429 more\n"
     ]
    }
   ],
   "source": [
    "# Finetuning loop\n",
    "\n",
    "for (dataset_name, domain), epochs in TRAINING_EPOCHS.items():\n",
    "    download_and_extract_dataset(dataset_name, DATA_PATH)\n",
    "\n",
    "    image_transform, seg_transform = get_preprocessing(\n",
    "        dataset_name, domain, is_training=True\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Finetuning on {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images \"\n",
    "    )\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        domain=domain,\n",
    "        transform=image_transform,  # Use transform instead of preprocess\n",
    "        seg_transform=seg_transform,  # Pass seg_transform too\n",
    "        base_path=DATA_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=0,\n",
    "        slice_2d=not USE_3D,\n",
    "        # new cache knobs\n",
    "        cache_max_items=CACHE_MAX_ITEMS,\n",
    "        enable_cache=ENABLE_CACHE,\n",
    "    )\n",
    "\n",
    "    #  Ensure the dataset is loaded correctly\n",
    "    if not isinstance(dataset, BaseDataset):\n",
    "        raise TypeError(\n",
    "            f\"Expected dataset to be an instance of BaseDataset, got {type(dataset)}\"\n",
    "        )\n",
    "\n",
    "    model = dataset.get_model(\n",
    "        encoder_type=encoder_type,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
