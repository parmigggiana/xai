{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb29726d",
   "metadata": {},
   "source": [
    "# Visualization Function Testing Notebook\n",
    "\n",
    "This notebook tests the visualization functions `visualize_sample_slice` with configurable dataset and domain settings. It focuses on ground truth visualization and skips 3D visualization as requested.\n",
    "\n",
    "## Features:\n",
    "- Tests CHAOS and MMWHS datasets\n",
    "- Supports CT and MR domains  \n",
    "- Configurable visualization parameters\n",
    "- Ground truth visualization testing\n",
    "- Error handling and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import torch and torchvision\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from src.datasets.registry import get_dataset\n",
    "from monai import transforms\n",
    "from monai.data import MetaTensor\n",
    "\n",
    "# Patch per retro-compatibilit√†: PyTorch < 2.6 non ha safe_globals (inutile?)\n",
    "# if not hasattr(torch.serialization, \"safe_globals\"):\n",
    "#    torch.serialization.safe_globals = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle\n",
    "import os\n",
    "\n",
    "IN_KAGGLE = False\n",
    "if os.environ.get(\"KAGGLE_URL_BASE\", \"\"):\n",
    "    IN_KAGGLE = True\n",
    "    !git clone https://github.com/parmigggiana/xai /kaggle/working/xai\n",
    "    %cd xai\n",
    "    !git fetch\n",
    "    !git reset --hard origin/main\n",
    "    %pip install 'monai[einops,itk,nibabel]>=1.5.0' git+https://github.com/timojl/clipseg.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f9166",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Change these constants to test different datasets and domains:\n",
    "- **DATASET_NAME**: \"CHAOS\" or \"MMWHS\"\n",
    "- **DOMAIN**: \"CT\" or \"MR\" \n",
    "- **ENCODER_TYPE**: \"resnet\" or \"swin_unetr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e714f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration constants\n",
    "DATASET_NAME = \"CHAOS\"  # Change to \"MMWHS\" if needed\n",
    "DOMAIN = \"MR\"  # Change to \"CT\" if needed\n",
    "ENCODER_TYPE = \"clipseg\"  # Changed from \"swin_unetr\" to \"clipseg\"\n",
    "BATCH_SIZE = 1\n",
    "NUM_WORKERS = 1\n",
    "USE_3D = False  # Set to True for 3D data processing\n",
    "SPATIAL_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efea59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing(dataset_name: str, domain: str, is_training=True):\n",
    "    \"\"\"\n",
    "    Build MONAI-native Compose transform pipelines for images and segmentations.\n",
    "    ImageDataset will wrap arrays as MetaTensor so metadata flows automatically.\n",
    "    \"\"\"\n",
    "    decode_func = get_decode_func(dataset_name, domain)\n",
    "\n",
    "    if USE_3D:\n",
    "        image_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            # transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "    else:\n",
    "        image_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "\n",
    "    if domain == \"CT\":\n",
    "        image_transforms.append(\n",
    "            transforms.ScaleIntensityRange(\n",
    "                a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        image_transforms.append(\n",
    "            transforms.NormalizeIntensity(nonzero=True, channel_wise=True)\n",
    "        )\n",
    "\n",
    "    if is_training:\n",
    "        image_transforms.extend(\n",
    "            [\n",
    "                transforms.RandGaussianNoise(prob=0.2, std=0.05),\n",
    "                transforms.RandAdjustContrast(prob=0.2, gamma=(0.9, 1.1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if not USE_3D:\n",
    "        image_transforms.append(transforms.RepeatChannel(repeats=3))\n",
    "\n",
    "    image_transforms.extend(\n",
    "        [\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE,\n",
    "                size_mode=\"longest\",\n",
    "                mode=\"area\",\n",
    "                anti_aliasing=True,\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if not USE_3D:\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "    else:\n",
    "        seg_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            # transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "\n",
    "    seg_transforms.extend(\n",
    "        [\n",
    "            transforms.Lambda(lambda x: decode_func(x)),\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE, size_mode=\"longest\", mode=\"nearest\"\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transforms.Compose(image_transforms), transforms.Compose(seg_transforms)\n",
    "\n",
    "\n",
    "def get_decode_func(dataset_name, domain):\n",
    "    from src.datasets.mmwhs import mmwhs_labels\n",
    "\n",
    "    decode = None\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        if domain in [\"MR\", \"MRI\"]:\n",
    "\n",
    "            def decode(labels):\n",
    "                return labels\n",
    "\n",
    "        elif domain == \"CT\":\n",
    "\n",
    "            def decode(labels):\n",
    "                return torch.where(labels > 0, 255.0, 0.0)\n",
    "\n",
    "    elif dataset_name == \"MMWHS\":\n",
    "\n",
    "        def decode(labels):\n",
    "            decoded_labels = torch.zeros_like(labels, dtype=torch.float32)\n",
    "            for i, label_val in enumerate(mmwhs_labels.keys()):\n",
    "                decoded_labels[labels == label_val] = i\n",
    "            return decoded_labels\n",
    "\n",
    "    if decode is None:\n",
    "        print(\n",
    "            f\"Warning: No decode function defined for {dataset_name} in {domain}. Returning labels unchanged.\"\n",
    "        )\n",
    "\n",
    "        def decode(labels):\n",
    "            return labels\n",
    "\n",
    "    return decode\n",
    "\n",
    "\n",
    "def metatensor_batch_to_dict(batch):\n",
    "    \"\"\"\n",
    "    Converts a batch from your dataset (list of MetaTensor) into a dict with 'image' and 'label'.\n",
    "    Assumes:\n",
    "    - batch is a list of length batch_size, each element is a MetaTensor or (MetaTensor, metadata)\n",
    "    - label is stored in metadata as 'label' key or as a separate tensor\n",
    "    \"\"\"\n",
    "    if len(batch) >= 2:\n",
    "        # If batch contains separate image and label tensors\n",
    "        image_data = batch[0]\n",
    "        label_data = batch[1]\n",
    "\n",
    "        # Extract image\n",
    "        if isinstance(image_data, tuple):\n",
    "            image, _ = image_data\n",
    "        else:\n",
    "            image = image_data\n",
    "\n",
    "        # Extract label\n",
    "        if isinstance(label_data, tuple):\n",
    "            label, _ = label_data\n",
    "        else:\n",
    "            label = label_data\n",
    "\n",
    "        # Convert to tensors if needed\n",
    "        image = (\n",
    "            image.data.float()\n",
    "            if hasattr(image, \"data\")\n",
    "            else torch.tensor(image, dtype=torch.float32)\n",
    "        )\n",
    "        label = (\n",
    "            label.data.float()\n",
    "            if hasattr(label, \"data\")\n",
    "            else torch.tensor(label, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Fallback to original logic if only one element\n",
    "        sample = batch[0]\n",
    "        if isinstance(sample, tuple):\n",
    "            data, metadata = sample\n",
    "        else:\n",
    "            data = sample\n",
    "            metadata = getattr(sample, \"meta\", {}) or {}\n",
    "\n",
    "        # Extract image and label\n",
    "        image = (\n",
    "            data.data.float()\n",
    "            if hasattr(data, \"data\")\n",
    "            else torch.tensor(data, dtype=torch.float32)\n",
    "        )\n",
    "        label = metadata.get(\"label\", None)\n",
    "\n",
    "    return {\"image\": image, \"label\": label}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17164a2",
   "metadata": {},
   "source": [
    "## Ground Truth Visualization Testing\n",
    "\n",
    "This section loads the dataset and tests the `visualize_sample_slice` function with ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29855ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.common import BaseDataset\n",
    "\n",
    "\n",
    "def test_ground_truth_visualization():\n",
    "    \"\"\"Test visualization functions with ground truth data.\"\"\"\n",
    "    print(\"üîç Testing Ground Truth Visualization...\")\n",
    "    print(f\"Dataset: {DATASET_NAME}, Domain: {DOMAIN}\")\n",
    "\n",
    "    image_transform, seg_transform = get_preprocessing(\n",
    "        DATASET_NAME, DOMAIN, is_training=False\n",
    "    )\n",
    "    # image_transform = None\n",
    "    # seg_transform = None\n",
    "\n",
    "    # Load dataset\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=DATASET_NAME,\n",
    "        domain=DOMAIN,\n",
    "        transform=image_transform,  # Use transform instead of preprocess\n",
    "        seg_transform=seg_transform,  # Pass seg_transform too\n",
    "        base_path=Path(\"data\"),\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "        slice_2d=True,\n",
    "    )\n",
    "\n",
    "    # Get a sample from the dataset\n",
    "    loader = dataset.train_loader\n",
    "    iterat = iter(loader)\n",
    "    batch = next(iterat)\n",
    "    batch = next(iterat)\n",
    "\n",
    "    # Convert batch to dictionary format\n",
    "    sample_dict = metatensor_batch_to_dict(batch)\n",
    "    print(f\"Image shape: {sample_dict['image'].shape}\")\n",
    "    if sample_dict[\"label\"] is not None:\n",
    "        print(f\"Label shape: {sample_dict['label'].shape}\")\n",
    "    else:\n",
    "        print(\"Label: None\")\n",
    "\n",
    "    # Extract a single sample\n",
    "    # sample = {\n",
    "    #    \"image\": batch[\"image\"],  # Keep batch dimension for inference\n",
    "    #    \"label\": batch[\"label\"],  # Keep batch dimension for inference\n",
    "    # }\n",
    "    return dataset, sample_dict, batch\n",
    "\n",
    "\n",
    "def test_inference_and_visualization(dataset, batch):\n",
    "    \"\"\"Test inference with semantic head training and visualize both GT and prediction.\"\"\"\n",
    "    print(\"üöÄ Testing Inference with Semantic Head Training...\")\n",
    "    print(f\"Dataset: {DATASET_NAME}, Domain: {DOMAIN}\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Get model with semantic head\n",
    "    model = dataset.get_model(ENCODER_TYPE)\n",
    "\n",
    "    # Check what checkpoint files exist\n",
    "    checkpoint_dir = Path(\"checkpoints\")\n",
    "    available_files = list(checkpoint_dir.glob(\"*.pth\"))\n",
    "    print(f\"Available checkpoint files: {[f.name for f in available_files]}\")\n",
    "\n",
    "    # Try different possible checkpoint names\n",
    "    possible_names = [\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_2d_finetuned.pth\",\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_{'3d' if USE_3D else '2d'}_finetuned.pth\",\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_2d_baseline.pth\",\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_{'3d' if USE_3D else '2d'}_baseline.pth\",\n",
    "    ]\n",
    "\n",
    "    checkpoint_path = None\n",
    "    for name in possible_names:\n",
    "        potential_path = checkpoint_dir / name\n",
    "        if potential_path.exists():\n",
    "            checkpoint_path = potential_path\n",
    "            print(f\"Using checkpoint: {checkpoint_path}\")\n",
    "            break\n",
    "\n",
    "    if checkpoint_path is None:\n",
    "        print(\"No suitable checkpoint found. Available files:\")\n",
    "        for f in available_files:\n",
    "            print(f\"  - {f.name}\")\n",
    "        raise FileNotFoundError(\"No suitable checkpoint found\")\n",
    "\n",
    "    # Load the checkpoint\n",
    "    try:\n",
    "        finetuned_state_dict = torch.load(\n",
    "            checkpoint_path, map_location=device, weights_only=True\n",
    "        )\n",
    "        if hasattr(finetuned_state_dict, \"state_dict\"):\n",
    "            finetuned_state_dict = finetuned_state_dict.state_dict()\n",
    "        model.encoder.load_state_dict(finetuned_state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        # Try without weights_only for older checkpoints\n",
    "        finetuned_state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        if hasattr(finetuned_state_dict, \"state_dict\"):\n",
    "            finetuned_state_dict = finetuned_state_dict.state_dict()\n",
    "        model.encoder.load_state_dict(finetuned_state_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    batch[\"image\"] = batch[\"image\"].to(device)\n",
    "\n",
    "    # Run inference\n",
    "    print(\"üîÆ Running inference...\")\n",
    "    outputs = model(batch[\"image\"])\n",
    "    preds = torch.argmax(outputs, dim=1, keepdim=True)\n",
    "\n",
    "    print(f\"Prediction unique values: {torch.unique(preds)}\")\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d, Conv2d, ConvTranspose2d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList, Sequential, ModuleDict\n",
    "from monai.networks.nets.swin_unetr import (\n",
    "    BasicLayer,\n",
    "    SwinTransformerBlock,\n",
    "    WindowAttention,\n",
    "    PatchMerging,\n",
    ")\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.modules.linear import Linear, NonDynamicallyQuantizableLinear, Identity\n",
    "from torch.nn.modules.activation import (\n",
    "    Softmax,\n",
    "    GELU,\n",
    "    LeakyReLU,\n",
    "    ReLU,\n",
    "    MultiheadAttention,\n",
    ")\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock, UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock, UnetOutBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.nn.modules.transformer import (\n",
    "    TransformerEncoderLayer,\n",
    "    TransformerEncoder,\n",
    "    TransformerDecoderLayer,\n",
    "    TransformerDecoder,\n",
    ")\n",
    "\n",
    "# CLIPSeg imports\n",
    "from src.CLIPSeg import CLIPSeg\n",
    "from clipseg.clipseg import CLIPDensePredT\n",
    "from clip.model import (\n",
    "    CLIP,\n",
    "    VisionTransformer,\n",
    "    Transformer,\n",
    "    ResidualAttentionBlock,\n",
    "    QuickGELU,\n",
    ")\n",
    "\n",
    "SAFE_GLOBALS = [\n",
    "    # SwinUNETR components\n",
    "    SwinUNETR,\n",
    "    SwinTransformer,\n",
    "    PatchEmbed,\n",
    "    Conv3d,\n",
    "    Dropout,\n",
    "    ModuleList,\n",
    "    BasicLayer,\n",
    "    SwinTransformerBlock,\n",
    "    LayerNorm,\n",
    "    WindowAttention,\n",
    "    Linear,\n",
    "    Softmax,\n",
    "    Identity,\n",
    "    MLPBlock,\n",
    "    GELU,\n",
    "    PatchMerging,\n",
    "    UnetrBasicBlock,\n",
    "    UnetResBlock,\n",
    "    Convolution,\n",
    "    LeakyReLU,\n",
    "    InstanceNorm3d,\n",
    "    UnetrUpBlock,\n",
    "    UnetOutBlock,\n",
    "    # CLIPSeg components\n",
    "    CLIPSeg,\n",
    "    CLIPDensePredT,\n",
    "    CLIP,\n",
    "    VisionTransformer,\n",
    "    Conv2d,\n",
    "    Sequential,\n",
    "    ResidualAttentionBlock,\n",
    "    MultiheadAttention,\n",
    "    NonDynamicallyQuantizableLinear,\n",
    "    QuickGELU,\n",
    "    Embedding,\n",
    "    ReLU,\n",
    "    ConvTranspose2d,\n",
    "    TransformerEncoderLayer,\n",
    "    TransformerEncoder,\n",
    "    TransformerDecoderLayer,\n",
    "    TransformerDecoder,\n",
    "    LayerNorm,\n",
    "    Transformer,\n",
    "    ModuleDict,\n",
    "]\n",
    "\n",
    "\n",
    "def load_checkpoint_safely(checkpoint_path, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Utility function to safely load PyTorch checkpoints with proper error handling.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path: Path to the checkpoint file\n",
    "        device: Device to load the checkpoint on\n",
    "\n",
    "    Returns:\n",
    "        Loaded checkpoint state dict\n",
    "    \"\"\"\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "    # Handle different checkpoint formats\n",
    "    if isinstance(checkpoint, dict):\n",
    "        if \"model_state_dict\" in checkpoint:\n",
    "            return checkpoint[\"model_state_dict\"]\n",
    "        elif \"state_dict\" in checkpoint:\n",
    "            return checkpoint[\"state_dict\"]\n",
    "        else:\n",
    "            return checkpoint\n",
    "    else:\n",
    "        return (\n",
    "            checkpoint.state_dict() if hasattr(checkpoint, \"state_dict\") else checkpoint\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference_and_visualization_clipseg(dataset, batch):\n",
    "    \"\"\"Test inference with CLIPSeg model using pre-trained weights.\"\"\"\n",
    "    print(\"üöÄ Testing Inference with CLIPSeg...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = dataset.get_model(encoder_type=ENCODER_TYPE)\n",
    "\n",
    "    # Check for fine-tuned checkpoint (optional)\n",
    "    checkpoint_dir = Path(\"checkpoints\")\n",
    "    possible_names = [\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_2d_clipseg_finetuned.pth\",\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_clipseg_finetuned.pth\",\n",
    "        f\"clipseg_{DATASET_NAME}_{DOMAIN}_finetuned.pth\",\n",
    "    ]\n",
    "\n",
    "    checkpoint_path = None\n",
    "    for name in possible_names:\n",
    "        potential_path = checkpoint_dir / name\n",
    "        if potential_path.exists():\n",
    "            checkpoint_path = potential_path\n",
    "            break\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        try:\n",
    "            # Use the safe loading function\n",
    "            state_dict = load_checkpoint_safely(checkpoint_path, device)\n",
    "\n",
    "            # Load fine-tuned state dict\n",
    "            missing_keys, unexpected_keys = model.load_state_dict(\n",
    "                state_dict, strict=False\n",
    "            )\n",
    "            print(f\"‚úÖ Successfully loaded fine-tuned checkpoint\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading fine-tuned checkpoint: {e}\")\n",
    "            print(\"Continuing with pre-trained weights only...\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Handle batch properly - it's a tuple from the DataLoader\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        sample_dict = metatensor_batch_to_dict(batch)\n",
    "        batch_image = (\n",
    "            sample_dict[\"image\"].unsqueeze(0)\n",
    "            if sample_dict[\"image\"].dim() == 3\n",
    "            else sample_dict[\"image\"]\n",
    "        )\n",
    "    else:\n",
    "        # Fallback if batch is already a dict\n",
    "        batch_image = batch[\"image\"]\n",
    "\n",
    "    # Handle tensor shape properly\n",
    "    print(f\"Original batch_image shape: {batch_image.shape}\")\n",
    "\n",
    "    if len(batch_image.shape) == 5:\n",
    "        batch_image = batch_image.squeeze(1)\n",
    "        print(f\"After squeezing dimension 1: {batch_image.shape}\")\n",
    "\n",
    "    batch_image = batch_image.to(device)\n",
    "\n",
    "    # Run inference\n",
    "    print(\"üîÆ Running CLIPSeg inference...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_image)\n",
    "        preds = torch.argmax(outputs, dim=1, keepdim=True)\n",
    "\n",
    "    print(f\"Prediction unique values: {torch.unique(preds)}\")\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2799974",
   "metadata": {},
   "source": [
    "## Dataset Visualization Method Test\n",
    "\n",
    "Test the `dataset.visualize_sample_slice()` method with the loaded data. This section will show both ground truth and prediction visualizations for the **same sample** to enable direct comparison. The dataset-specific implementation automatically applies the correct rotation and flip parameters for each dataset type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecbce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## Dataset Visualization Method Test\n",
    "dataset, sample, batch = test_ground_truth_visualization()\n",
    "\n",
    "# Test dataset.visualize_sample_slice method\n",
    "print(\"\\nüìä Testing dataset.visualize_sample_slice method...\")\n",
    "\n",
    "# Debug: check shapes before processing\n",
    "print(f\"Original image shape: {sample['image'].shape}\")\n",
    "print(f\"Original label shape: {sample['label'].shape}\")\n",
    "\n",
    "\n",
    "# Apply squeeze to remove batch and channel dimensions for visualization\n",
    "sample_viz = {\n",
    "    \"image\": sample[\"image\"].squeeze().cpu().numpy(),\n",
    "    \"label\": sample[\"label\"].squeeze().cpu().numpy(),\n",
    "}\n",
    "\n",
    "print(f\"After squeeze - Image shape: {sample_viz['image'].shape}\")\n",
    "print(f\"After squeeze - Label shape: {sample_viz['label'].shape}\")\n",
    "\n",
    "# üîç DEBUG: Check dimensionality before visualization\n",
    "print(f\"\\nüîç DEBUG - seg_slice dimensionality:\")\n",
    "seg_slice = sample_viz[\"label\"]\n",
    "print(f\"  seg_slice.shape: {seg_slice.shape}\")\n",
    "print(f\"  seg_slice.ndim: {seg_slice.ndim}\")\n",
    "print(f\"  seg_slice type: {type(seg_slice)}\")\n",
    "\n",
    "# Check if it's the right shape for legend generation\n",
    "if seg_slice.ndim >= 2:\n",
    "    print(\"  ‚úÖ Shape is compatible with legend generation\")\n",
    "    unique_vals = np.unique(seg_slice)\n",
    "    print(f\"  Unique values in seg_slice: {unique_vals}\")\n",
    "else:\n",
    "    print(\"  ‚ùå Shape is NOT compatible with legend generation\")\n",
    "    print(\"  Need to fix the shape before visualization\")\n",
    "\n",
    "print(f\"  seg_slice.shape: {seg_slice.shape}\")\n",
    "print(f\"  seg_slice.ndim: {seg_slice.ndim}\")\n",
    "\n",
    "dataset.visualize_sample_slice(sample_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26243048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the SAME sample with CLIPSeg\n",
    "print(\"\\nüîÆ Testing dataset.visualize_sample_slice method with CLIPSeg PREDICTIONS...\")\n",
    "\n",
    "preds = test_inference_and_visualization_clipseg(dataset, batch)\n",
    "\n",
    "\n",
    "print(f\"Applying {DOMAIN} domain encoding...\")\n",
    "encoded_preds = dataset.encode(preds)\n",
    "print(f\"Encoded prediction unique values: {torch.unique(encoded_preds)}\")\n",
    "\n",
    "# Prepare prediction sample for visualization with proper squeeze\n",
    "pred_sample_viz = {\n",
    "    \"image\": sample[\"image\"].squeeze().cpu().numpy(),\n",
    "    \"label\": encoded_preds.squeeze().cpu().numpy(),\n",
    "}\n",
    "\n",
    "\n",
    "# Use the dataset visualization method\n",
    "print(\"\\nüìä Visualizing CLIPSeg predictions...\")\n",
    "# Use the SAME orientation parameters as GT to avoid mismatches\n",
    "dataset.visualize_sample_slice(pred_sample_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28978d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.task_vector import TaskVector\n",
    "\n",
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in [\"MMWHS\", \"CHAOS\"]:\n",
    "    for domain in [\"CT\", \"MR\"]:\n",
    "        baseline_checkpoint = (\n",
    "            Path(\"checkpoints\") / f\"{dataset_name}_{domain}_2d_baseline.pth\"\n",
    "        )\n",
    "        finetuned_checkpoint = (\n",
    "            Path(\"checkpoints\") / f\"{dataset_name}_{domain}_2d_finetuned.pth\"\n",
    "        )\n",
    "        with torch.serialization.safe_globals(safe_globals=SAFE_GLOBALS):\n",
    "            task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "            # Remove keys associated with the .out layer from the task vector\n",
    "            out_layer_keys = [k for k in task_vector.keys() if \"out.\" in k]\n",
    "            for k in out_layer_keys:\n",
    "                del task_vector[k]\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector\n",
    "\n",
    "# Build composite task vectors using arithmetic\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS_CT\": task_vectors[\"MMWHS_MR\"]\n",
    "    + task_vectors[\"CHAOS_CT\"]\n",
    "    - task_vectors[\"CHAOS_MR\"],\n",
    "    \"MMWHS_MR\": task_vectors[\"MMWHS_CT\"]\n",
    "    + task_vectors[\"CHAOS_MR\"]\n",
    "    - task_vectors[\"CHAOS_CT\"],\n",
    "    \"CHAOS_CT\": task_vectors[\"CHAOS_MR\"]\n",
    "    + task_vectors[\"MMWHS_CT\"]\n",
    "    - task_vectors[\"MMWHS_MR\"],\n",
    "    \"CHAOS_MR\": task_vectors[\"CHAOS_CT\"]\n",
    "    + task_vectors[\"MMWHS_MR\"]\n",
    "    - task_vectors[\"MMWHS_CT\"],\n",
    "}\n",
    "\n",
    "# Only add extra_kwarg 'liver_only' for CHAOS dataset\n",
    "extra_kwargs = {}\n",
    "if DATASET_NAME == \"CHAOS\":\n",
    "    extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "composite_key = f\"{DATASET_NAME}_{DOMAIN}\"\n",
    "\n",
    "dataset_kwargs = dict(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    domain=DOMAIN,\n",
    "    base_path=Path(\"data\"),\n",
    "    batch_size=1,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    slice_2d=True,\n",
    ")\n",
    "dataset_kwargs.update(extra_kwargs)\n",
    "target_dataset = get_dataset(**dataset_kwargs)\n",
    "model = target_dataset.get_model(encoder_type=\"swin_unetr\")\n",
    "\n",
    "# Apply composite task vector for target domain\n",
    "composite_task_vector = composite_task_vectors[composite_key]\n",
    "model.load_task_vector(composite_task_vector)\n",
    "\n",
    "# Overwrite the model's head with the saved one from the same task\n",
    "head_filename = Path(\"checkpoints\") / f\"{DATASET_NAME}_{DOMAIN}_'3d'_head.pth\"\n",
    "with torch.serialization.safe_globals([UnetOutBlock, Convolution, Conv3d]):\n",
    "    model.head.load_state_dict(\n",
    "        torch.load(\n",
    "            head_filename, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        ).state_dict()\n",
    "    )\n",
    "\n",
    "# Predict single sample\n",
    "model.to(device)\n",
    "batch[\"image\"] = batch[\"image\"].to(device)\n",
    "\n",
    "# Run inference\n",
    "print(\"üîÆ Running inference...\")\n",
    "outputs = model(batch[\"image\"])\n",
    "preds = torch.argmax(outputs, dim=1, keepdim=True)\n",
    "\n",
    "comb_sample = {\n",
    "    \"image\": sample[\"image\"],\n",
    "    \"label\": preds,\n",
    "}\n",
    "# Prepare the combined sample for visualization\n",
    "comb_sample[\"image\"] = comb_sample[\"image\"].squeeze().permute(2, 1, 0).cpu().numpy()\n",
    "comb_sample[\"label\"] = comb_sample[\"label\"].squeeze().permute(2, 1, 0).cpu().numpy()\n",
    "\n",
    "# Visualize the combined sample\n",
    "dataset.visualize_sample_slice(comb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dataset method for prediction visualization\n",
    "\n",
    "dataset.visualize_sample_slice(sample_viz)\n",
    "dataset.visualize_sample_slice(pred_sample_viz)\n",
    "# dataset.visualize_sample_slice(comb_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
