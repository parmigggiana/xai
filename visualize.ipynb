{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb29726d",
   "metadata": {},
   "source": [
    "# Visualization Function Testing Notebook\n",
    "\n",
    "This notebook tests the visualization functions `visualize_sample_slice` with configurable dataset and domain settings. It focuses on ground truth visualization and skips 3D visualization as requested.\n",
    "\n",
    "## Features:\n",
    "- Tests CHAOS and MMWHS datasets\n",
    "- Supports CT and MR domains  \n",
    "- Configurable visualization parameters\n",
    "- Ground truth visualization testing\n",
    "- Error handling and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "804e8132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "# First, import torch and torchvision\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from src.datasets.registry import get_dataset\n",
    "from monai import transforms\n",
    "from monai.data import MetaTensor\n",
    "\n",
    "# Patch per retro-compatibilit√†: PyTorch < 2.6 non ha safe_globals (inutile?)\n",
    "#if not hasattr(torch.serialization, \"safe_globals\"):\n",
    "#    torch.serialization.safe_globals = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e7f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle\n",
    "import os\n",
    "\n",
    "IN_KAGGLE = False\n",
    "if os.environ.get(\"KAGGLE_URL_BASE\", \"\"):\n",
    "    IN_KAGGLE = True\n",
    "    !git clone https://github.com/parmigggiana/xai /kaggle/working/xai\n",
    "    %cd xai\n",
    "    !git fetch\n",
    "    !git reset --hard origin/main\n",
    "    %pip install 'monai[einops,itk,nibabel]>=1.5.0' git+https://github.com/timojl/clipseg.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f9166",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Change these constants to test different datasets and domains:\n",
    "- **DATASET_NAME**: \"CHAOS\" or \"MMWHS\"\n",
    "- **DOMAIN**: \"CT\" or \"MR\" \n",
    "- **ENCODER_TYPE**: \"resnet\" or \"swin_unetr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e714f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration constants\n",
    "DATASET_NAME = \"CHAOS\"  # Change to \"MMWHS\" if needed\n",
    "DOMAIN = \"MR\"  # Change to \"CT\" if needed\n",
    "ENCODER_TYPE = \"clipseg\"  # Changed from \"swin_unetr\" to \"clipseg\"\n",
    "BATCH_SIZE = 1\n",
    "NUM_WORKERS = 1\n",
    "USE_3D = False  # Set to True for 3D data processing\n",
    "SPATIAL_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efea59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing(dataset_name: str, domain: str, is_training=True):\n",
    "    \"\"\"\n",
    "    Build MONAI-native Compose transform pipelines for images and segmentations.\n",
    "    ImageDataset will wrap arrays as MetaTensor so metadata flows automatically.\n",
    "    \"\"\"\n",
    "    decode_func = get_decode_func(dataset_name, domain)\n",
    "\n",
    "    if USE_3D:\n",
    "        image_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            # transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "    else:\n",
    "        image_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "\n",
    "    if domain == \"CT\":\n",
    "        image_transforms.append(\n",
    "            transforms.ScaleIntensityRange(\n",
    "                a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        image_transforms.append(\n",
    "            transforms.NormalizeIntensity(nonzero=True, channel_wise=True)\n",
    "        )\n",
    "\n",
    "    if is_training:\n",
    "        image_transforms.extend(\n",
    "            [\n",
    "                transforms.RandGaussianNoise(prob=0.2, std=0.05),\n",
    "                transforms.RandAdjustContrast(prob=0.2, gamma=(0.9, 1.1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if not USE_3D:\n",
    "        image_transforms.append(transforms.RepeatChannel(repeats=3))\n",
    "\n",
    "    image_transforms.extend(\n",
    "        [\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE,\n",
    "                size_mode=\"longest\",\n",
    "                mode=\"area\",\n",
    "                anti_aliasing=True,\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if not USE_3D:\n",
    "        seg_transforms = [\n",
    "            transforms.Lambda(lambda x: x.squeeze(-1)),\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "        ]\n",
    "    else:\n",
    "        seg_transforms = [\n",
    "            transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "            # transforms.Orientation(axcodes=\"RAS\"),\n",
    "        ]\n",
    "\n",
    "    seg_transforms.extend(\n",
    "        [\n",
    "            transforms.Lambda(lambda x: decode_func(x)),\n",
    "            transforms.Resize(\n",
    "                spatial_size=SPATIAL_SIZE, size_mode=\"longest\", mode=\"nearest\"\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.EnsureType(dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transforms.Compose(image_transforms), transforms.Compose(seg_transforms)\n",
    "\n",
    "\n",
    "def get_decode_func(dataset_name, domain):\n",
    "    from src.datasets.mmwhs import mmwhs_labels\n",
    "\n",
    "    decode = None\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        if domain in [\"MR\", \"MRI\"]:\n",
    "            def decode(labels):\n",
    "                return labels\n",
    "        elif domain == \"CT\":\n",
    "            def decode(labels):\n",
    "                return torch.where(labels > 0, 255.0, 0.0)\n",
    "    elif dataset_name == \"MMWHS\":\n",
    "        def decode(labels):\n",
    "            decoded_labels = torch.zeros_like(labels, dtype=torch.float32)\n",
    "            for i, label_val in enumerate(mmwhs_labels.keys()):\n",
    "                decoded_labels[labels == label_val] = i\n",
    "            return decoded_labels\n",
    "\n",
    "    if decode is None:\n",
    "        print(\n",
    "            f\"Warning: No decode function defined for {dataset_name} in {domain}. Returning labels unchanged.\"\n",
    "        )\n",
    "        def decode(labels):\n",
    "            return labels\n",
    "\n",
    "    return decode\n",
    "\n",
    "def metatensor_batch_to_dict(batch):\n",
    "    \"\"\"\n",
    "    Converts a batch from your dataset (list of MetaTensor) into a dict with 'image' and 'label'.\n",
    "    Assumes:\n",
    "    - batch is a list of length batch_size, each element is a MetaTensor or (MetaTensor, metadata)\n",
    "    - label is stored in metadata as 'label' key or as a separate tensor\n",
    "    \"\"\"\n",
    "    if len(batch) >= 2:\n",
    "        # If batch contains separate image and label tensors\n",
    "        image_data = batch[0]\n",
    "        label_data = batch[1]\n",
    "\n",
    "        # Extract image\n",
    "        if isinstance(image_data, tuple):\n",
    "            image, _ = image_data\n",
    "        else:\n",
    "            image = image_data\n",
    "\n",
    "        # Extract label\n",
    "        if isinstance(label_data, tuple):\n",
    "            label, _ = label_data\n",
    "        else:\n",
    "            label = label_data\n",
    "\n",
    "        # Convert to tensors if needed\n",
    "        image = image.data.float() if hasattr(image, \"data\") else torch.tensor(image, dtype=torch.float32)\n",
    "        label = label.data.float() if hasattr(label, \"data\") else torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    else:\n",
    "        # Fallback to original logic if only one element\n",
    "        sample = batch[0]\n",
    "        if isinstance(sample, tuple):\n",
    "            data, metadata = sample\n",
    "        else:\n",
    "            data = sample\n",
    "            metadata = getattr(sample, \"meta\", {}) or {}\n",
    "\n",
    "        # Extract image and label\n",
    "        image = data.data.float() if hasattr(data, \"data\") else torch.tensor(data, dtype=torch.float32)\n",
    "        label = metadata.get(\"label\", None)\n",
    "\n",
    "    return {\"image\": image, \"label\": label}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17164a2",
   "metadata": {},
   "source": [
    "## Ground Truth Visualization Testing\n",
    "\n",
    "This section loads the dataset and tests the `visualize_sample_slice` function with ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29855ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.common import BaseDataset\n",
    "\n",
    "\n",
    "def test_ground_truth_visualization():\n",
    "    \"\"\"Test visualization functions with ground truth data.\"\"\"\n",
    "    print(\"üîç Testing Ground Truth Visualization...\")\n",
    "    print(f\"Dataset: {DATASET_NAME}, Domain: {DOMAIN}\")\n",
    "\n",
    "    image_transform, seg_transform = get_preprocessing(\n",
    "        DATASET_NAME, DOMAIN, is_training=False\n",
    "    )\n",
    "    # image_transform = None\n",
    "    # seg_transform = None\n",
    "\n",
    "    # Load dataset\n",
    "    dataset: BaseDataset = get_dataset(\n",
    "        dataset_name=DATASET_NAME,\n",
    "        domain=DOMAIN,\n",
    "        transform=image_transform,  # Use transform instead of preprocess\n",
    "        seg_transform=seg_transform,  # Pass seg_transform too\n",
    "        base_path=Path(\"data\"),\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "        slice_2d=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Get a sample from the dataset\n",
    "    loader = dataset.train_loader\n",
    "    iterat = iter(loader)\n",
    "    batch = next(iterat)\n",
    "    batch = next(iterat)\n",
    "\n",
    "    # Convert batch to dictionary format\n",
    "    sample_dict = metatensor_batch_to_dict(batch)\n",
    "    print(f\"Image shape: {sample_dict['image'].shape}\")\n",
    "    if sample_dict['label'] is not None:\n",
    "        print(f\"Label shape: {sample_dict['label'].shape}\")\n",
    "    else:\n",
    "        print(\"Label: None\")\n",
    "\n",
    "    # Extract a single sample\n",
    "    #sample = {\n",
    "    #    \"image\": batch[\"image\"],  # Keep batch dimension for inference\n",
    "    #    \"label\": batch[\"label\"],  # Keep batch dimension for inference\n",
    "    #}\n",
    "    return dataset, sample_dict, batch\n",
    "\n",
    "\n",
    "def test_inference_and_visualization(dataset, batch):\n",
    "    \"\"\"Test inference with semantic head training and visualize both GT and prediction.\"\"\"\n",
    "    print(\"üöÄ Testing Inference with Semantic Head Training...\")\n",
    "    print(f\"Dataset: {DATASET_NAME}, Domain: {DOMAIN}\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Get model with semantic head\n",
    "    model = dataset.get_model(ENCODER_TYPE)\n",
    "\n",
    "    # Check what checkpoint files exist\n",
    "    checkpoint_dir = Path(\"checkpoints\")\n",
    "    available_files = list(checkpoint_dir.glob(\"*.pth\"))\n",
    "    print(f\"Available checkpoint files: {[f.name for f in available_files]}\")\n",
    "\n",
    "    # Try different possible checkpoint names\n",
    "    possible_names = [\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_2d_finetuned.pth\",\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_{'3d' if USE_3D else '2d'}_finetuned.pth\",\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_2d_baseline.pth\",\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "    ]\n",
    "\n",
    "    checkpoint_path = None\n",
    "    for name in possible_names:\n",
    "        potential_path = checkpoint_dir / name\n",
    "        if potential_path.exists():\n",
    "            checkpoint_path = potential_path\n",
    "            print(f\"Using checkpoint: {checkpoint_path}\")\n",
    "            break\n",
    "\n",
    "    if checkpoint_path is None:\n",
    "        print(\"No suitable checkpoint found. Available files:\")\n",
    "        for f in available_files:\n",
    "            print(f\"  - {f.name}\")\n",
    "        raise FileNotFoundError(\"No suitable checkpoint found\")\n",
    "\n",
    "    # Load the checkpoint\n",
    "    try:\n",
    "        finetuned_state_dict = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "        if hasattr(finetuned_state_dict, 'state_dict'):\n",
    "            finetuned_state_dict = finetuned_state_dict.state_dict()\n",
    "        model.encoder.load_state_dict(finetuned_state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        # Try without weights_only for older checkpoints\n",
    "        finetuned_state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        if hasattr(finetuned_state_dict, 'state_dict'):\n",
    "            finetuned_state_dict = finetuned_state_dict.state_dict()\n",
    "        model.encoder.load_state_dict(finetuned_state_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    batch[\"image\"] = batch[\"image\"].to(device)\n",
    "\n",
    "    # Run inference\n",
    "    print(\"üîÆ Running inference...\")\n",
    "    outputs = model(batch[\"image\"])\n",
    "    preds = torch.argmax(outputs, dim=1, keepdim=True)\n",
    "\n",
    "    print(f\"Prediction unique values: {torch.unique(preds)}\")\n",
    "\n",
    "    return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faef3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d, Conv2d, ConvTranspose2d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList, Sequential, ModuleDict\n",
    "from monai.networks.nets.swin_unetr import BasicLayer, SwinTransformerBlock, WindowAttention, PatchMerging\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.modules.linear import Linear, NonDynamicallyQuantizableLinear, Identity\n",
    "from torch.nn.modules.activation import Softmax, GELU, LeakyReLU, ReLU, MultiheadAttention\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock, UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock, UnetOutBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.nn.modules.transformer import (\n",
    "    TransformerEncoderLayer, TransformerEncoder,\n",
    "    TransformerDecoderLayer, TransformerDecoder\n",
    ")\n",
    "\n",
    "# CLIPSeg imports\n",
    "from src.CLIPSeg import CLIPSeg\n",
    "from clipseg.clipseg import CLIPDensePredT\n",
    "from clip.model import CLIP, VisionTransformer, Transformer, ResidualAttentionBlock, QuickGELU\n",
    "\n",
    "SAFE_GLOBALS = [\n",
    "    # SwinUNETR components\n",
    "    SwinUNETR, SwinTransformer, PatchEmbed, Conv3d, Dropout, ModuleList,\n",
    "    BasicLayer, SwinTransformerBlock, LayerNorm, WindowAttention, Linear,\n",
    "    Softmax, Identity, MLPBlock, GELU, PatchMerging, UnetrBasicBlock,\n",
    "    UnetResBlock, Convolution, LeakyReLU, InstanceNorm3d, UnetrUpBlock,\n",
    "    UnetOutBlock,\n",
    "\n",
    "    # CLIPSeg components\n",
    "    CLIPSeg, CLIPDensePredT, CLIP, VisionTransformer, Conv2d, Sequential,\n",
    "    ResidualAttentionBlock, MultiheadAttention, NonDynamicallyQuantizableLinear,\n",
    "    QuickGELU, Embedding, ReLU, ConvTranspose2d, TransformerEncoderLayer,\n",
    "    TransformerEncoder, TransformerDecoderLayer, TransformerDecoder,\n",
    "    LayerNorm, Transformer, ModuleDict,\n",
    "]\n",
    "\n",
    "def load_checkpoint_safely(checkpoint_path, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Utility function to safely load PyTorch checkpoints with proper error handling.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path: Path to the checkpoint file\n",
    "        device: Device to load the checkpoint on\n",
    "\n",
    "    Returns:\n",
    "        Loaded checkpoint state dict\n",
    "    \"\"\"\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "\n",
    "    # Handle different checkpoint formats\n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            return checkpoint['model_state_dict']\n",
    "        elif 'state_dict' in checkpoint:\n",
    "            return checkpoint['state_dict']\n",
    "        else:\n",
    "            return checkpoint\n",
    "    else:\n",
    "        return checkpoint.state_dict() if hasattr(checkpoint, 'state_dict') else checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffcd491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference_and_visualization_clipseg(dataset, batch):\n",
    "    \"\"\"Test inference with CLIPSeg model using pre-trained weights.\"\"\"\n",
    "    print(\"üöÄ Testing Inference with CLIPSeg...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = dataset.get_model(encoder_type=ENCODER_TYPE)\n",
    "\n",
    "\n",
    "    # Check for fine-tuned checkpoint (optional)\n",
    "    checkpoint_dir = Path(\"checkpoints\")\n",
    "    possible_names = [\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_2d_clipseg_finetuned.pth\",\n",
    "        f\"{DATASET_NAME}_{DOMAIN}_clipseg_finetuned.pth\",\n",
    "        f\"clipseg_{DATASET_NAME}_{DOMAIN}_finetuned.pth\",\n",
    "    ]\n",
    "\n",
    "    checkpoint_path = None\n",
    "    for name in possible_names:\n",
    "        potential_path = checkpoint_dir / name\n",
    "        if potential_path.exists():\n",
    "            checkpoint_path = potential_path\n",
    "            break\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        try:\n",
    "            # Use the safe loading function\n",
    "            state_dict = load_checkpoint_safely(checkpoint_path, device)\n",
    "\n",
    "            # Load fine-tuned state dict\n",
    "            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"‚úÖ Successfully loaded fine-tuned checkpoint\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading fine-tuned checkpoint: {e}\")\n",
    "            print(\"Continuing with pre-trained weights only...\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Handle batch properly - it's a tuple from the DataLoader\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        sample_dict = metatensor_batch_to_dict(batch)\n",
    "        batch_image = sample_dict['image'].unsqueeze(0) if sample_dict['image'].dim() == 3 else sample_dict['image']\n",
    "    else:\n",
    "        # Fallback if batch is already a dict\n",
    "        batch_image = batch[\"image\"]\n",
    "\n",
    "    # Handle tensor shape properly\n",
    "    print(f\"Original batch_image shape: {batch_image.shape}\")\n",
    "\n",
    "    if len(batch_image.shape) == 5:\n",
    "        batch_image = batch_image.squeeze(1)\n",
    "        print(f\"After squeezing dimension 1: {batch_image.shape}\")\n",
    "\n",
    "    batch_image = batch_image.to(device)\n",
    "\n",
    "    # Run inference\n",
    "    print(\"üîÆ Running CLIPSeg inference...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_image)\n",
    "        preds = torch.argmax(outputs, dim=1, keepdim=True)\n",
    "\n",
    "    print(f\"Prediction unique values: {torch.unique(preds)}\")\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2799974",
   "metadata": {},
   "source": [
    "## Dataset Visualization Method Test\n",
    "\n",
    "Test the `dataset.visualize_sample_slice()` method with the loaded data. This section will show both ground truth and prediction visualizations for the **same sample** to enable direct comparison. The dataset-specific implementation automatically applies the correct rotation and flip parameters for each dataset type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cecbce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Ground Truth Visualization...\n",
      "Dataset: CHAOS, Domain: MR\n",
      "Dataset MR total samples: 623\n",
      "Split sizes - Train: 436, Val: 93, Test: 94\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m## Dataset Visualization Method Test\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dataset, sample, batch = \u001b[43mtest_ground_truth_visualization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Test dataset.visualize_sample_slice method\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Testing dataset.visualize_sample_slice method...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtest_ground_truth_visualization\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m batch = \u001b[38;5;28mnext\u001b[39m(iterat)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Convert batch to dictionary format\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m sample_dict = \u001b[43mmetatensor_batch_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_dict[\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_dict[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mmetatensor_batch_to_dict\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03mConverts a batch from your dataset (list of MetaTensor) into a dict with 'image' and 'label'.\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03mAssumes:\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[33;03m- batch is a list of length batch_size, each element is a MetaTensor or (MetaTensor, metadata)\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m- label is stored in metadata as 'label' key or as a separate tensor\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) >= \u001b[32m2\u001b[39m:\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# If batch contains separate image and label tensors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     image_data = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    116\u001b[39m     label_data = batch[\u001b[32m1\u001b[39m]\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# Extract image\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## Dataset Visualization Method Test\n",
    "dataset, sample, batch = test_ground_truth_visualization()\n",
    "\n",
    "# Test dataset.visualize_sample_slice method\n",
    "print(\"\\nüìä Testing dataset.visualize_sample_slice method...\")\n",
    "\n",
    "# Debug: check shapes before processing\n",
    "print(f\"Original image shape: {sample['image'].shape}\")\n",
    "print(f\"Original label shape: {sample['label'].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# Apply squeeze to remove batch and channel dimensions for visualization\n",
    "sample_viz = {\n",
    "    \"image\": sample[\"image\"].squeeze().cpu().numpy(),\n",
    "    \"label\": sample[\"label\"].squeeze().cpu().numpy()\n",
    "}\n",
    "\n",
    "print(f\"After squeeze - Image shape: {sample_viz['image'].shape}\")\n",
    "print(f\"After squeeze - Label shape: {sample_viz['label'].shape}\")\n",
    "\n",
    "# üîç DEBUG: Check dimensionality before visualization\n",
    "print(f\"\\nüîç DEBUG - seg_slice dimensionality:\")\n",
    "seg_slice = sample_viz['label']\n",
    "print(f\"  seg_slice.shape: {seg_slice.shape}\")\n",
    "print(f\"  seg_slice.ndim: {seg_slice.ndim}\")\n",
    "print(f\"  seg_slice type: {type(seg_slice)}\")\n",
    "\n",
    "# Check if it's the right shape for legend generation\n",
    "if seg_slice.ndim >= 2:\n",
    "    print(\"  ‚úÖ Shape is compatible with legend generation\")\n",
    "    unique_vals = np.unique(seg_slice)\n",
    "    print(f\"  Unique values in seg_slice: {unique_vals}\")\n",
    "else:\n",
    "    print(\"  ‚ùå Shape is NOT compatible with legend generation\")\n",
    "    print(\"  Need to fix the shape before visualization\")\n",
    "\n",
    "print(f\"  seg_slice.shape: {seg_slice.shape}\")\n",
    "print(f\"  seg_slice.ndim: {seg_slice.ndim}\")\n",
    "\n",
    "dataset.visualize_sample_slice(sample_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26243048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the SAME sample with CLIPSeg\n",
    "print(\"\\nüîÆ Testing dataset.visualize_sample_slice method with CLIPSeg PREDICTIONS...\")\n",
    "\n",
    "preds = test_inference_and_visualization_clipseg(dataset, batch)\n",
    "\n",
    "\n",
    "print(f\"Applying {DOMAIN} domain encoding...\")\n",
    "encoded_preds = dataset.encode(preds)\n",
    "print(f\"Encoded prediction unique values: {torch.unique(encoded_preds)}\")\n",
    "\n",
    "# Prepare prediction sample for visualization with proper squeeze\n",
    "pred_sample_viz = {\n",
    "    \"image\": sample[\"image\"].squeeze().cpu().numpy(),\n",
    "    \"label\": encoded_preds.squeeze().cpu().numpy()\n",
    "}\n",
    "\n",
    "\n",
    "# Use the dataset visualization method\n",
    "print(\"\\nüìä Visualizing CLIPSeg predictions...\")\n",
    "# Use the SAME orientation parameters as GT to avoid mismatches\n",
    "dataset.visualize_sample_slice(pred_sample_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dataset method for prediction visualization\n",
    "\n",
    "dataset.visualize_sample_slice(sample_viz)\n",
    "dataset.visualize_sample_slice(pred_sample_viz)\n",
    "#dataset.visualize_sample_slice(comb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b5378",
   "metadata": {},
   "source": [
    "# Visualization Composite Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34c3573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector for CHAOS dataset in MR domain with 2d images\n",
      "Building task vector for CHAOS dataset in CT domain with 2d images\n",
      "Building task vector for MMWHS dataset in MR domain with 2d images\n",
      "Building task vector for MMWHS dataset in CT domain with 2d images\n"
     ]
    }
   ],
   "source": [
    "# SWIN UNETR Task Vectors\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.networks.nets.swin_unetr import SwinTransformer\n",
    "from monai.networks.blocks.patchembedding import PatchEmbed\n",
    "from torch.nn.modules.conv import Conv3d\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from monai.networks.nets.swin_unetr import BasicLayer\n",
    "from monai.networks.nets.swin_unetr import SwinTransformerBlock\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from monai.networks.nets.swin_unetr import WindowAttention\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.activation import Softmax\n",
    "from torch.nn.modules.linear import Identity\n",
    "from monai.networks.blocks.mlp import MLPBlock\n",
    "from torch.nn.modules.activation import GELU\n",
    "from monai.networks.nets.swin_unetr import PatchMerging\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetResBlock\n",
    "from monai.networks.blocks.convolutions import Convolution\n",
    "from torch.nn.modules.activation import LeakyReLU\n",
    "from torch.nn.modules.instancenorm import InstanceNorm3d\n",
    "from monai.networks.blocks.unetr_block import UnetrUpBlock\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from torch.nn.modules.conv import ConvTranspose3d\n",
    "\n",
    "safe_globals = [\n",
    "    SwinUNETR,\n",
    "    SwinTransformer,\n",
    "    PatchEmbed,\n",
    "    Conv3d,\n",
    "    Dropout,\n",
    "    ModuleList,\n",
    "    BasicLayer,\n",
    "    SwinTransformerBlock,\n",
    "    LayerNorm,\n",
    "    WindowAttention,\n",
    "    Linear,\n",
    "    Softmax,\n",
    "    Identity,\n",
    "    MLPBlock,\n",
    "    GELU,\n",
    "    PatchMerging,\n",
    "    UnetrBasicBlock,\n",
    "    UnetResBlock,\n",
    "    Convolution,\n",
    "    LeakyReLU,\n",
    "    InstanceNorm3d,\n",
    "    UnetrUpBlock,\n",
    "    ConvTranspose3d,\n",
    "    UnetOutBlock,\n",
    "]\n",
    "##\n",
    "\n",
    "## CLIPSeg Task Vectors\n",
    "from src.CLIPSeg import CLIPSeg\n",
    "from clipseg.clipseg import CLIPDensePredT\n",
    "from clip.model import (\n",
    "    CLIP,\n",
    "    VisionTransformer,\n",
    "    LayerNorm,\n",
    "    Transformer,\n",
    "    ResidualAttentionBlock,\n",
    "    QuickGELU,\n",
    ")\n",
    "from torch.nn.modules.conv import Conv2d, ConvTranspose2d\n",
    "from torch.nn.modules.container import Sequential\n",
    "from torch.nn.modules.activation import MultiheadAttention, ReLU\n",
    "from torch.nn.modules.linear import NonDynamicallyQuantizableLinear\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.nn.modules.transformer import (\n",
    "    TransformerEncoderLayer,\n",
    "    TransformerEncoder,\n",
    "    TransformerDecoderLayer,\n",
    "    TransformerDecoder,\n",
    ")\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn.modules.container import ModuleDict\n",
    "\n",
    "safe_globals.extend(\n",
    "    [\n",
    "        CLIPSeg,\n",
    "        CLIPDensePredT,\n",
    "        CLIP,\n",
    "        VisionTransformer,\n",
    "        Conv2d,\n",
    "        LayerNorm,\n",
    "        Transformer,\n",
    "        Sequential,\n",
    "        ResidualAttentionBlock,\n",
    "        MultiheadAttention,\n",
    "        NonDynamicallyQuantizableLinear,\n",
    "        QuickGELU,\n",
    "        Embedding,\n",
    "        ReLU,\n",
    "        ConvTranspose2d,\n",
    "        TransformerEncoderLayer,\n",
    "        TransformerEncoder,\n",
    "        TransformerDecoderLayer,\n",
    "        TransformerDecoder,\n",
    "        relu,\n",
    "        ModuleDict,\n",
    "    ]\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "from src.task_vector import TaskVector\n",
    "\n",
    "DATASET_NAMES = [\"CHAOS\", \"MMWHS\"]\n",
    "DOMAINS = [\"MR\",\"CT\"]\n",
    "CHECKPOINT_PATH = \"checkpoints/\"\n",
    "DATA_PATH = \"data/\"\n",
    "CHECKPOINT_PATH = Path(CHECKPOINT_PATH)\n",
    "DATA_PATH = Path(DATA_PATH)\n",
    "\n",
    "\n",
    "# Build Task Vectors for each dataset and domain\n",
    "task_vectors = {}\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for domain in DOMAINS:\n",
    "        print(\n",
    "            f\"Building task vector for {dataset_name} dataset in {domain} domain with {'3d' if USE_3D else '2d'} images\"\n",
    "        )\n",
    "        baseline_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_baseline.pth\"\n",
    "        )\n",
    "        finetuned_checkpoint = (\n",
    "            CHECKPOINT_PATH\n",
    "            / f\"{dataset_name}_{domain}_{'3d' if USE_3D else '2d'}_finetuned.pth\"\n",
    "        )\n",
    "        if not baseline_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Baseline checkpoint for {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "        if not finetuned_checkpoint.exists():\n",
    "            print(\n",
    "                f\"Finetuned checkpoint {dataset_name} {domain} does not exist. Skipping task vector creation.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        with torch.serialization.safe_globals(\n",
    "            safe_globals=safe_globals,\n",
    "        ):\n",
    "            task_vector = TaskVector(baseline_checkpoint, finetuned_checkpoint)\n",
    "            # Remove keys associated with the output layers from the task vector\n",
    "            # For swin it's all layers starting with '.out'\n",
    "            # For clipseg it might not be necessary since the model architecture isn't dependent on the number of output features\n",
    "            if ENCODER_TYPE == \"swin_unetr\":\n",
    "                for k in task_vector.keys():\n",
    "                    if k.startswith(\".out\"):\n",
    "                        del task_vector[k]\n",
    "        task_vectors[f\"{dataset_name}_{domain}\"] = task_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aac9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build composite task vectors using arithmetic\n",
    "composite_task_vectors = {\n",
    "    \"MMWHS\": task_vectors[\"MMWHS_MR\"]\n",
    "    + task_vectors[\"MMWHS_CT\"],\n",
    "    \"CHAOS\": task_vectors[\"CHAOS_MR\"]\n",
    "    + task_vectors[\"CHAOS_CT\"],\n",
    "    \"MR\": task_vectors[\"CHAOS_MR\"]\n",
    "    + task_vectors[\"MMWHS_MR\"],\n",
    "    \"CT\": task_vectors[\"CHAOS_CT\"]\n",
    "    + task_vectors[\"MMWHS_CT\"],\n",
    "}\n",
    "alpha = 0.5\n",
    "\n",
    "\n",
    "# CLIPSeg inference using composite_task_vectors (no checkpoints)\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _get_images_from_batch(batch):\n",
    "    if isinstance(batch, dict):\n",
    "        images = batch.get(\"image\") or batch.get(\"images\")\n",
    "    elif isinstance(batch, (list, tuple)):\n",
    "        images = batch[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported batch type: {type(batch)}\")\n",
    "    if hasattr(images, \"as_tensor\"):\n",
    "        images = images.as_tensor()\n",
    "    return images\n",
    "\n",
    "\n",
    "def _to_numpy_img(x):\n",
    "    if hasattr(x, \"detach\"):\n",
    "        x = x.detach()\n",
    "    if hasattr(x, \"cpu\"):\n",
    "        x = x.cpu()\n",
    "    x = x.float()\n",
    "    # expect [B, C, H, W]\n",
    "    if x.ndim == 4:\n",
    "        x = x[0, 0]\n",
    "    elif x.ndim == 3:\n",
    "        x = x[0]\n",
    "    return x.numpy()\n",
    "\n",
    "\n",
    "def _to_numpy_mask(x):\n",
    "    if hasattr(x, \"detach\"):\n",
    "        x = x.detach()\n",
    "    if hasattr(x, \"cpu\"):\n",
    "        x = x.cpu()\n",
    "    # expect [B, 1, H, W]\n",
    "    if x.ndim == 4:\n",
    "        x = x[0, 0]\n",
    "    elif x.ndim == 3:\n",
    "        x = x[0]\n",
    "    return x.long().numpy()\n",
    "\n",
    "\n",
    "def _plot_triplet(img_np, pred_np, gt_np=None, title_prefix=\"\"):\n",
    "    cols = 3 if gt_np is not None else 2\n",
    "    fig, axes = plt.subplots(1, cols, figsize=(12, 4))\n",
    "    axes = axes if isinstance(axes, (list, tuple, np.ndarray)) else [axes]\n",
    "    axes[0].imshow(img_np, cmap=\"gray\")\n",
    "    axes[0].set_title(f\"{title_prefix} image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(pred_np, cmap=\"nipy_spectral\", interpolation=\"nearest\")\n",
    "    axes[1].set_title(f\"{title_prefix} pred\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    if gt_np is not None and cols == 3:\n",
    "        axes[2].imshow(gt_np, cmap=\"nipy_spectral\", interpolation=\"nearest\")\n",
    "        axes[2].set_title(f\"{title_prefix} label\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_inference_and_visualization_clipseg(dataset, batch, composite_key=None, scaling=None, encoder_type_override=None):\n",
    "    \"\"\"Run CLIPSeg inference applying a composite task vector instead of a checkpoint.\n",
    "\n",
    "    Args:\n",
    "        dataset: dataset instance providing get_model and classnames\n",
    "        batch: a single batch from a DataLoader\n",
    "        composite_key: optional key to select the composite task vector. If None,\n",
    "                       tries dataset name, domain (e.g., \"MMWHS\", \"CHAOS\", \"CT\", \"MR\"),\n",
    "                       then combined key \"<dataset>_<domain>\" (e.g., \"MMWHS_CT\").\n",
    "        scaling: optional scaling coef for the task vector (defaults to global alpha if available)\n",
    "        encoder_type_override: force a specific encoder_type (defaults to global encoder_type)\n",
    "    Returns:\n",
    "        preds tensor shaped [B, 1, H, W] with class indices\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Testing Inference with CLIPSeg + composite_task_vectors‚Ä¶\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    enc = encoder_type_override or encoder_type\n",
    "    model = dataset.get_model(encoder_type=enc)\n",
    "\n",
    "    # Resolve composite key\n",
    "    key = composite_key\n",
    "    if key is None:\n",
    "        ds_key = getattr(dataset, \"name\", type(dataset).__name__)\n",
    "        dom_key = getattr(dataset, \"domain\", None)\n",
    "        dom_key = str(dom_key).upper().replace(\"MRI\", \"MR\") if dom_key else None\n",
    "        combined_key = f\"{ds_key}_{dom_key}\" if dom_key else None\n",
    "        if \"composite_task_vectors\" in globals() and ds_key in composite_task_vectors:\n",
    "            key = ds_key\n",
    "        elif \"composite_task_vectors\" in globals() and dom_key in composite_task_vectors:\n",
    "            key = dom_key\n",
    "        elif \"composite_task_vectors\" in globals() and combined_key in composite_task_vectors:\n",
    "            key = combined_key\n",
    "\n",
    "    # Resolve scaling\n",
    "    if scaling is None:\n",
    "        scaling = globals().get(\"alpha\", 1.0)\n",
    "\n",
    "    # Apply composite task vector if available\n",
    "    if \"composite_task_vectors\" in globals():\n",
    "        ctv = composite_task_vectors.get(key)\n",
    "        if ctv is not None:\n",
    "            print(f\"üîß Applying composite task vector: key='{key}', alpha={scaling}\")\n",
    "            model.load_task_vector(ctv, scaling_coef=float(scaling))\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No composite task vector found for key '{key}'. Using base model.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è composite_task_vectors not defined. Using base model.\")\n",
    "\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # Prepare images\n",
    "    images = _get_images_from_batch(batch)\n",
    "    if images.ndim == 5:  # e.g. [B, 1, H, W, D] -> squeeze depth/channel dims for 2D case\n",
    "        images = images.squeeze(1)\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1, keepdim=True)\n",
    "\n",
    "    print(f\"Prediction unique values: {torch.unique(preds)}\")\n",
    "    return preds\n",
    "\n",
    "\n",
    "# Helper: pick an available loader (test > val > train)\n",
    "\n",
    "def _get_any_loader(ds):\n",
    "    return getattr(ds, \"test_loader\", None) or getattr(ds, \"val_loader\", None) or getattr(ds, \"train_loader\", None)\n",
    "\n",
    "\n",
    "# Quick visualization harness for composite_task_vectors\n",
    "\n",
    "def visualize_composite_predictions(dataset_name: str, target_domain: str, use_key: str, n_samples: int = 2):\n",
    "    print(f\"\\n=== Visualize predictions: dataset='{dataset_name}' domain='{target_domain}' with composite='{use_key}' (alpha={alpha}) ===\")\n",
    "    image_transform, seg_transform = get_preprocessing(dataset_name, target_domain, is_training=False)\n",
    "\n",
    "    extra_kwargs = {}\n",
    "    if dataset_name == \"CHAOS\":\n",
    "        # Keep consistent with experiments above\n",
    "        extra_kwargs[\"liver_only\"] = True\n",
    "\n",
    "    ds = get_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        base_path=DATA_PATH,\n",
    "        domain=target_domain,\n",
    "        transform=image_transform,\n",
    "        seg_transform=seg_transform,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=0,\n",
    "        slice_2d=not USE_3D,\n",
    "        cache_max_items=CACHE_MAX_ITEMS,\n",
    "        enable_cache=ENABLE_CACHE,\n",
    "        **extra_kwargs,\n",
    "    )\n",
    "\n",
    "    loader = _get_any_loader(ds)\n",
    "    if loader is None:\n",
    "        print(\"‚ö†Ô∏è No loader available.\")\n",
    "        return\n",
    "\n",
    "    batch = next(iter(loader))\n",
    "    preds = test_inference_and_visualization_clipseg(ds, batch, composite_key=use_key, scaling=alpha)\n",
    "\n",
    "    # Prepare small preview\n",
    "    images = _get_images_from_batch(batch)\n",
    "    labels = batch.get(\"label\") if isinstance(batch, dict) else (batch[1] if isinstance(batch, (list, tuple)) and len(batch) > 1 else None)\n",
    "\n",
    "    img_np = _to_numpy_img(images)\n",
    "    pred_np = _to_numpy_mask(preds)\n",
    "    gt_np = _to_numpy_mask(labels) if labels is not None else None\n",
    "\n",
    "    _plot_triplet(img_np, pred_np, gt_np, title_prefix=f\"{dataset_name}/{target_domain} [{use_key}]\")\n",
    "\n",
    "\n",
    "# Drive a few previews for each dataset/domain with all composite key variants\n",
    "for ds_name in DATASET_NAMES:\n",
    "    for dom in DOMAINS:\n",
    "        # dataset-level key (Part 1)\n",
    "        if ds_name in composite_task_vectors:\n",
    "            visualize_composite_predictions(ds_name, dom, use_key=ds_name)\n",
    "        # domain-level key (Part 1)\n",
    "        if dom in composite_task_vectors:\n",
    "            visualize_composite_predictions(ds_name, dom, use_key=dom)\n",
    "        # combined key (Part 2)\n",
    "        combined_key = f\"{ds_name}_{dom}\"\n",
    "        if combined_key in composite_task_vectors:\n",
    "            visualize_composite_predictions(ds_name, dom, use_key=combined_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
